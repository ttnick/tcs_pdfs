\documentclass[11pt, a4paper]{article}
\usepackage[left=3cm, right=3cm, bottom=3cm]{geometry}
\linespread{1.05}

% packages
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{stmaryrd}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{mathdots}
\usepackage{mathtools}
\usepackage{listings}
\usepackage[linesnumbered, ruled, vlined, ngerman]{algorithm2e}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{array}
\usepackage[justification=centering]{caption}
\usepackage{subcaption}
\usetikzlibrary{arrows, automata, graphs, shapes, petri, decorations.pathmorphing}
\usepackage{hyperref}

% meta
\clubpenalty = 10000
\widowpenalty = 10000
\displaywidowpenalty = 10000
\parindent = 0pt

% define environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}[definition]{Beispiel}
\newtheorem*{example*}{Beispiel}
\newtheorem*{remark*}{Bemerkung}

\theoremstyle{plain}
\newtheorem{theorem}[definition]{Satz}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Korollar}

\numberwithin{equation}{section}

\renewcommand{\labelenumi}{(\roman{enumi})}
\makeatletter
\newcommand*{\shifttext}[2]{
	\settowidth{\@tempdima}{#2}
	\makebox[\@tempdima]{\hspace*{#1}#2}
}
\makeatother
\def\Rho{\mathrm{P}}

\newenvironment{problem}[1]{\begin{tabular}{|p{.96\textwidth}|} \hline \textsc{#1}\\}{\\ \hline \end{tabular}}
\newenvironment{subproof}[1][\proofname]{\renewcommand{\qedsymbol}{$\blacksquare$}\vspace{-3ex}\begin{proof}[#1]}{\end{proof}\vspace{-3ex}}
\newcommand{\comp}[1]{\overline{#1}}
\newcommand{\qedw}{\hfill$\square$}
\newcommand{\qedb}{\hfill$\blacksquare$}
\newcommand{\shuffle}{\mathrel{\shifttext{5pt}{$\equiv$}\shifttext{-5pt}{$=$}}}
%\newcommand{\reaches}[1]{\overset{#1}{\rightarrow}}
\newcommand{\reaches}{\xrightarrow}
\newcommand{\reachess}[2]{\overset{#1}{\underset{#2}{xrightarrow}}}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\ind}{index}
\DeclareMathOperator{\bin}{bin}
\DeclareMathOperator{\xor}{xor}
\DeclareMathOperator{\lex}{\prec_\text{lex}}
\DeclareMathOperator{\canon}{\prec_\text{cn}}
\let\emptyset\varnothing

\lstset{numbers=left, xleftmargin=.2\textwidth, xrightmargin=.2\textwidth, basicstyle=\ttfamily\bfseries}

% Here we go...
\begin{document}

% title page
\pagestyle{empty}
\begin{center}
    Rheinisch-Westf\"alische Technische Hochschule Aachen\\[10em]

    \begin{LARGE}
    		Skript zur Vorlesung\\[1.5em]
		\textbf{Formale Systeme, Automaten, Prozesse}
    \end{LARGE}
	\vfill
    \begin{Large}
    		Letzte Änderung:\\
    		\today\\[2em]
		Autor:\\
		Niklas Rieken\\
	\end{Large}
	\vfill
	\includegraphics[scale=.4]{icons/cc.png}
	\includegraphics[scale=.4]{icons/by.png}
	\includegraphics[scale=.4]{icons/sa.png}\\
	Dieses Werk ist lizenziert unter einer Creative Commons Namensnennung -- Weitergabe unter gleichen Bedingungen 4.0 International Lizenz.
\end{center}


\newpage
% acknowledgements
\vspace*{\fill}
\section*{Hinweise}
Dieses Skript entstand aus der Vorlesung Formale Systeme, Automaten, Prozesse an der RWTH Aachen von Prof. Dr. Wolfgang Thomas und Prof. Dr. Martin Grohe vom Lehrstuhl Informatik~7 in den Sommersemestern 2015 und 2016. Ein paar Notationen und Definitionen sind außerdem adaptiert aus dem Skript zu den Diskreten Strukturen und Lineare Algebra I für Informatiker von Dr. Timo Hanke und Prof. Dr. Gerhard Hiß vom Lehrstuhl~D für Mathematik.\\
\ \\
Falls inhaltliche oder sprachliche Fehler gefunden werden oder Unklarheiten aufkommen beim Lesen dieses Skripts, würde ich mich sehr über eine Mail an \href{mailto:niklas@fosap.de}{\texttt{niklas@fosap.de}} freuen.
\vspace*{\fill}


\newpage
% table of contents
\tableofcontents


\newpage
\pagestyle{headings}
\section{Mathematisches Vorwissen}\label{sec:pre}
In diesem ersten Kapitel fixieren wir einige mathematischen Notationen und geben elementare Sätze aus der diskreten Mathematik, die im weiteren verlauf der Vorlesung benötigt werden. In der Regel sollten nahezu alle Begriffe und Notationen aus dem ersten Semester bereits bekannt sein. Deshalb ist dieses Kapitel eher nur für Sommersemesteranfänger bestimmt. Die in diesem Abschnitt gesammelten Definitionen und Sätze sind nicht alle relevant, aber dennoch nützlich zu kennen im weiteren Verlauf des Studiums.


\subsection{Mengen}\label{sec:pre_sets}
Der Begriff Menge geht auf Georg Cantor aus dem 19. Jahrhundert zurück und wurde (verglichen mit späteren Definitionen in diesem Skript) informell beschrieben.
\begin{quote}
	Unter einer ``Menge`` verstehen wir jede Zusammenfassung $M$ von bestimmten wohlunterschiedenen Objekten $m$ unserer Anschauung oder unseres Denkens (welche die ``Elemente`` von $M$ genannt werden) zu einem Ganzen.
\end{quote}

Wir definieren eine Menge wie folgt
\begin{definition}
	Eine \textit{Menge} $M$ ist etwas, zu dem jedes beliebige Objekt $x$ entweder \textit{Element} der Menge ist ($x \in M$), oder nicht ($x \notin M$).
\end{definition}
Mengen selbst können auch wieder als Objekte aufgefasst werden, also Elemente anderer Mengen sein. Wir vermeiden jedoch Aussagen über ``Mengen, die sich selbst enthalten``, da so schnell Widersprüche entstehen können (vgl. Russel'sche Antinomie). Wir schließen uns der weit verbreiteten \textit{Zermelo-Fraenkel-Mengenlehre} an, dazu geben wir jedoch keine Details (diese findet man zum Beispiel in der Logik 2-Vorlesung im Wahlpflichtbereich). Wir schauen uns nur an, wie wir Mengen im allgemeinen betrachten können. Folgende Definition sind dabei elementar.
\begin{definition}\label{def:subsets}
	Seien $M, N$ zwei Mengen. $N$ ist eine \textit{Teilmenge} von $M$ ($N \subseteq M$) bzw. $M$ eine \textit{Obermenge} von $N$ ($M \supseteq N$), wenn für alle $x \in N$ gilt, dass auch $x \in M$.\\
	Wir sagen $N$ ist eine \textit{echte Teilmenge} von $M$ ($N \subset M$) bzw. $M$ eine \textit{echte Obermenge} von $N$ ($M \supset N$), wenn es zusätzlich ein $y \in M$ gibt mit $y \notin N$.\\
	$M$ und $N$ sind \textit{gleich} ($M = N$), wenn sowohl $M \subseteq N$ als auch $N \subseteq M$ gilt.
\end{definition}
Wir kommen nun zum Mächtigkeitsbegriff der Mengenlehre, der für die Anzahl der Elemente einer Menge beschreibt.
\begin{definition}
	Sei $M$ eine Menge. $M$ heißt \textit{endlich}, wenn $M$ nur endlich viele Elemente besitzt, dann beschreibt $|M|$ die Anzahl der Elemente von $M$. Andernfalls heißt $M$ \textit{unendlich} und wir schreiben $|M| = \infty$. Man nennt $|M|$ die \textit{Mächtigkeit} von $M$.
\end{definition}
Um eine konkrete Menge zu zu benennen gibt es im Wesentlichen vier verschiedene Möglichkeiten:
\begin{enumerate}
	\item \textit{Aufzählen.} Die Elemente der Menge werden aufgelistet und in Mengenklammern ($\{, \}$) eingeschlossen. Reihenfolge und Wiederholungen spielen keine Rolle.
		$$
			\{ 3, 4.5, \pi, \diamondsuit \} = \{ \pi, 4.5, \diamondsuit, \diamondsuit, 3 \} \subseteq \{ \diamondsuit, \pi, 4.5, 3, \clubsuit \}. 
		$$
	\item \textit{Beschreiben.} Mengen können durch Worte beschrieben werden.
		$$
			\text{Menge der natürlichen Zahlen} = \{ 0, 1, 2, 3, \ldots \} \eqqcolon \mathbb{N}.
		$$
		Aber Achtung: Natürliche Sprache neigt zu Uneindeutigkeit!
	\item \textit{Aussondern.} Sei $M$ eine Menge, dann ist
		$$
			\{ x \in M \mid \varphi(x) \}
		$$
		die Menge aller Elemente aus $M$, die die Eigenschaft $\varphi$ erfüllen. Zum Beispiel:
		$$
			\mathbb{P} \coloneqq \{ n \in \mathbb{N} \mid n \text{ hat genau zwei Teiler} \}
		$$
		als Menge aller Primzahlen.
	\item \textit{Abbilden.} Sei $M$ eine Menge und $f$ ein Ausdruck, der für jedes $x \in M$ definiert ist. Dann ist
		$$
			\{ f(x) : x \in M \}
		$$
		die Menge aller Ausdrücke $f(x)$, wobei jedes $x \in M$ in $f$ eingesetzt wird. Zum Beispiel:
		$$
			\{ n^2 : n \in \mathbb{N} \}
		$$
		als Menge aller Quadtratzahlen.
\end{enumerate}
Wir können Abbilden und Aussondern auch kombinieren, zum Beispiel mit:
$$
	\{ n^2 : n \in \mathbb{N} \mid n \text{ ungerade} \}
$$
als Menge aller Quadratzahlen von ungeraden natürlichen Zahlen. Man würde hier jedoch abkürzend schreiben:
$$
	\{ n^2 : n \in \mathbb{N} \text{ ungerade} \}.
$$
Mit der Definition, dass eine Zahl $n$ ungerade ist, wenn ein $k$ existiert mit $2k+1 = n$ (und umgekehrt $n$ gerade, wenn ein $k$ exisitiert mit $n = 2k$) könnten wir auch schreiben:
$$
	\{ n^2 : n \in 2\mathbb{N}+1 \}.
$$
Eine wichtige Menge haben wir bisher außen vor gelassen: die \textit{leere Menge}. Wir schreiben $\emptyset \coloneqq \{ \}$. Gelegentlich verwenden wir außerdem folgende Notation, wenn wir nur eine endliche geordnete Menge benötigen: $[\ell] \coloneqq \{ 0, 1, \ldots, \ell-1 \}$. Ein-elementige Mengen (z.B. $[1] = \{ 0 \}$) nennt man auch \textit{Singleton}.
\begin{remark*}
	Im Allgemeinen gibt es keine Elemente in Mengen, die mehrfach vorkommen. Man kann dies explizit zulassen, durch \textit{Multimengen}, z.B.: $\{^\ast 1, 1, 2, \pi, \clubsuit, \pi ^\ast\}$.
\end{remark*}


\subsection{Operationen auf Mengen}\label{sec:pre_setops}
Im folgendem betrachten wir Mengen immer als Teilmenge eines \textit{Universums} (oder auch \textit{Grundemenge}) $\mathcal{U}$. In der Analysis ist das typischerweise die Menge der reellen Zahlen $\mathbb{R}$ (oder die Menge der komplexen Zahlen $\mathbb{C}$), die betrachteten Teilmengen sind oftmals Intervalle in denen zum Beispiel Funktionen auf Stetigkeit hin untersucht werden. Vorweg: Wir betrachten später im Allgemeinen das Universum $\Sigma^\ast$ und Sprachen als Teilmenge von eben diesem. Genaueres folgt im nächsten Kapitel.\\
Um die Operatoren auf den Mengen zu veranschaulichen gibt es die sogenannten \textit{Venn-Diagramme}, bei denen Kreise oder Ellipsen die Mengen visualisieren. In Abbildung~\ref{fig:venn_subset} finden wir zum Beispiel für die Inklusion ($\subseteq$) ein entsprechendes Diagramm.
\begin{figure}
	\centering
	\input{figs/venn_subset}
	\caption{Venn-Diagramm für $A \subseteq B$.}
	\label{fig:venn_subset}
\end{figure}
Wir definieren nun einige Operationen auf Mengen ähnlich wie Addition und Multiplikation usw. auf Zahlen. Zusätzlich zur formalen Definition befindet sich in Abbildung~\ref{fig:venn} auch ein passendes Venn-Diagramm. Die jeweils eingefärbte Fläche kennzeichnet die resultierende Menge. $\mathcal{U}$ sei ein beliebiges aber festes Universum.
\begin{definition}\label{def:setops}
	Seien $A, B$ Mengen. 
	\begin{enumerate}[label=(\alph*)]
		\item Die \textit{Vereinigung} von $A$ und $B$ ist definiert als 
			$$ 
				A \cup B \coloneqq \{ a \in \mathcal{U} \mid a \in A \text{ oder } a \in B \}.
			$$
			Für endliche und unendliche Vereinigungen (z.B. gegeben durch eine Indexmenge $I = \{ 0, 1, \ldots \}$) schreiben wir abkürzend
			$$
				\bigcup_{i \in I} A_i = A_0 \cup A_1 \cup \ldots
			$$
		\item Der \textit{Schnitt} von $A$ und $B$ ist definiert als
			$$
				A \cap B \coloneqq \{ a \in A \mid a \in B \}.
			$$
			Für endlichen und unendlichen Schnitt (z.B. gegeben durch eine Indexmenge $I = \{ 0, 1, \ldots \}$) schreiben wir abkürzend
			$$
				\bigcap_{i \in I} A_i = A_0 \cap A_1 \cap \ldots
			$$
		\item Das \textit{Komplement} von $A$ is definiert als
			$$
				\comp{A} \coloneqq \{ a \in \mathcal{U} \mid a \notin A \}.
			$$
		\item Die \textit{Differenz} (auch: \textit{relatives Komplement}) von $A$ und $B$ ist definiert als
			$$
				A \setminus B \coloneqq A \cap \comp{B}.
			$$
		\item Das \textit{kartesische Produkt} zwischen $A$ und $B$ ist definiert als
			$$
				A \times B \coloneqq \{(a, b) : a \in A, b \in B \}.
			$$
			Für ein endliches Produkt einer Menge $A$ auf sich selbst schreiben wir abkürzend
			$$
				A^k \coloneqq A \times A^{k-1}, \quad\quad A^1 \coloneqq A, \quad\quad A^0 \coloneqq \{ \bullet \},
			$$
			wobei $\bullet$ ein beliebiges Platzhaltersymbol ist, d.h. $A^0$ ist für jedes $A$ ein Singleton.\\
			Die Elemente eines kartesischen Produkts $(x_1, \ldots, x_k)$ heißen $k$-\textit{Tupel}. Für $k = 2, 3, 4, \ldots$ kann man auch \textit{Paar, Tripel, Quadrupel, \ldots} sagen.
		\item Die \textit{Potenzmenge} von $A$ ist definiert als
			$$
				2^A \coloneqq \{ M \subseteq \mathcal{U} \mid M \subseteq A \}.
			$$
	\end{enumerate}
\end{definition}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/venn_union}
		\caption{$A \cup B$}
		\label{fig:venn_union}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/venn_intersection}
		\caption{$A \cap B$}
		\label{fig:venn_intersection}
	\end{subfigure}\\
	\ \\
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/venn_complement}
		\caption{$\comp{A}$}
		\label{fig:venn_complement}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/venn_setminus}
		\caption{$A \setminus B$}
		\label{fig:venn_setminus}
	\end{subfigure}
	\caption{Venn-Diagramme für Mengen-Operationen.}
	\label{fig:venn}
\end{figure}


\subsection{Relationen}\label{sec:pre_relations}
Relationen drücken Beziehungen oder Zusammenhänge zwischen Elementen aus. Im Allgemeinen können dies Beziehungen zwischen beliebig vielen Elementen sein und wir werden verschieden stellige Relationen auch im Laufe der Vorlesung benutzen. In diesem Abschnitt legen wir aber ein besonderes Augenmerk auf 2-stellige Relationen.
\begin{definition}
	Es seien $M_1, \ldots, M_k$ nicht-leere Mengen. Eine Teilmenge $R \subseteq M_1 \times \ldots \times M_k$ heißt \textit{Relation} zwischen $M_1, \ldots, M_k$ (oder auf $M$, falls $M = M_1 = \ldots = M_k$).
\end{definition}
Für 2-stellige Relationen verwenden wir oft Symbole wie $\sim, \prec$ und schreiben dann statt $(a, b) \in\, \sim$ intuitiver $a \sim b$.
\begin{definition}
	Sei $\sim \,\subseteq M \times M$ eine 2-stellige Relation. $\sim$ heißt
	\begin{itemize}
		\item \textit{reflexiv}, falls $x \sim x$ für alle $x \in M$,
		\item \textit{symmetrisch}, falls für alle $x, y \in M$ mit $x \sim y$ auch $y \sim x$ gilt,
		\item \textit{antisymmetrisch}, falls für alle $x, y \in M$ mit $x \sim y$ und $y \sim x$ gilt, dass $x = y$,
		\item \textit{transitiv}, falls für alle $x, y, z \in M$ mit $x \sim y$ und $y \sim z$ gilt, dass $x \sim z$.
	\end{itemize}
\end{definition}
Wir klassifizieren außerdem 2-stellige Relationen, falls sie bestimmte Eigenschaften haben.
\begin{definition}\label{def:relations}
	Sei $\sim \,\subseteq M \times M$ eine 2-stellige Relation. $\sim$ heißt
	\begin{itemize}
		\item \textit{Äquivalenzrelation}, falls sie reflexiv, symmetrisch und transitiv ist,
		\item \textit{(partielle) Ordnung}, falls sie reflexiv, antisymmetrisch und transitiv ist,
		\item \textit{Totalordnung}, falls sie partielle Ordnung ist und für alle $x, y \in M$ entweder $x \sim y$ oder $y \sim x$ gilt. 
	\end{itemize}	 
\end{definition}
\begin{example*}
	\
	\begin{enumerate}
		\item Die Relation $\leq$ ist auf $\mathbb{N}$ eine Totalordnung.
		\item Die Relation $\{(a, b) \in \mathbb{R}^2 \mid a^2 = b^2 \}$ ist eine Äquivalenzrelation auf $\mathbb{R}$.
	\end{enumerate}
\end{example*}
\begin{definition}\label{def:equivalenceclass}
	Sei $\sim$ eine Äquivalenzrelation auf einer Menge $M$. Für $x \in M$ heißt
	$$
		[x]_\sim \coloneqq x /_\sim \coloneqq \{ y \in M \mid x \sim y \}
	$$
	die \textit{Äquivalenzklasse} von $x$. Die Menge aller Äquivalenzklassen von $\sim$ wird notiert mit $M /_\sim \coloneqq \{ [x]_\sim : x \in M \}$. Der \textit{Index} einer Äquivalenrelation ($\ind(\sim)$) ist die Anzahl ihrer Äquivalenzklassen (möglicherweise $\infty$).
\end{definition}
\begin{definition}
	Sei $A$ eine Menge. Eine \textit{Partition} von $A$ ist eine Menge $\mathcal{P} \subset 2^A \setminus \{ \emptyset \}$, sodass
	\begin{enumerate}
		\item $P \cap Q = \emptyset$ für alle $P, Q \in \mathcal{P}$ mit $P \neq Q$,
		\item $\bigcup_{P \in \mathcal{P}} P = A$.
	\end{enumerate}
	Die Mengen $P \in \mathcal{P}$ heißen \textit{Teile} oder \textit{Klassen} der Partition $\mathcal{P}$.
\end{definition}
\begin{remark*}
	Sei $A$ eine Menge.
	\begin{enumerate}
		\item Ist $\sim$ eine Äquivalenzrelation auf $A$, so ist $\{[x]_\sim : x \in A\}$ eine Partition von $A$.
		\item Ist $\mathcal{P}$ eine Partition auf $A$, so ist $\sim_\mathcal{P}$ mit
			$$
				x \sim_\mathcal{P} y \text{ g.d.w. } x, y \in P  \text{ für ein } P \in \mathcal{P}
			$$
			eine Äquivalenzrelation auf $A$ und es gilt $\mathcal{P} = \{[x]_{\sim_\mathcal{P}} : x \in A\}$.
	\end{enumerate}
\end{remark*}


\subsection{Gesetze für Mengen}\label{sec:pre_setlaws}
In diesem Abschnitt sammeln wir ein paar Gesetzmäßigkeiten, die für Mengen gelten. Manche davon sind offensichtlich, wir werden aber auch zu ein paar Aussagen die Beweise geben, manche bleiben als Übung.
\begin{remark*}
	Für die Inklusion gilt offensichtlich für jede Menge $M$
	$$
		\emptyset \subseteq M \subseteq M \subseteq \mathcal{U}.
	$$
	Insbesondere ist die Relation $\subseteq$ reflexiv. Sie ist außerdem transitiv und per Definition der Gleichheit von Mengen antisymmetrisch, also eine partielle Ordnung.\par
	Schnitt und Vereinigung sind per Definition offenbar \textit{assoziativ} (d.h. $A \cup (B \cup C) = (A \cup B) \cup C$ und $A \cap (B \cap C) = (A \cap B) \cap C$) und \textit{kommutativ} (d.h. $A \cup B = B \cup A$ und $A \cap B = B \cap A$). Außerdem sind diese beiden Operationen zueinander \textit{distributiv}, was wir im folgenden einmal zeigen wollen. 
\end{remark*}
Das Beweisschema für solche Aufgaben ist stets das selbe und sollte deshalb auch ruhig übernommen werden für Übungsaufgaben. Tricks sind selten notwendig, es ist meist
\begin{center}
	\textit{Definition anwenden -- triviale Umformung -- Definition anwenden -- Profit}.
\end{center}
\begin{theorem}[Distributivgesetz]
	Für Mengen $A, B, C$ gilt:
	\begin{enumerate}
		\item $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$,
		\item $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$.
	\end{enumerate}
	\begin{proof}
		Wir zeigen nur Aussage (i), die zweite Hälfte geht analog. Wir müssen zwei Richtungen beweisen.\\
		``$\subseteq$``: Sei $a \in A \cup (B \cap C)$. D.h. $a \in A$ oder $a \in B \cap C$.
		\begin{itemize}
			\item $a \in A$. Dann ist $a$ auch in $A \cup B$ und $A \cup C$ (da $\cup$ die Mengen nicht verkleinert). Also ist $a$ auch im Schnitt dieser beiden Mengen.
			\item $a \in B \cap C$. Dann ist $a \in B$ und $a \in C$. Also (da wie oben $\cup$ die Menge nicht verkleinert) ist $a \in A \cup B$ und $a \in A \cup C$. Somit ist $a$ auch wieder im Schnitt beider Mengen.
		\end{itemize}
		``$\supseteq$``: Sei $a \in (A \cup B) \cap (A \cup C)$. Dann ist $a \in A \cup B$ und $a \in A \cup C$ ($\ast$). Wir unterscheiden zwei Fälle:
		\begin{itemize}
			\item $a \in A$. Unabhängig von $B, C$ ist dann $a \in A \cup (B \cap C)$.
			\item $a \notin A$. Dann muss $a \in B$ und $a \in C$ gelten, sonst würde ($\ast$) nicht gelten. Somit ist $a \in B \cap C$ und damit auch wieder $a \in A \cup (B \cap C)$. 
		\end{itemize}
		Wir haben also $A \cup (B \cap C) \subseteq (A \cup B) \cap (A \cup C)$ und $A \cup (B \cap C) \supseteq (A \cup B) \cap (A \cup C)$ gezeigt. Somit muss Gleichheit zwischen diesen beiden Mengen vorliegen.
	\end{proof}
\end{theorem}
Weiterhin nützlich sind noch folgende Bemerkungen.
\begin{theorem}[DeMorgan'sche Gesetze]
	Für Mengen $A, B$ gilt:
	\begin{itemize}
		\item $\comp{(A \cup B)} = \comp{A} \cap \comp{B}$,
		\item $\comp{(A \cap B)} = \comp{A} \cup \comp{B}$.
	\end{itemize}
	\begin{proof}
		Jeder Mensch sollte mal einen solchen Beweis selbst geführt haben, deshalb bleibt dieser hier als Übung.
	\end{proof}
\end{theorem}
\begin{theorem}[Absorptionsgesetz]
	Für Mengen $A, B$ gilt:
	\begin{itemize}
		\item $A \cup (A \cap B) = A$,
		\item $A \cap (A \cup B) = A$.
	\end{itemize}
	\begin{proof}
		Wir überlassen diesen Beweis dem eifrigen Kopf, der dieses Skript liest.
	\end{proof}
\end{theorem}


\subsection{Funktionen}\label{sec:pre_mappings}
\begin{definition}
	Seien $M, N$ Mengen. Eine \textit{Funktion} (oder \textit{Abbildung)} $f$ von $M$ nach $N$ ist eine Vorschrift (z.B. eine Formel), die jedem $x \in M$ genau ein $f(x) \in N$ zuordnet. Wir schreiben dazu
	$$
		f\colon M \to N, \quad x \mapsto f(x).
	$$
	$M$ heißt der \textit{Definitionsbereich} (auch Domäne) von $f$, $N$ heißt der \textit{Wertebereich} von $f$. $f(x)$ ist das \textit{Bild} von $x$ unter $f$ und $x$ ist ein \textit{Urbild} von $f(x)$ unter $f$. Die Menge aller Funktionen von $M$ nach $N$ wird mit $N^M$ bezeichnet. Falls $M = A^n, N = A$ für ein nicht-leeres $A$ ist, sagen wir auch, dass $f$ eine \textit{$n$-stellige} Funktion über $A$ ist. Desweitere nennen wir eine $0$-stellige Funktion auch \textit{Konstante}.
\end{definition}
\begin{definition}
	Sei $f\colon M \to N$ eine Funktion.
	\begin{itemize}
		\item Für jede Teilmenge $X \subseteq M$ ist $f[X] \coloneqq \{ f(x) : x \in X \}$ das \textit{Bild von $X$ unter $f$}. Falls $X = M$ sprechen wir auch nur vom \textit{Bild} von $f$.
		\item Für jede Teilmenge $Y \subseteq N$ ist $f^{-1}[Y] \coloneqq \{ x \in M \mid f(x) \in Y \}$ das \textit{Urbild von $Y$ unter $f$}.
	\end{itemize}
\end{definition}
\begin{definition}
	Zu einer Menge $M$ ist $\id_M\colon M \to M, x \mapsto x$ die \textit{Identität}.
\end{definition}
\begin{definition}
	Eine Funktion $f\colon M \to N$ heißt
	\begin{itemize}
		\item \textit{surjektiv}, falls für alle $y \in N$ ein $x \in M$ mit $f(x) = y$ existiert,
		\item \textit{injektiv}, falls für alle $x, x^\prime \in M$ mit $x \neq x^\prime$ gilt, dass $f(x) \neq f(x^\prime)$,
		\item \textit{bijektiv}, falls $f$ surjektiv und injektiv ist.
	\end{itemize}
\end{definition}
\begin{example*}
	Die Addition zweier natürlicher Zahlen kann als Funktion aufgefasst werden:
	$$
		+\colon \mathbb{N}^2 \to \mathbb{N}, \quad (m, n) \mapsto m+n.
	$$
	$+$ ist surjektiv (jedes $y \in \mathbb{N}$ wird z.B. durch $(y, 0) \in \mathbb{N}^2$ getroffen), aber nicht injektiv ($1 \in \mathbb{N}$ wird sowohl von $(1, 0)$ als auch $(0, 1)$ getroffen).
\end{example*}
Für Funktionen $f\colon [k] \to M$ für beliebige $k \in \mathbb{N}$ in beliebige $M$ können wir auch abkürzend die Tupelschreibweise $(y_0, \ldots, y_{k-1})$ verwenden. Dann ist $y_i = f(i)$ für alle $i \in [k]$.\par
Wir fügen nun noch ein paar nützliche Definitionen an um dieses Skript in sich abgeschlossen zu halten.
\begin{definition}
	Sei $f\colon M \to N$ eine Funktion und $A \subseteq M$. Dann ist $f\vert_A\colon A \to N$ die \textit{Einschränkung von $f$ auf $A$} mit $f\vert_A(x) = f(x)$ für alle $x \in A$.
\end{definition}
\begin{definition}
	Wir schreiben für zwei Funktionen $f, g\colon M \to N$, dass sie \textit{identisch} sind ($f \equiv g$), wenn für alle $x \in M$ gilt, dass $f(x) = g(x)$.
\end{definition}
\begin{definition}
	Die \textit{Komposition} zweier Funktionen $f\colon M \to M^\prime, g\colon M^\prime \to N$ ist bezeichnet mit $g \circ f\colon M \to N, x \mapsto g(f(x))$.
\end{definition}
\begin{definition}
	Seien $f\colon M \to N, g\colon N \to M$ Funktionen. Dann heißt $g$ eine \textit{linksseitige (rechtseitige) Umkehrfunktion} von $f$, wenn $g \circ f \equiv id_M$ ($f \circ g \equiv id_N$). Wenn $g$ sowohl links- als auch rechtsseitige Umkehrfunktion ist sprechen wir schlicht von einer \textit{Umkehrfunktion} von $f$ und bezeichnen diese mit $f^{-1}$.
\end{definition}
\begin{remark*}
	Die Schreibweise $f^{-1}$ benutzen wir sowohl für das Urbild als auch für Umkehrabbildungen, was jedoch zwei verschiedene Begriffe sind!
\end{remark*}


\subsection{Strukturen}\label{sec:pre_structures}
Wir haben Mengen, Relationen und Abbildungen jeweils eigenständig kennengelernt. Tat\-säch\-lich benutzen wir sie allerdings nur zusammen. Beispielsweise sind die natürlichen Zahlen für sich allein nicht sehr interessant, wenn man auf ihnen nicht addieren könnte. Umgekehrt sind auch Funktionsvorschriften wertlos, wenn sie nicht auf Elemente einer Menge angewendet werden kann. Wir fassen alles nun unter dem Begriff einer \textit{Struktur} zusammen.
\begin{definition}
	Sei $\tau = \{ R_1, \ldots, R_m, f_1, \ldots, f_n \}$ eine Menge von Relationssymbolen und Funktionssymbolen, eine sogenannte \textit{Signatur}. Eine \textit{$\tau$-Struktur} ist ein Paar $\mathfrak{A} = (A, \mathfrak{a})$, wobei $A$ eine nicht-leere Menge ist, die \textit{Universum} (oder \textit{Träger}) heißt und $\mathfrak{a}$ ist eine Funktion, die jedem $k$-stelligem Relationssymbol $R \in \tau$ eine $k$-stellige Relation und jedem $k$-stelligem Funktionssymbol $f \in \tau$ eine $k$-stellige Funktion zuordnet.
\end{definition}
Statt $\mathfrak{a}(R), \mathfrak{a}(f)$ schreibt man auch häufig $R^\mathfrak{A}, f^\mathfrak{A}$. In der mathematischen Logik ist es wichtig eine Unterscheidung zwischen Relations- und Funktionssymbolen und ihren Interpretationen durch konkrete Relationen $R^\mathfrak{A}$ bzw. Funktionen $f^\mathfrak{A}$ zu machen. Deshalb sollten Schreibweisen wie $\mathfrak{N} = (\mathbb{N}, +)$ nur als Abkürzungen verstanden werden für $\mathfrak{N} = (\mathbb{N}, +^\mathfrak{N})$ mit $+^\mathfrak{N}\colon \mathbb{N}^2 \to \mathbb{N}, (m, n) \mapsto +^\mathfrak{N}(m, n) = m + n$, wobei wir mit $m + n$ den gewohnten $n$-ten Nachfolger von $m$ bezeichnen. Der Autor hofft, dass es klar ist worauf er hinaus wollte.\par
Wir können Strukturen wieder klassifizieren nach sogenannten \textit{Axiomen}, die sie erfüllen.
\begin{definition}
	Eine Struktur vom Typ $(A, \bullet)$ heißt \textit{Magma}. Ein Magma $(A, \bullet)$ heißt \textit{abelsch} (oder \textit{kommutativ)}, falls für alle $a, b \in A$ gilt: $a \bullet b = b \bullet a$. Es heißt \textit{assoziativ} (oder \textit{Halbgruppe}) falls für alle $a, b, c \in A$ gilt: $(a \bullet b) \bullet c = a \bullet (b \bullet c)$.
\end{definition}
\begin{definition}
	Sei $\mathfrak{A} = (A, \bullet)$ eine Struktur mit $\bullet\colon A \times A \to A$. Wir sagen $\mathfrak{A}$ ist ein \textit{Monoid}, wenn folgende Axiome gelten:
	\begin{enumerate}
		\item \textit{Assoziativität}, für alle $a, b, c \in A$ gilt $(x \bullet y) \bullet z = x \bullet (y \bullet z)$.
		\item \textit{neutrales Element}, es gibt ein $e \in A$, sodass für alle $a \in A$ gilt $a \bullet e = e \bullet a = a$.
	\end{enumerate}
	Gilt zusätzlich
	\begin{enumerate}\setcounter{enumi}{3}
		\item \textit{Kommutativität}, für alle $a, b \in A$ gilt $a \bullet b = b \bullet a$.
	\end{enumerate}
	so heißt das Monoid \textit{abelsch}.
\end{definition}
\begin{example}
	Sei $A$ eine beliebige nicht-leere Menge und $\mathbb{B} = \{ 0, 1 \}$.
	\begin{enumerate}
		\item $(\mathbb{N}, +)$ ist abelsches Monoid mit neutralem Element $0$,
		\item $(\mathbb{R}, \cdot)$ ist abelsches Monoid mit neutralem Element $1$,
		\item $(A^A, \circ)$ ist Monoid mit neutralem Element $\id_A$,
		\item $(\mathbb{B}, \wedge)$ ist abelsches Monoid mit neutralem Element $1$.
	\end{enumerate}
\end{example}
Es ist bei Monoiden auch üblich, dass das neutrale Element mit in die Struktur geschrieben wird also, z.B. $(\mathbb{R}, \cdot, 1)$.
\begin{definition}
	Sei $\mathfrak{A} = (A, \bullet)$ ein Monoid. Eine Teilmenge $E \subseteq A$ heißt \textit{Erzeugendensystem} von $\mathfrak{A}$ falls jedes $a \in A$ als $a = e_0 \bullet \ldots \bullet e_{n-1}$ mit $e_i \in E$ für alle $i \in [n]$ dargestellt werden kann. $E$ heißt \textit{frei}, falls diese Darstellung eindeutig ist. In diesem Fall nennen wir $\mathfrak{A}$ auch das von $E$ \textit{frei erzeugte Monoid} (oder unter Missbrauch der Notation auch \textit{freies Monoid}).
\end{definition}
\begin{remark*}
	Das neutrale Element wird stets durch eine \textit{leere} Anwendung dargestellt. Beispielsweise ist für $(\mathbb{N}, +)$ die Menge $\{1\}$ ein (freies) Erzeugendensystem, denn jede natürliche Zahl $n$ lässt sich als Summe von $n$ $1$en darstellen $n = \sum_{i=1}^n 1$, für $n = 0$ erhalten wir eben genau die leere Summe ohne Summanden.
\end{remark*}
\begin{definition}
	Sei $(A, \bullet, e)$ Monoid und $a \in A$.
	\begin{itemize}
		\item Gibt es ein $b \in A$ mit $a \bullet b = e$ so heißt $a$ \textit{rechtsinvertierbar}.
		\item Gibt es ein $b \in A$ mit $b \bullet a = e$ so heißt $a$ \textit{linksinvertierbar}.
		\item Ist $a$ rechts- und linksinvertierbar, dann heißt $a$ eine \textit{Einheit}.
		\item Gibt es ein $b \in A$ mit $b \bullet a = a \bullet b = e$ so heißt $a$ \textit{invertierbar} und $b$ \textit{invers} zu $a$. 
	\end{itemize}
	Die \textit{Menge aller Einheiten} von $A$ bezeichnen wir mit $A^\times$.
\end{definition}
Wir bezeichnen die Inversen zu Einheiten $a$ in der Regel mit $a^{-1}$ oder $-a$. 
\begin{definition}
	Sei $\mathfrak{A} = (A, \bullet, e)$ ein Monoid. Falls in $\mathfrak{A}$ zusätzlich
	\begin{enumerate}\setcounter{enumi}{2}
		\item \textit{Invertierbarkeit}, für alle $a \in A$ existiert $b \in A$ mit $a \bullet b = b \bullet a = e$, 
	\end{enumerate}
	dann ist $\mathfrak{A}$ eine \textit{Gruppe} (bzw. \textit{abelsche Gruppe}).
\end{definition}
\begin{lemma}
	Ist $(A, \bullet, e)$ Monoid, so ist $(A^\times, \bullet, e)$ Gruppe (Einheitengruppe).
	\begin{proof}
		Per Definition ist jedes Element in $A^\times$ eine Einheit. Außerdem ist $e$ stets eine Einheit, da es invers zu sich selbst ist. Es bleibt zu zeigen, dass $A^\times$ auch abgeschlossen ist unter der Operation $\bullet$. Sei dazu $a, b \in A^\times$ und $a^{-1}, b^{-1}$ die jeweiligen Inversen $(\ast)$. Wir zeigen das $a \bullet b$ Einheit ist mit Inversem $b^{-1} \bullet a^{-1}$:
		\[
			(a \bullet b) \bullet (b^{-1} \bullet a^{-1}) \overset{\text{(i)}}{=} a \bullet (b \bullet b^{-1}) \bullet a^{-1} \overset{(\ast)}{=} a \bullet e \bullet a^{-1} \overset{\text{(ii)}}{=} a \bullet a^{-1} \overset{(\ast)}{=} e. \qedhere
		\]
	\end{proof}
\end{lemma}
\begin{example}
	Sei $\mathbb{B} = \{ 0, 1 \}$. Das Symbol $\nleftrightarrow$ steht für das Boolsche exklusive Oder ($\xor$).
	\begin{enumerate}
		\item $(\mathbb{B}, \nleftrightarrow)$ ist abelsche Gruppe,
		\item $(\mathbb{R} \setminus \{ 0 \}, \cdot)$ ist abelsche Gruppe,
		\item $(\mathbb{Z}, -)$ ist Gruppe.
	\end{enumerate}
\end{example}
\begin{definition}
	Sei $\mathfrak{A} = (A, \bullet)$ eine Gruppe und $B \subseteq A$ nicht-leer. $\mathfrak{B} = (B, \bullet)$ ist eine \textit{Untergruppe} von $\mathfrak{A}$, wenn für alle $a, b \in B$ auch $a \bullet b^{-1} \in B$ gilt.
\end{definition}
Natürlich ist es auch möglich eine solche Klassifizierung von Strukturen mit mehr als einer Funktion vorzunehmen.
\begin{definition}
	Eine Struktur $(A, \oplus, \odot)$ heißt \textit{Ring}, falls gilt
	\begin{enumerate}
		\item $(A, \oplus)$ ist abelsche Gruppe,
		\item $(A, \odot)$ ist Halbgruppe,
		\item \textit{Distributivität}, für alle $a, b, c \in A$ gilt $a \odot (b \oplus c) = a \odot b \oplus a \odot c$ und $(a \oplus b) \odot c = a \odot c \oplus b \odot c$.
	\end{enumerate}
	Das neutrale Element von $(A, \oplus)$ heißt \textit{Nullelement} des Rings. Der Ring heißt \textit{kommutativ}, falls er bzgl. $\odot$ kommutativ ist, ansonsten \textit{nicht-kommutativ}. Ist $(A, \odot)$ Monoid, so heißt der Ring \textit{unitär}. Das neutrale Element von $(A, \odot)$ heißt dann \textit{Einselement}. Ist $(A, \oplus)$ nur eine abelsche Halbgruppe, so heißt $(A, \oplus, \odot)$ \textit{Halbring}. Ist $( A, \oplus)$ Halbgruppe, aber $(A, \odot)$ Gruppe, so heißt $(A, \oplus, \odot)$ \textit{Halbkörper}.
\end{definition}
\begin{definition}
	Eine Struktur $(A, \oplus, \odot)$ heißt \textit{Körper}, wenn gilt
	\begin{enumerate}
		\item $(A, \oplus)$ ist abelsche Gruppe,
		\item $(A \setminus \{0\}, \odot)$ ist abelsche Gruppe (mit $0$ neutralem Element bzgl. $\oplus$),
		\item Distributivität ist erfüllt.
	\end{enumerate}
	Ist $(A \setminus \{0\}, \odot)$ nur Gruppe (ohne Kommutativität), so ist $(A, \oplus, \odot)$ \textit{Schiefkörper}.
\end{definition}
\begin{remark*}
	Jeder Körper ist ein kommutativer Ring und auch ein Schiefkörper. Jeder Schiefkörper ist ein Ring.
\end{remark*}


\subsection{Graphen}\label{sec:pre_graphs}
Eine weitere wichtige Klasse von Strukturen sind Graphen. Dise werden häufig genutzt um Operatoren wie Funktionen und Relationen zu visualisieren, allerdings untersucht man Graphen auch selbst als mathematische Objekte.
\begin{definition}
	Ein \textit{Graph} ist eine Struktur $G = (V, E)$ mit \textit{Knotenmenge} $V$ und \textit{Kantenmenge} (oder Multimenge) $E \subseteq V \times V$.
\end{definition}
\begin{definition}
	Sei $G = (V, E)$ ein Graph, $v, w \in V$ und $e = (v, w) \in E$.
	\begin{itemize}
		\item $v, w$ heißen \textit{adjazent} (oder \textit{benachbart}) durch $e$.
		\item $e$ ist \textit{inzident} zu $v$ und $w$.
		\item Zwei Kanten heißen \textit{inzident}, wenn sie einen geimensamen Endknoten haben.
		\item $e$ heißt \textit{Schleife} (oder \textit{Loop}), falls $v = w$.
		\item Zwei Kanten $e, e^\prime$ heißen \textit{parallel}, falls $e = e^\prime$.
		\item Ein Graph heißt \textit{einfach}, falls er keine parallelen Kanten enthält.
		\item Ein Graph heißt \textit{ungerichtet}, falls $(v, w) \in E$ impliziert, dass $(w, v) \in E$, ansonsten \textit{gerichtet}. 
	\end{itemize}
\end{definition}
Für einen ungerichteten Graphen können wir auch $e = \{v, w\}$ schreiben (statt zwei Kanten $(v, w), (w, v)$ zu betrachten. Für einen fixierten Knoten $v$ bezeichnet man die Menge aller Nachbarn von $v$ häufig mit $\Gamma(v)$.
\begin{definition}
	Sei $G = (V, E)$ ein Graph und $v \in V$. Der \textit{Knotengrad} von $v$ ist definiert durch
	$$
		\deg(v) = \left| \{ e \in E \mid e = \{v, w\}, w \neq v \} \right|.
	$$
	Für gerichtete Graphen können wir auch vom \textit{Eingangs-} und \textit{Ausgangsgrad} sprechen:
	\begin{align*}
		\deg^-(v) &= \left| \{ e \in E \mid e = (v, w), w \neq v \} \right|,\\
		\deg^+(v) &= \left| \{ e \in E \mid e = (w, v), w \neq v \} \right|.
	\end{align*}
\end{definition}
\begin{definition}
	Ein Graph $G = (V, E)$ heißt $k$-\textit{regulär}, falls für alle $v \in V$ gilt $\deg(v) = k$.
	$G$ heißt \textit{vollständig}, falls für alle Knotenpaare $v \neq w \in V$ gilt: $(v, w) \in E$. Mit $K_n$ bezeichnen wir den vollständigen Graphen mit $n$ Knoten. Ist der Teilgraph $G[U]$ vollständig, so heißt $U \subseteq V$ \textit{Clique} in $G$.
	$G$ heißt \textit{bipartit}, falls sich $V$ als disjunkte Vereinigung aus $U, W$ darstellen lässt, sodass für alle $(u, v) \in E$ entweder $u \in U, v \in W$ oder $u \in W, v \in U$ gilt. Er heißt \textit{vollständig bipartit}, wenn zudem gilt, dass $E = \{ (u, w) \mid u \in U, w \in W \}$. Wir schreiben $K_{p,q}$ mit $p = |U|$ und $q = |W|$ für den vollständig bipartiten Graphen mit Knotenmenge $V = U \cup W, U \cap W = \emptyset$.
\end{definition}
\begin{remark*}
	Den Graphen $S_n \coloneqq K_{1,n-1}$ nennt man auch \textit{Stern}.
\end{remark*}
\begin{definition}
	Sei $G = (V, E)$ ein Graph. Ein \textit{Teilgraph} von $G$ ist ein Graph $G^\prime = (V^\prime, E^\prime)$, wenn gilt $V^\prime \subseteq V$ und $E^\prime \subseteq E$. Ein \textit{induzierter Teilgraph} von $G$ ist ein ein Graph $G^\prime = (V^\prime, E^\prime)$, wenn gilt $V^\prime \subseteq V$ und $E^\prime = E \cap V^\prime \times V^\prime$. Wir schreiben für den durch $V^\prime$ induzierten Teilgraphen von $G$ auch $G[V^\prime]$.
\end{definition}
\begin{definition}
	Sei $G = (V, E)$ ein Graph.
	\begin{itemize}
		\item Ein \textit{Kantenzug} der Länge $\ell$ ist ein Tupel $(v_0, v_1, \ldots, v_\ell)$ von Knoten mit $(v_i, v_{i+1}) \in E$ für alle $i \in [\ell]$.
		\item Ein Kantenzug $(v_0, \ldots, v_\ell)$ heißt \textit{Pfad}, falls die Knoten paarweise verschieden sind.
		\item Ein \textit{Kreis} ist ein Kantenzug $(v_0, \ldots, v_\ell)$, falls $\ell \geq 3$, $(v_0, \ldots, v_{\ell-1})$ ein Pfad ist und $v_0 = v_\ell$.
		\item Eine \textit{Tour} ist ein Kantenzug $(v_0, \ldots, v_\ell)$, falls die Kanten $(v_i, v_{i+1})$ für alle $i \in [\ell]$ paarweise verschieden sind und $v_0 = v_\ell$. 
	\end{itemize}
\end{definition}
Wir nutzen selbsterklärende, abkürzende Schreibweisen, wie $(u, v)$-Kantenzug und Bezeichnungen wie Anfangs- oder Endknoten bzgl. Kantenzügen oder Pfaden.
\begin{definition}
	Sei $G = (V, E)$ ein Graph. $G$ heißt \textit{zusammenhängend}, falls für alle $u, v \in V$ gilt, dass es einen $(u, v)$-Kantenzug gibt. Ein zusammenhängender Teilgraph $G[U]$ heißt \textit{Zusammenhangskomponente} von $G$, falls für alle $v \notin U$ der Graph $G[U \cup \{v\}]$ nicht zusammenhängend ist.
\end{definition}
\begin{definition}
	Ein kreisfreier Graph heißt \textit{Wald}. Ein zusammenhängender Wald heißt \textit{Baum}. Die Knoten eines Waldes mit $\deg(v) \leq 1$ heißen \textit{Blätter}. In einem gerichteten Baum heißt ein ausgezeichneter Knoten $r \in V$ \textit{Wurzel}, wenn $\deg^+(r) = 0$.
\end{definition}


\subsection{Beweismethoden}\label{sec:pre_proofs}
Wir sind fast am Ende des Einführungsabschnitts angekommen. Bekanntlich wollen wir in der Mathematik Sätze, d.h. mathematische Eigenschaften auf verschiedenen Strukturen beweisen. Ein Beweis ist dabei eine endliche Folge logischer Schlüsse. Die mathematische Logik gibt dazu genaue Einblicke wie Beweissysteme funktionieren. Im Wesentlichen geht es darum mit einer möglichst kleinen Menge von \textit{Axiomen} (das sind Aussagen, die als wahr angenommen werden, zum Beispiel fordert man, dass jede natürliche Zahl einen Nachfolger hat) Schritt für Schritt die Sätze herzuleiten. Genauer werden wir darauf aber nicht eingehen, wir verwenden vielmehr die Verfahren, die sich Logiker über die Jahrhunderte überlegt haben. Wir unterscheiden zwischen vier verschiedenen Beweisarten: Deduktion, Widerspruchsbeweis, Beweis der Kontraposition und Beweis durch vollständige Induktion. Dabei sei gesagt, dass diese Abgrenzung nicht immer eindeutig ist. Diese vier Methoden lassen sich (zum Beispiel in Beweisen größerer Theoreme) auch kombinieren. Gelegentlich liefern Beweise auch eine Konstruktion mit dem die Aussage klar wird.
Wir starten mit der Deduktion oder dem direktem Beweis. Wir betrachten dafür Aussagen der Form: 
\begin{center}
	\textit{Wenn Aussagen A gilt, dann auch Aussage B.}
\end{center}
Zum Beweis solcher Aussagen nehmen wir an, dass $A$ tatsächlich wahr ist und verwenden sogenannte \textit{Schlussregeln}, dass dann auch $B$ wahr sein muss. Wir zeigen dies an einem Beispiel.
\begin{theorem}
	Seien $a, b \in \mathbb{Z}$ Falls $a$ gerade ist, so ist auch $ab$ gerade.
	\begin{proof}
		Da $a$ gerade ist existiert ein $k \in \mathbb{Z}$ mit $2 \cdot k = a$. Dann ist aber $a \cdot b = (2 \cdot k) \cdot b = 2 \cdot \underbrace{(k \cdot b)}_{\in \mathbb{Z}}$ und somit ist $a \cdot b$ gerade.
	\end{proof}
\end{theorem}
Der zweite Weg, der Widerspruchsbeweis, ist ein indirekter Beweis (wie auch die Kontraposition später). Das Schema diesmal ist
\begin{center}
	\textit{Angenommen Aussage A wäre falsch, dann folgt Unsinn.}
\end{center}
Im Lateinischen spricht man auch von \textit{reductio ad absurdum} (Zurückführung auf das Sinnlose). Man zeigt damit, dass wenn $A$ nicht gilt, dass dann eine bereits andere Aussage (von der man den Wahrheitsgehalt kennt) nicht stimmen könnte. Mit diesem Widerspruch kann man dann folgern, dass $A$ doch wahr ist. Wir machen wieder ein Beispiel.
\begin{theorem}\label{thm:primes}
	Es gibt unendlich viele Primzahlen.
	\begin{proof}
		Angenommen es gäbe nur endlich viele Primzahlen. Sei dann $P \coloneqq \{p_0, \ldots, p_\ell\}$ die Menge aller vielen Primzahlen, d.h. $\ell \in \mathbb{N}$. Wir definieren nun die Zahl 
		$$
			q \coloneqq 1 + \prod_{i=0}^\ell p_i.
		$$
		Wir untersuchen zwei Fälle:
		\begin{itemize}
			\item $q$ prim. Dann war $P$ jedoch noch nicht die Menge aller Primzahlen wie oben agenommen, denn $q > p$ für alle $p \in P$. $\lightning$
			\item $q$ nicht prim. Dann existiert eine eindeutige Primfaktorzerlegung von $q$. Jedoch lässt jede Primzahl $p \in P$ einen Rest von $1$ bei ganzzahliger Division. Also muss es noch mehr Primzahlen geben als die in $P$. $\lightning$
		\end{itemize}
		In beiden Fällen erhalten wir also einen Widerspruch zur ursprünglichen Aussage, dass ein endliches $P$ alle Primzahlen enthalten würde. Also muss es unendlich viele Primzahlen geben.
	\end{proof}
\end{theorem}
Eine weitere Form des indirekten Beweises ist es die \textit{Kontraposition} der Aussage zu zeigen. D.h. statt wie beim direkten Beweis aus Aussage $A$ Aussage $B$ zu folgern zeigt man, dass aus der Negation von $B$ die Negation von $A$ folgt.
\begin{theorem}
	Sei $a^2$ gerade, dann ist auch $a$ gerade.
	\begin{proof}
		Angenommen $a$ wäre ungerade, d.h. es gibt ein $k \in \mathbb{Z}$ mit $a = 2k + 1$. Betrachte dann
		\begin{align*}
			a^2 &= (2k + 1)^2\\
			&= 4k^2 + 4k + 1\\
			&= 2\underbrace{(2k^2 + 2k)}_{\in \mathbb{Z}} + 1.
		\end{align*}
		Also ist $a^2$ ebenfalls ungerade. Es folgt vermöge Kontraposition, dass für gerade $a^2$ auch $a$ gerade ist.
	\end{proof}
\end{theorem}
Zuletzt betrachten wir das Beweisprinzip der vollständigen Induktion. Hiermit lassen sich leicht Aussagen für natürliche Zahlen zeigen. Man zeigt dafür zunächst, dass eine Aussage für einen Basisfall erfüllt ist (in dieser Vorlesung gewöhnlich für $n = 0$). Danach beweist man, dass die Aussage, sofern sie für ein $n \in \mathbb{N}$ erfüllt ist, dass sie dann auch für $n+1$ erfüllt ist. Dieses Beweisschema beruht auf dem Induktionsprinzip der natürlichen Zahlen, welches besagt: Für jede Teilmenge $M \subseteq \mathbb{N}$ gilt: Falls $0 \in M$ und für jedes $n \in M$ auch $n+1 \in M$, dann ist $M = \mathbb{N}$. Wollen wir also eine Aussage $A$ für alle $n \in \mathbb{N}$ beweisen, zeigen wir also lediglich, dass die Menge $M \coloneqq \{n \in \mathbb{N} \mid A \text{ ist wahr für } n\}$ gleich $\mathbb{N}$ ist. Wir starten jeweils mit einer \textit{Induktionsverankerung} (oder \textit{Induktionanfang}), die für einen Basisfall die Aussage $A$ beweist (z.B. für $A(0)$). Mit der \textit{Induktionshypothese} (oder \textit{Induktionannahme}, \textit{Induktionsvoraussetzung}) nehmen wir an, dass die Aussage $A(n)$ bereits gezeigt haben bis hin zu einem bestimmten $n \in \mathbb{N}$. Mit dem \textit{Induktionsschritt} (oder \textit{Induktionsschluss}) zeigen wir, dass aus er Gültigkeit von $A(k)$ für $k \leq n$ die Gültigkeit von $A(n+1)$ folgt.
\begin{theorem}
	Für jede endliche Menge $M$ gilt $|2^M| = 2^{|M|}$.
	\begin{proof}
		Wir machen eine Induktion über die Größe von $M$.\\
		Induktionsverankerung: $|M| = 0$, d.h. $M = \emptyset$. Es gilt $|2^M| = |2^\emptyset| = |\{\emptyset\}| = 1$. \checkmark\\
		Induktionshypothese (IH): Angenommen die Aussage gelte für ein $M$ der Größe $n \in \mathbb{N}$.\\
		Induktionsschritt: Sei $M^\prime \coloneqq M \cup \{a\}$ wobei $a$ ein beliebiges Element ist, welches nicht in $M$ vorkommt. Die Menge wird also um ein Element größer. Dann erhalten wir
		\begin{align*}
			\Big| 2^{M \cup \{a\}} \Big| &= \Big| \overbrace{2^M}^{\text{Teilmengen ohne } a} \cup \overbrace{\{A \cup \{a\} : A \subseteq M \}}^{\text{Teilmengen mit } a} \Big|\\
			&= |2^M| + |\{A \cup \{a\} : A \in 2^M \}|\\
			&= |2^M| + |2^M|\\
			&= 2 \cdot |2^M|\\
			&\overset{\text{IH}}{=} 2\cdot 2^{|M|}\\
			&= 2^{|M|+1}\\
			&= 2^{|M \cup \{a\}|}. \qedhere
		\end{align*}
	\end{proof}
\end{theorem}



\newpage
\section{Alphabete, Wörter, Sprachen}\label{sec:awl}
Das erste Kapitel hat uns mit den nötigen mathematischen Grundlagen versorgt, die wir als Modellierungswerkzeuge in der theoretischen Informatik verwenden wollen. Wir definieren dazu später abstrakte Rechenmodelle, sogenannte Automaten, um das Verhalten von konkreten Rechenmodellen (z.B. Computern) formal zu erfassen. Zunächst sehen wir uns an, wie wir ganz allgemein diese konkreten Rechenmodelle funktionieren und wie sich diese Funktionsweisen möglichst knapp und allgemein (d.h. abstrakt, ``vereinfacht auf das wesentliche``) darstellen lassen. Nach diesem Kapitel haben wir das Hauptthema der Vorlesung, die \textit{abstrakte Automatentheorie}, vorbereitet.

\subsection{Grundlegende Definitionen}\label{sec:awl_def}
Wir fassen Aktionen eines Computers (oder eines Getränkeautomaten, \ldots) in unserer Abstrakion als \textit{Symbole} (Buchstaben) auf, im Rahmen dieser Vorlesung sind das immer nur endlich viele, d.h. das \textit{Alphabet} ist endlich. Aktionsfolgen (z.B. vom Einwurf einer Münze bis zur Ausgabe des Getränkes) entsprechen somit einem \textit{Wort}. Die Menge aller gültigen Aktionsfolgen (solche, die für das betrachtete System ``sinnvoll`` sind) bezeichnen wir dann als \textit{Sprache des Automaten}.
\begin{definition}
	Ein \textit{Alphabet} ist eine nicht-leere endliche Menge, deren Elemente als \textit{Symbole} bezeichnet sind.
\end{definition}
Alphabete werden durch griechische Großbuchstaben $\Sigma, \Gamma$ oder Variationen wie $\Sigma_1, \Gamma^\prime$ bezeichnet. Symbole werden durch kleine lateinische Buchstaben $a, b, c, \ldots$ oder arabische Ziffern $0, 1, \ldots$ bezeichnet.
\begin{example}
	\
	\begin{enumerate}
		\item Das \textit{Boole'sche Alphabet} $\mathbb{B} \coloneqq \{ 0, 1 \}$.
		\item Das \textit{Morsealphabet} $\{ \cdot, -, \,\,\, \}$.
		\item Das \textit{ASCII-Alphabet} für zum Beispiel Textdateien.
	\end{enumerate}
\end{example}

\begin{definition}
	Ein \textit{Wort} über einem Alphabet $\Sigma$ ist eine Abbildung
	$$
		w\colon [n] \to \Sigma.
	$$
	Für $n = 0$ ist $w\colon \emptyset \to \Sigma$ das \textit{leere Wort}, was wir als $\varepsilon$ bezeichnen.	Die \textit{Länge} des Wortes $w$ ist bezeichnet mit $|w| = n$. Mit $|w|_a \coloneqq \left| \{ i \in [n] \mid w(i) = a \} \right|$ bezeichnen wir die \textit{Häufigkeit des Symbols} $a$ im Wort $w$.
\end{definition}
Wie in Abschnitt~\ref{sec:pre_mappings} lässt sich $w$ auch als Tupel $(a_0, \ldots, a_{n-1})$ schreiben. Wir gehen hier sogar noch einen Schritt weiter und benutzen $a_0 a_1 \ldots a_{n-1}$ als Abkürzung für die langen Schreibweisen. Für Wörter verwenden wir in der Regel $u, v, w$ und Varianten als Bezeichner. In der Literatur sind auch kleine griechische Buchstaben $\alpha, \beta, \ldots$ gebräuchlich.
\begin{definition}
	Sei $\Sigma$ ein Alphabet. Dann ist $\Sigma^n \coloneqq \Sigma^{[n]}$ die \textit{Menge aller Wörter mit Länge} $n$ über $\Sigma$.
	Die \textit{Menge aller Wörter} ist definiert als
	$$
		\Sigma^\ast \coloneqq \bigcup_{n \in \mathbb{N}} \Sigma^n
	$$
	und $\Sigma^+ \coloneqq \Sigma^\ast \setminus \{ \varepsilon \}$.
\end{definition}
Wie in Abschnitt~\ref{sec:pre_setops} angekündigt wird für ein fixiertes $\Sigma$ die Menge $\Sigma^\ast$ unser Universum sein.
\begin{definition}
	Eine \textit{(formale) Sprache} über einem Alphabet $\Sigma$ ist eine Teilmenge von $\Sigma^\ast$.
\end{definition}
Sprachen bezeichnen wir in der Regel mit $L, K, \ldots$ und Varianten.
\begin{example}
	\
	\begin{itemize}
		\item Die leere Sprache $\emptyset$.
		\item Die Sprache, die nur das leere Wort enthält $\{\varepsilon\}$.
		\item Die Sprache aller Binärdarstellungen von Primzahlen $\{\bin(n) : n \in \mathbb{P}\}$.
		\item Die Menge aller grammatikalisch korrekten deutschen Sätze.
	\end{itemize}
\end{example}


\subsection{Operationen und Relationen auf Wörtern und Sprachen}\label{sec:awl_wordops}
Durch diese vorgegangen Definitionen haben wir das Fundament für die theoretische Informatik bereits definiert. Da dies nur mithilfe von Funktionen und Mengen  passiert ist lassen sich Beweismethoden und Ergebnisse aus der Mathematik einfach übertragen. Wir definieren nun noch ein paar Operationen auf Wörtern und erweitern diese Definitionen auf Sprachen.
\begin{definition}
	Seien $u, v \in \Sigma^\ast$ mit $m = |u|, n = |v|$. Die \textit{Konkatenation} (Verkettung) ist definiert als
	\begin{align*}
		(u \cdot v)&\colon [m{+}n] \to \Sigma \text{ mit}\\
		(u \cdot v)(i) &= \left\lbrace \begin{array}{ll}u(i), & i < m\\ v(i-m), & i \geq m. \end{array} \right.
	\end{align*}
	Außerdem ist $u^0 \coloneqq \varepsilon$ und $u^n \coloneqq u \cdot u^{n-1}$.
\end{definition}
Aus Bequemlichkeitsgründen wird der Punkt auch weggelassen. Wir schreiben ihn nur, wenn er der Übersicht dient. Für Sprachen erhalten wir noch die Definitionen.
\begin{definition}
	Seien $L, K \subseteq \Sigma^\ast$ Sprachen.
	\begin{enumerate}
		\item \textit{Konkatenation.} $L \cdot K \coloneqq \{ uv : u \in L, v \in K \}$ und $L^0 \coloneqq \{ \varepsilon \}, L^n \coloneqq L \cdot L^{n-1}$.
		\item \textit{Inklusion.} Wie in Definition~\ref{def:subsets}.
		\item \textit{Vereinigung, Schnitt, Komplement, Differenz.} Wie in Definiton~\ref{def:setops}.
		\item \textit{Kleene'scher Abschluss.} (auch \textit{Iteration}, \textit{Kleene-Stern})
			$$
				L^\ast \coloneqq \bigcup_{n \in \mathbb{N}} L^n.
			$$
	\end{enumerate}
\end{definition}
\begin{definition}
	Seien $u, v$ Wörter. Dann ist $u$
	\begin{itemize}
		\item \textit{Präfix} von $v$ (geschrieben: $u \sqsubseteq v$), falls es ein Wort $w$ gibt mit $v = uw$,
		\item \textit{Infix} von $v$, falls es Wörter $w, w^\prime$ gibt mit $v = wuw^\prime$,
		\item \textit{Suffix} von $v$, falls es ein Wort $w$ gibt mit $v = wu$.
	\end{itemize}
\end{definition}
\begin{example}
	Sei $v = aaba$.
	\begin{itemize}
		\item Die Präfixe von $v$ sind $\varepsilon, a, aa, aab, aaba$.
		\item Die Suffixe von $v$ sind $\varepsilon, a, ba, aba, aaba$.
		\item Die Infixe von $v$ sind alle Präfixe und Suffixe, sowie $ab, b$.
	\end{itemize}
\end{example}
\begin{definition}
	Sei $\Sigma$ ein durch $\prec$ geordnetes, endliches Alphabet. Die \textit{lexikographische Ordnung} $\lex$ auf Wörtern $u = a_0 \ldots a_{n-1}, v = b_0 \ldots b_{m-1}$, über $\Sigma$ ist definiert durch:
	\begin{align*}
		u \lex v &\text{ g.d.w. } u \sqsubseteq v\\
		& \text{ oder es gibt ein } k, \text{ s.d. für alle } i < k \text{ gilt } a_i = b_i \text{ und } a_k \prec b_k.
	\end{align*}
\end{definition}
Gewöhnlich betrachten wir ja eine Teilmenge des lateinischen Alphabets $\Sigma_\text{lat} = \{a, \ldots, z\}$. Wir nehmen im Folgenden immer die Ordnung $a \prec b \prec \ldots \prec z$ an.
\begin{example}
	Sei $\Sigma = \{a, b, c\}$.
	\begin{enumerate}[label=\arabic*)]
		\item $abc \lex abcc$,
		\item $aaaa \lex ab$,
		\item $\varepsilon \lex u$ für alle $u \in \Sigma^\ast$.
	\end{enumerate}
\end{example}
Eine weitere Ordnung auf $\Sigma^\ast$ wäre die folgende:
\begin{definition}
	Sei $\Sigma$ ein durch $\prec$ geordnetes, endliches Alphabet. Die \textit{kanonische Ordnung} $\canon$ auf Wörtern $u, v$, über $\Sigma$ ist definiert durch:
	\begin{align*}
		u \canon v &\text{ g.d.w. } |u| < |v|\\
		&\text{ oder } |u| = |v| \text{ und } u \lex v.
	\end{align*}
\end{definition}
Aufgezählt ist die kanonische Ordnung über $\{a, b, c\}^\ast$ einfach
$$
	\varepsilon \canon a \canon b \canon c \canon aa \canon ab \canon ac \canon ba \canon bb \canon \ldots \canon aaa \canon aab \canon \ldots	
$$
oder über $\mathbb{B}^\ast$
$$
	\varepsilon \canon 0 \canon 1 \canon 00 \canon 01 \canon 10 \canon 11 \canon 000 \canon 001 \canon \ldots
$$
\begin{lemma}\label{lem:cn}
	Seien $u, v \in \Sigma^\ast$ mit $u \canon v$. Dann gilt für alle $w \in \Sigma^\ast$
	$$
		uw \canon vw.
	$$
	\begin{proof}
		Ist $|u| < |v|$, so ist auch $|uw| < |vw|$ und damit $uw \canon vw$. Sei also $|u| = |v|$. Dann existiert eine Position $k$ mit $u_k \prec v_k$ und für alle $i < k$ gilt $u_i = v_i$. Mit dem selben $k$ erhalten wir auch die Ordnung $uw \canon vw$.
	\end{proof}
\end{lemma}
Man sagt auch, dass die Konkatenation $\cdot$ die Ordnung $\canon$ \textit{respektiert}.
\begin{remark*}
	Das letzte Lemma gilt nicht für $\lex$, denn $a \lex aa$, aber $a \cdot b \not\prec_\text{lex} aa \cdot b$.
\end{remark*}


\subsection{Gesetze für Wörter und Sprachen}\label{sec:awl_wordlaws}
In diesem Abschnitt wollen wir einige Gesetzmäßigkeiten, die bei der Anwendung von Operationen auf Wörtern und Sprachen gelten, herausarbeiten. Einige Eigenschaften über\-tra\-gen sich sofort aus denen für Mengen aus Abschnitt~\ref{sec:pre_setlaws}. Bei anderen ist etwas mehr zu zeigen und bei wieder anderen gibt es vielleicht auch zunächst unintuitive Unterschiede.
\begin{remark*}
	Für das leere Wort $\varepsilon$ gilt:
	\begin{enumerate}
		\item Es ist für jedes Wort sowohl Präfix, Infix als auch Suffix.
		\item Es ist das \textit{neutrale Element} der Konkatenation, d.h. für alle $w \in \Sigma^\ast$ gilt $\varepsilon w = w = w \varepsilon$.
		\item Daran anknüpfend gilt für jede Sprache $L$, dass $ \{ \varepsilon \} L = L = L  \{ \varepsilon \}$.
		\item Für jede Menge $A$ (inklusive dem Fall $A = \emptyset$) ist $A^0 = \{ \varepsilon \}$.
		\item $\varepsilon$ ist eindeutig, d.h. es gibt kein zweites Wort mit diesen Eigenschaften.
		\item Weil es häufig durcheinander gebracht wird: $\{ \varepsilon \} \neq \emptyset$.
	\end{enumerate}
\end{remark*}
\begin{remark*}
	Für Vereinigung, Schnitt, Differenz, \ldots von Sprachen gelten die selben Regeln (Assoziativ-, Kommutativ-, Distributivgesetze, deMorgan, Absorption, \ldots) wie für Mengen.
\end{remark*}
\begin{remark*}
	Für jede Sprache $L$ gilt, dass $\emptyset L = L \emptyset = \emptyset$.
	\begin{proof}
		Wir zeigen, dass $L \emptyset$ leer ist. Der andere Fall geht analog. Angenommen es existiert ein $w \in L \emptyset$. Dann lässt sich $w$ zerlegen in $w = uv$ mit $u \in L, v \in \emptyset$. Da ein solches $v$ nicht existieren kann (leere Menge), kann auch die gesamte Zerlegung und somit auch $w$ nicht existieren. Also ist $L \emptyset$ leer.
	\end{proof}
\end{remark*}



\newpage
\section{Endliche Automaten und Reguläre Sprachen}\label{sec:regular}
Wir haben formale Sprachen eingeführt als Modellierung für Prozessabläufe auf z.B. Computern. Diese Ansicht werden wir zunächst aber beiseite legen und erst in Kapitel~\ref{sec:process} wieder aufgreifen. In der Zwischenzeit untersuchen wir Sprachen auf verschiedene Eigenschaften und klassifizieren unter anderem nach der sogenannten \textit{Chomsky-Hierarchie}. Wir prüfen, wie sich Sprachen von formalen Systemen (Automaten, abstrakte Rechenmodelle) erkennen lassen. Diese Systeme wirken manchmal etwas künstlich, sind aber sehr sinnvoll, da sie sich mit den uns zu Verfügungen Werkzeugen aus der Mathematik gut handhaben lassen. Die daraus entstehenden Resultate haben außerdem auch einen nicht zu vernachlässigenden ästhetischen Wert für die theoretische Informatik. Zugegeben, der Praxisbezug offenbart sich bei einigen Sätzen nicht sofort und ist vielleicht auch gar nicht überalll vorhanden. Dennoch sollte der Wert dieser Ergebnisse auch nicht unterschätzt werden, denn wir liefern hier auch die Grundlagen zur Untersuchung, was sich prinzipiell mit Computern überhaupt berechnen lässt und die Erkenntnis, dass es Probleme in der Informatik gibt, die von einem Computer mehr Funktionalität beansprucht als andere Probleme (und vielleicht sogar mehr als ein Computer prinzipiell haben kann), sollte Motivation genug sein, sich mit theoretischer Informatik auseinanderzusetzen.

\subsection{Deterministische Endliche Automaten}\label{sec:regular_dfa}
Wir beginnen mit der Art Automaten, die ``die einfachste`` Klasse formaler Sprachen erkennen kann.
\begin{definition}
	Ein \textit{deterministischer endlicher Automat (DFA)} (von engl.: deterministic finite automaton) ist ein Quintupel
	$$
		(Q, \Sigma, \delta, q_0, F),
	$$
	mit
	\begin{itemize}
		\item $Q$ einer nicht-leeren, endlichen Menge von \textit{Zuständen},
		\item $\Sigma$ einem nicht-leeren, endlichen \textit{Eingabealphabet},
		\item $\delta\colon Q \times \Sigma \to Q$ einer \textit{Transitionsfunktion},
		\item $q_0 \in Q$ einem \textit{Startzustand},
		\item $F \subseteq Q$ einer Menge von \textit{akzeptierenden Zustände} (oder \textit{Endzustände}).
	\end{itemize}
\end{definition}
DFAs lassen sich auch problemlos als Strukturen wie in Abschnitt~\ref{sec:pre_structures} auffassen. Aus Gründen der Lesbarkeit verzichten wir jedoch darauf. Wir bezeichnen DFAs stets mit $\mathcal{A}, \mathcal{B}, \ldots$, Zustände mit $p, q, r, s$ und Variationen.\\
Es lassen sich auch durchaus noch einfacherere Rechenmodelle definieren (z.B. durch Restriktionen gegenüber der Größe der Zustandsmenge), dies ist jedoch vorerst nicht sinnvoll. Auf die bereits angesprochene Chomsky-Hierarchie werden wir noch genauer eingehen, aber auch dort sind die Sprachen, die durch DFAs erkannt werden können, als die einfachste Klasse bezeichnet.\\
Möchte man einen DFA konkret angeben, so ist die Darstellung als Transitionsgraph sinnvoller, als die Angabe des 5-Tupels.
\begin{definition}
	Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein DFA. Der \textit{Transitionsgraph} von $\mathcal{A}$ ein beschrifteter Graph $G_\mathcal{A} = ((Q, E), \lambda, q_0, F)$ mit
	$$
		E = \{ (p, q) \mid \text{es ex. } a \in \Sigma \text{ mit } \delta(p, a) = q \}
	$$
	und
	$$
		\lambda\colon E \to 2^\Sigma, \quad (p, q) \mapsto \{ a \mid \delta(p, a) = q \}.
	$$
\end{definition}
Aus Gründen der Bequemlichkeit, lassen wir die Mengenklammern bei der Beschriftung der Transitionen weg. Der Startzustand $q_0$ bekommt einfach eine eingehende Kante ohne Beschriftung und ohne Startknoten. Die Endzustände werden zusätzlich eingekreist. Ein Beispieltransitionsgraph ist in Abbildung~\ref{fig:dfa_ex1}.
\begin{figure}
	\centering
	\input{figs/dfa_ex1}
	\caption{DFA für die Sprache aus Beispiel~\ref{exp:dfa_runs}.}
	\label{fig:dfa_ex1}
\end{figure}
Wir werden die Begriffe Automat und Transitionsgraph gelegentlich synonym verwenden, da sich sowohl im Graphen als auch in der ursprünglichen Automatenstruktur alle Informationen befinden. Wir greifen dann auf den Begriff zurück, der für das aktuelle Thema die bequemere Anschauung hat.\\
Wir schauen uns nun das Verhalten eines DFA auf einem Wort an.
\begin{definition}
	Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein DFA.
	Ein \textit{Lauf} von $\mathcal{A}$ auf einem Wort $w = a_0 \ldots a_{n-1}$ für ein $n \in \mathbb{N}$ ist eine endliche Folge
	$$
		(r_0, a_0, r_1, a_1, \ldots, a_{n-1}, r_n),
	$$
	wobei $r_0, \ldots, r_n \in Q$ und $a_0, \ldots, a_{n-1} \in \Sigma$, sodass
	\begin{enumerate}
		\item $r_0 = q_0$,
		\item $\delta(r_i, a_i) = r_{i+1}$ für $i \in [n]$.
	\end{enumerate}
	Wir sagen ein Lauf ist \textit{akzeptierend}, wenn zusätzlich $r_n \in F$ gilt.
\end{definition}
Wir bezeichnen Läufe in der Regel mit $\varrho$ bzw. Variationen. Statt $(r_0, a_0, r_1, a_1, \ldots, a_{n-1}, r_n)$ schreiben wir auch verkürzend $(r_0, r_1, \ldots, r_n)$, wenn die Symbole nicht relevant für unsere Betrachtungen sind.
\begin{remark*}
	Zu jedem Wort $w \in \Sigma^\ast$ existiert genau ein Lauf von $\mathcal{A}$ auf $w$.
\end{remark*}
\begin{definition}
\
	\begin{enumerate}
		\item Ein DFA $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ \textit{akzeptiert} ein Wort $w \in \Sigma^\ast$, wenn der Lauf von $\mathcal{A}$ akzeptierend ist. Andernfalls \textit{verwirft} $\mathcal{A}$ das Wort $w$.
		\item Die von einem DFA $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ \textit{erkannte Sprache} ist
			$$
				L(\mathcal{A}) \coloneqq \{ w \in \Sigma^\ast \mid \mathcal{A} \text{ akzeptiert } w \}.
			$$
		\item Eine Sprache $L$ heißt \textit{DFA-erkennbar}, wenn es einen DFA $\mathcal{A}$ gibt, sodass $L = L(\mathcal{A})$.
	\end{enumerate}
\end{definition}
\begin{example}\label{exp:dfa_runs}
	Betrachte erneut den Automaten $\mathcal{A}$ in Abbildung~\ref{fig:dfa_ex1}.
	\begin{itemize}
		\item Sei $w_1 = abaaba$. Der Lauf von $\mathcal{A}$ auf $w_1$ ist
			$$
				\varrho_1 = (q_0, q_1, q_2, q_1, q_1, q_2, q_1).
			$$
			D.h. $\mathcal{A}$ akzeptiert $w_1$.
		\item Sei $w_2 = baa$. Der Lauf von $\mathcal{A}$ auf $w_2$ ist
			$$
				\varrho_2 = (q_0, q_3, q_3, q_3).
			$$
			D.h. $\mathcal{A}$ verwirft $w_2$.
		\item $\mathcal{A}$ erkennt die Sprache 
			$$
				L = \{ w \in \{a, b\}^\ast \mid w \text{ beginnt und endet mit } a \}.
			$$
	\end{itemize}
	Die letzte Aussage sagt etwas über das Verhalten des Automaten auf allen, d.h. unendlich vielen, Wörtern aus. Man kann also nicht für jedes Wort einzeln zeigen, dass sich der Automat korrekt verhält. Stattdessen beweisen wir die Aussage per Induktion.
	\begin{proof}
		Wir zeigen die folgende Aussage:
		\begin{center}
			$\mathcal{A}$ akzeptiert das Wort $w$ {g.d.w.} $w$ beginnt und endet mit $a$.
		\end{center}
		Beweis per vollständige Induktion über Wortlänge $n \in \mathbb{N}$. Ist $r = (r_0, \ldots, r_n)$ der Lauf auf dem Wort $w = a_0 \ldots a_{n-1}$, so ist
		$$
			r_n = \left\lbrace 
					\begin{array}{ll}
						q_0, & w = \varepsilon\\
						q_1, & a_0 = a_{n-1} = a\\
						q_2, & a_0 = a \text{ und } a_{n-1} = b\\
						q_3, & a_0 = b.
					\end{array}
				\right.
		$$
		Wir wollen also zeigen, dass ein Lauf von $\mathcal{A}$ auf einem Wort dann und nur dann in $q_1$, dem einzigen akzeptierendem Zustand, endet, wenn $w$ mit $a$ beginnt und endet.\\
		Induktionsverankerung: $n = 0$. Dann ist $w = \varepsilon$ und der Lauf von $\mathcal{A}$ auf $w$ ist $r = (q_0)$, also auch $r_n = r_0 = q_0$.\checkmark\\
		Induktionshypothese (IH): Für $w = a_0 \ldots a_{n-1}$ sei $r = (r_0, \ldots, r_n)$ der Lauf auf $\mathcal{A}$ mit $r_n$ wie in der Behauptung.\\
		Induktionsschritt: Betrachte nun das Wort $w = a_0 \ldots a_n$.
		\begin{itemize}
			\item $a_0 \ldots a_{n-1} = \varepsilon$. Dann ist nach IH $r_n = q_0$ und somit
				$$
					r_{n+1} = \delta(r_n, a_n) = \left\lbrace
							\begin{array}{ll}
								q_1, & a_n = a\\
								q_3, & a_n = b.
							\end{array}
						\right.
				$$
			\item $a_0 = a_{n-1} = a$. Dann ist nach IH $r_n = q_1$ und somit
				$$
					r_{n+1} = \delta(r_n, a_n) = \left\lbrace
							\begin{array}{ll}
								q_1, & a_n = a\\
								q_2, & a_n = b.
							\end{array}
						\right.
				$$
			\item $a_0 = a$ und $a_{n-1} = b$. Dann ist nach IH $r_n = q_2$ und somit
				$$
					r_{n+1} = \delta(r_n, a_n) = \left\lbrace
							\begin{array}{ll}
								q_1, & a_n = a\\
								q_2, & a_n = b.
							\end{array}
						\right.
				$$
			\item $a_0 = b$. Dann ist nach IH $r_n = q_3$ und somit
				$$
					r_{n+1} = \delta(r_n, a_n) = q_3
				$$
				unabhängig von $a_n$.
		\end{itemize}
		Insgesamt gilt also: 
		\begin{align*}
			% if you think this is ugly, I agree.
			w \text{ beginnt und endet mit } a. &\text{ g.d.w. } \text{Ist } r = (r_0, \ldots, r_n) \text{ der Lauf von }\\ &\quad\quad\quad\,\, \mathcal{A} \text{ auf } w \text{ so gilt } r_n = q_1 \in F.\\
			&\text{ g.d.w. } \mathcal{A} \text{ akzeptiert } w.\\
			&\text{ g.d.w. } L(\mathcal{A}) = L.
		\end{align*}
	\end{proof}
\end{example}
So ausführlich wie hier werden wir später nicht mehr beweisen, dass ein gegebener Automat ``das richtige tut``, sollte dies nicht klar sein werden wir die Funktionsweise nur grob erläutern. Ausführliche Beweise sind im Wesentlichen dann gefordert, wenn man zum Beispiel zeigen möchte, dass eine Sprache nicht DFA-erkennbar ist.


\subsection{Abschlusseigenschaften DFA-erkennbarer Sprachen}\label{sec:regular_closure}
Bei einer Einteilung aller möglichen Sprachen in Klassen will man in einer möglichst sinnvollen Weise vorgehen. Damit meint man u.a., dass die einzelnen Klassen in sich abgeschlossen sind bzgl. verschiedener Operationen oder auch andere Eigenschaften haben, die in einer wissenschaftlichen Weise ``schön`` sind. Die folgenden Abschnitte sind dazu da um zu zeigen, dass DFA-erkennbare Sprachen dies in vielerlei Hinsicht erfüllen. In diesem Abschnitt beginnen wir damit zu zeigen, dass DFA-erkennbare Sprachen unter den üblichen Mengenoperationen (Komplementbildung, Vereinigung und Schnitt) abgeschlossen sind. Im späteren Verlauf werden wir noch einige andere Operationen betrachten.\par
Wir zeigen als erstes, dass die DFA-erkennbaren Sprachen unter Komplementbildung abgeschlossen sind.
\begin{theorem}\label{thm:regular_complement}
	Sei $L \subseteq \Sigma^\ast$ eine DFA-erkennbare Sprache. Dann ist auch $\comp{L}$ DFA-erkennbar.
	\begin{proof}
		Sei $L \subseteq \Sigma^\ast$ eine beliebige DFA-erkennbare Sprache. D.h. es existiert ein DFA $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$, der $L$ erkennt. Wir konstruieren aus $\mathcal{A}$ den DFA
		$$
			\comp{\mathcal{A}} = (Q, \Sigma, \delta, q_0, Q \setminus F),
		$$
		welcher die Sprache $\comp{L}$ erkennt. Die Konstruktion vertauscht also lediglich akzeptierende und nicht-akzeptierende Zustände. Wir müssen nun noch zeigen, dass diese Konstruktion korrekt ist, also formal, dass $\comp{L(\mathcal{A})} = L(\comp{\mathcal{A}})$ gilt. Dazu zeigen wir folgende Aussage, aus der offensichtlich die Behauptung folgt.
		\begin{center}
			Für alle $w \in \Sigma^\ast$ gilt: $\mathcal{A}$ akzeptiert $w$ {g.d.w.} $\comp{\mathcal{A}}$ verwirft $w$.
		\end{center}
		Sei also $w \in \Sigma^\ast$. Da $\mathcal{A}$ und $\comp{\mathcal{A}}$ den selben Startzustand $q_0$ und die selbe Transitionsfunktion $\delta$ haben, haben beide Automaten den selben (eindeutigen) Lauf $(r_0, \ldots, r_n)$ auf $w$. $\mathcal{A}$ akzeptiert $w$ falls $r_n \in F$. Dann gilt $r_n \notin Q \setminus F$ und somit $\comp{\mathcal{A}}$ verwirft $w$. Die andere Richtung geht analog. Also gilt die Behauptung.
	\end{proof}
\end{theorem}
Diese Abschlusseigenschaft war einfach zu beweisen, da wir nur eine kleine Änderung am ursprünglichen DFA machen mussten und dann wieder einfach über den eindeutigen Lauf argumentieren konnten. Die Idee hinter der Konstruktion ist auch sehr intuitiv, da wir genau die Wörter akzeptieren wollen, die vom ursprünglichen Automaten verworfen wurden, also deren Läufe in nicht-akzeptierenden Zuständen enden. Der Ansatz die Zustände einfach zu vertauschen drängt sich also geradezu auf.\par
Für den Abschluss unter Vereinigung ist etwas mehr Arbeit zu machen. Wir haben also nun zwei DFA-erkennbare Sprachen (und damit die zugehörigen Automaten) und wollen nun prüfen ob ein Wort in wenigstens einer der beiden Sprachen liegt. Wir müssen nun also einen Automaten konstruieren, der zwei gegebene Automaten simuliert. Diese Simulation muss synchron bzw. parallel stattfinden, eine sequentielle (d.h. Hintereinander-)Ausführung beider Automaten ist nicht möglich (da DFAs bereits gelesene Symbole ``vergessen``, ein explizites ``Abspeichern`` ist nur möglich für konstant viele Symbole -- das reicht nicht für beliebig große Eingabelängen). Wir präsentieren für die parallele Ausführung nun die Produktkonstruktion im Rahmen des nächsten Satzes.
\begin{theorem}\label{thm:regular_intersection}
	Seien $L_1, L_2 \subseteq \Sigma^\ast$ DFA-erkennbare Sprachen. Dann ist auch $L_1 \cap L_2$ DFA-erkennbar.
	\begin{proof}
		Seien $\mathcal{A}_1 = (Q_1, \Sigma, \delta_1, q_0^1, F_1)$ und $\mathcal{A}_1 = (Q_2, \Sigma, \delta_2, q_0^2, F_2)$ die DFAs für $L_1, L_2$. Wir betrachten den \textit{Produktautomaten}
		$$
			\mathcal{A} \coloneqq (Q_1 \times Q_2, \Sigma, \delta, (q_0^1, q_0^2), F)
		$$
		mit $\delta((p, q), a) = (\delta_1(p, a), \delta_2(q, a))$ für alle $p \in Q_1, q \in Q_2, a \in \Sigma$ und $F = F_1 \times F_2$.\\
		Wir zeigen nun, dass $L(\mathcal{A}) = L(\mathcal{A}_1) \cap L(\mathcal{A}_2)$ ist.
		Sei dazu $w = a_0 \ldots a_{n-1}$ gegeben. Wir zeigen, dass $\mathcal{A}$ akzeptiert $w$ {g.d.w.} $\mathcal{A}_1$ und $\mathcal{A}_2$ das Wort $w$ akzeptiert.\\
		``Wenn $\mathcal{A}_1$ und $\mathcal{A}_2$ akzeptiert, dann akzeptiert $\mathcal{A}$``:
		Es gibt also einen Lauf $(r_0, \ldots, r_n)$ mit $r_n \in F_1$ von $\mathcal{A}_1$ und einen Lauf $(p_0, \ldots, p_n)$ mit $p_n \in F_2$ von $\mathcal{A}_2$ jeweils auf $w$. Dann ist der Lauf auf auf $\mathcal{A}$
		$$
			((r_0, p_0), \ldots, (r_n, p_n)),
		$$
		da in jedem Schritt 
		\begin{align*}
			\delta((r_i, p_i), a_i) &= (\delta_1(r_i, a_i), \delta_2(p_i, a_i)) \\
			&= (r_{i+1}, p_{i+1}).
		\end{align*}
		Wegen $r_n \in F_1$ und $p_n \in F_2$ ist auch $(r_n, p_n) \in F_1 \times P_2 = F$. Also akzeptiert $\mathcal{A}$.\\
		``Wenn $\mathcal{A}$ akzeptiert, dann akzeptieren $\mathcal{A}_1$ und $\mathcal{A}_2$``:
		Wir zeigen die Kontraposition dieser Aussage. $\mathcal{A}_1$ oder $\mathcal{A}_2$ verwirft. Die Läufe von $\mathcal{A}_1, \mathcal{A}_2$ sind also $(r_0, \ldots, r_n)$ und $(p_0, \ldots, p_n)$ mit $r_n \notin F_1$ oder $p_n \notin F_2$. Wie oben ist $((r_0, p_0), \ldots, (r_n, p_n))$ der Lauf von $\mathcal{A}$ und es gilt $(r_n, p_n) \notin F_1 \times F_2 = F$. Somit verwirft auch $\mathcal{A}$ das Wort.
	\end{proof}
\end{theorem}
Eine Beispielkonstruktion für einen Produktautomaten für den Schnitt zweier DFA-er\-kenn\-bar\-er Sprachen befindet sich in Abbildung~\ref{fig:dfa_product}.
\begin{figure}
	\centering
	\input{figs/dfa_product}
	\caption{Produktkonstruktion: Der Automat $\mathcal{A}$ erkennt die Sprache $\{ w \in \{a, b \}^\ast \mid |w|_a + |w|_b \text{ teilbar durch } 3 \}$, $\mathcal{B}$ erkennt die Sprache $\{ ua : u \in \{a, b\}^\ast \}$. Der Produktautomat erkennt den Schnitt.}
	\label{fig:dfa_product}
\end{figure}
Aus den Sätzen~\ref{thm:regular_complement} und~\ref{thm:regular_intersection} erhält man nun einfach alle übrigen Abschlusseigenschaften für die üblichen Mengenoperationen, aber auch mit der Produktkonstruktion lassen sich diese Eigenschaften zeigen.
\begin{corollary}\label{cor:regular_intersection}
	Seien $L_1, L_2$ DFA-erkennbare Sprachen. Dann sind auch die Sprachen $L_1 \cup L_2, L_1 \setminus L_2$ DFA-erkannbar.
	\begin{proof}
		Wegen Satz~\ref{thm:regular_complement} sind auch $\comp{L_1}, \comp{L_2}$ DFA-erkennbar und damit nach Satz~\ref{thm:regular_intersection} $\comp{L_1} \cap \comp{L_2}$. Mit erneuter Anwendung von Satz~\ref{thm:regular_complement} ist auch $\comp{\comp{L_1} \cap \comp{L_1}}$ DFA-erkennbar, was nach den DeMorgan'schen Gesetzen $L_1 \cup L_2$ entspricht. Alternativ kann man in der Produktkonstruktion auch $F = (F_1 \times Q_2) \cup (Q_1 \times F_2)$ setzen und erhält einen DFA für $L_1 \cup L_2$.\\
		Für $L_1 \setminus L_2$ setzt man $F = F_1 \times (Q_2 \setminus F_2)$ und erhält einen DFA für die Differenz.
	\end{proof}
\end{corollary}
Die Produktkonstruktion ist eine sehr grundlegende Konstruktion, die in ähnlicher Form immer wieder Anwendungen findet. Wir können auch eigene Sprachoperatoren erfinden und DFA-erkennbare Sprachen daraufhin untersuchen ob sie abgeschlossen sind bzgl. dieser Operationen.
\begin{example}
	Seien $L, K \subseteq \Sigma^\ast$. Wir definieren die Operation \textit{Perfect Shuffle} wie folgt:
	$$
		L \shuffle K \coloneqq \{ a_0 b_0 \ldots a_{n-1} b_{n-1} : a_0 \ldots a_{n-1} \in L, b_0 \ldots b_{n-1} \in K \}.
	$$
\end{example}
\begin{theorem}
	Seien $L_1, L_2 \subseteq \Sigma^\ast$ DFA-erkennbare Sprachen. Dann ist auch $L_1 \shuffle L_2$ DFA-erkennbar.
	\begin{proof}
		Seien $\mathcal{A}_1 = (Q_1, \Sigma, \delta_1, q_0^1, F_1), \mathcal{A}_2 = (Q_2, \Sigma, \delta_2, q_0^2, F_2)$ die DFAs für $L_1, L_2$. Wir definieren wieder einen Produktautomaten
		$$
			\mathcal{A} = (Q_1 \times Q_2 \times [2], \Sigma, \delta, (q_0^1, q_0^2, 0), F)
		$$
		mit
		$$
			\delta((p, q, i), a) = \left\lbrace
				\begin{array}{ll}
					(\delta_1(p, a), q, 1), & i = 0\\
					(p, \delta_2(q, a), 0), & i = 1
				\end{array}
			\right.
		$$
		und $F = F_1 \times F_2 \times \{0\}$. Die Idee ist diesmal nur eine \textit{quasi-parallele} Simulation von $\mathcal{A}_1$ und $\mathcal{A}_2$. Die dritte Komponente des Zustands gibt an in welchem Automat $\mathcal{A}$ den nächsten Schritt simulieren soll. Wir akzeptieren, wenn beide Simulationen in einem Endzustand sind und der zuletzt durchgeführte Simulationsschritt auf $\mathcal{A}_2$ war. Wir zeigen nun noch, dass die Konstruktion funktioniert.\\
		``$L(\mathcal{A}) \subseteq L_1 \shuffle L_2$``: Sei $w = c_0 \ldots c_{n-1} \in L(\mathcal{A})$. D.h. es existiert ein Lauf 
		$$
			\varrho = ((p_0, q_0, i_0), \ldots, (p_n, q_n, i_n))
		$$
		von $\mathcal{A}$ auf $w$ mit $p_n \in F_1, q_n \in F_2$ und $i_j = j \mod 2$ und $i_0 = i_n = 0$ (insbesondere ist $|w|$ gerade), wobei abwechselnd in jedem Schritt $j$ die $1 + i_j$te Komponenten gleich bleibt (s. Definition von $\delta$). Aus den geraden Positionen in $\varrho$ (die mit der dritten Komponente $0$) ergeben dann einen Lauf von $\mathcal{A}_1$ auf dem Wort $c_0 c_2 \ldots c_{n-2}$. Umgekehrt sind die ungeraden Positionen induziert durch den Lauf von $\mathcal{A}_2$ auf $c_1 c_3 \ldots c_{n-1}$. Wegen $p_n \in F_1$ und $i_n = 0$ ist $p_{n-1} = p_n \in F_1$ und somit akzeptiert $\mathcal{A}_1$ das Wort $c_0 c_2 \ldots c_{n-2}$. Analog für $\mathcal{A}_2$. Also ist $w \in L(\mathcal{A}_1) \shuffle L(\mathcal{A}_2)$.\\
		``$L(\mathcal{A}) \supseteq L_1 \shuffle L_2$``: Sei $w = a_0 b_0 \ldots a_{n-1} b_{n-1} \in L_1 \shuffle L_2$. Dann exitieren Läufe $\varrho_1 = (p_0, \ldots, p_n), \varrho_2 = (q_0, \ldots, q_n)$ mit $p_n \in F_1, q_n \in F_2$ von $\mathcal{A}_1, \mathcal{A}_2$. Nach Definition von $\delta$ ist der Lauf auf $\mathcal{A}$ dann
		$$
			((p_0, q_0, 0), (p_1, q_0, 1), \ldots, (p_n, q_{n-1}, 1), (p_n, q_n, 0))
		$$
		und $\mathcal{A}$ akzeptiert $w$.
	\end{proof}
\end{theorem}


\subsection{Nichtdeterministische Endliche Automaten}\label{sec:regular_nfa}
Im letzten Abschnitt haben wir gesehen, dass unter einigen Operationen abgeschlossen sind. Die Mengenoperationen wirken dabei ohnehin von Vorteil für weitere Betrachtungen unter mathematischen Aspekten. Perfect Shuffle könnte man dagegen als eine Routine sehen, ob zum Beispiel ein Prozessor zwei Prozessen wirklich abwechselnd Berechnungszeit gibt. Wir würden gerne weitere Abschlusseigenschaften kennenlernen, besonders die Konkatenation unter der Kleene-Stern wären nun interessant. Eine simultane Ausführung zweier Automaten bringt hier jedoch nichts mehr, da beide Operationen eher von sequentieller Natur sind (Hintereinanderausführung, Wiederholung). DFAs sind für diese Aufgaben zunächst nicht sonderlich sinnvoll. Denkbar wäre ein Modell, dass nach Erreichen eines Endzustandes den nächsten Automaten (im Falle der Konkatenation) auf dem Rest des Wortes startet. Ein DFA weiß aber nicht ad hoc wie das Wort zerlegt ist, es kann also sein, dass nach Erreichen eines Endzustandes der erste Automat noch weiterlaufen soll und erst bei erneutem Besuch eines akzeptierenden Zustandes das zweite Wort losgeht. Zu diesem Zweck führen wir das Prinzip des Nichtdeterminismus ein. Ein Automat kann dann ``raten`` in welchen Zustand er wechseln soll (bzw. ob er ``den nächsten Automaten startet``). Dieses Konzept wirkt etwas unnatürlich, da es nicht von einem Computer simuliert werden kann (Zufall $\neq$ Nichtdeterminismus!), in unserem Modell rät der Automat stets ``richtig``.
\begin{definition}
	Ein \textit{nicht-deterministischer endlicher Automat (NFA)} (von engl.: non-deterministic finite automaton) ist ein Quintupel
	$$
		(Q, \Sigma, \Delta, q_0, F),
	$$
	mit
	\begin{itemize}
		\item $Q$ einer nicht-leeren, endlichen Menge von \textit{Zuständen},
		\item $\Sigma$ einem nicht-leeren, endlichen \textit{Eingabealphabet},
		\item $\Delta \subseteq Q \times \Sigma \times Q$ einer \textit{Transitionsrelation},
		\item $q_0 \in Q$ einem \textit{Startzustand},
		\item $F \subseteq Q$ einer Menge von \textit{akzeptierenden Zustände} (oder \textit{Endzustände}).
	\end{itemize}
\end{definition}
Wir bezeichnen NFAs genau wie DFAs mit $\mathcal{A}, \mathcal{B}$ usw. Die Transitionsgraphen sehen ebenfalls genauso aus, nur, dass nun Zustände mehrere Kanten haben können, die gleich beschriftet sind. Außerdem ist es erlaubt, dass Transitionen komplett fehlen.
\begin{remark*}
	Eine äquivalente und ebenfalls verbreitete Definition benutzt statt einer Transitionsrelation wieder eine Transitionsfunktion $\delta\colon Q \times \Sigma \to 2^Q$. 
\end{remark*}
\begin{remark*}
	Auch wenn es widersprüchlich klingt, aber jeder DFA kann auch als ein NFA gesehen werden. Genauer gesagt ist ein DFA ein NFA bei dem die Transitionsrelation der Graph einer totalen Funktion $Q \times \Sigma \to Q$ ist.
\end{remark*}
Bisher ist noch nicht klar, wie das Akzeptanzverhalten von NFAs sein soll.
\begin{definition}
	Sei $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein NFA.
	Ein \textit{Lauf} von $\mathcal{A}$ auf einem Wort $w = a_0 \ldots a_{n-1}$ für ein $n \in \mathbb{N}$ ist eine endliche Folge
	$$
		(r_0, a_0, r_1, a_1, \ldots, a_{n-1}, r_n),
	$$
	wobei $r_0, \ldots, r_n \in Q$ und $a_0, \ldots, a_{n-1} \in \Sigma$, sodass
	\begin{enumerate}
		\item $r_0 = q_0$,
		\item Für alle $i \in [n]$ gilt, dass $(r_i, a_i, r_{i+1}) \in \Delta$.
	\end{enumerate}
	Wir sagen ein Lauf ist \textit{akzeptierend}, wenn zusätzlich $r_n \in F$ gilt.
\end{definition}
\begin{remark*}
	Für NFAs müssen Läufe nicht mehr eindeutig sein. Es kann zu einem Wort mehrere Läufe geben oder auch gar keine Läufe, wenn entsprechende Transitionen fehlen.
\end{remark*}
\begin{remark*}
	Produktkonstruktionen um zum Beispiel den Schnitt zweier NFA-er\-kenn\-bar\-en Sprachen zu erkennen funktioniert für NFAs genauso wie für DFAs. Schwieriger ist es aber schon das Komplement zu erkennen -- ein einfaches vertauschen von akzeptierenden und nicht-akzeptierenden Zuständen genügt nicht, da weiterhin Wörter aus der Sprache rausgelassen werden für die kein Lauf existiert.
\end{remark*}
\begin{definition}
\
	\begin{enumerate}
		\item Ein NFA $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ \textit{akzeptiert} ein Wort $w \in \Sigma^\ast$, wenn es mindestens einen akzeptierenden Lauf von $\mathcal{A}$ gibt. Andernfalls \textit{verwirft} $\mathcal{A}$ das Wort $w$.
		\item Die von einem NFA $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ \textit{erkannte Sprache} ist
			$$
				L(\mathcal{A}) \coloneqq \{ w \in \Sigma^\ast \mid \mathcal{A} \text{ akzeptiert } w \}.
			$$
		\item Eine Sprache $L$ heißt \textit{NFA-erkennbar}, wenn es einen NFA $\mathcal{A}$ gibt, sodass $L = L(\mathcal{A})$.
	\end{enumerate}
\end{definition}
\begin{example}\label{exp:compare_dfa_nfa}
	Wir betrachten die Sprache
	$$
		L = \{ w \in \{a, b\}^\ast \mid \text{das vorletzte Symbol in } w \text{ ist } a \}.
	$$
	In Abbildung~\ref{fig:nfa_ex1} befindet sich ein DFA und ein NFA für die Sprache. Wir sehen, dass der NFA weniger Zustände hat. Dies ist nicht überraschen, denn der DFA muss sich stets das zuletzt gelesene Symbol im Zustand merken, während der NFA dies nicht muss und einfach ``rät`` welches Symbol das vorletzte ist.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/nfa_ex1dfa}
		\caption{DFA}
		\label{fig:nfa_ex1dfa}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/nfa_ex1nfa}
		\caption{NFA}
		\label{fig:nfa_ex1nfa}
	\end{subfigure}
	\caption{Ein DFA und ein NFA für die Sprache aus Beispiel~\ref{exp:compare_dfa_nfa}.}
	\label{fig:nfa_ex1}
\end{figure}
Für NFAs ist das \textit{Wortproblem}, das Problem ob ein Automat ein gegebenes Wort akzeptiert, nicht ganz so offensichtlich zu lösen wie für DFAs. Bei DFAs müssen wir lediglich die eindeutige Transitionsfolge anwenden und prüfen ob der letzte Zustand ein akzeptierender ist. Für NFAs können wir natürlich alle möglichen Läufe ausprobieren und prüfen, ob ein akzeptierender dabei ist. Doch es gieht effizienter mit der Erreichbarkeitsrelation.
\begin{definition}\label{def:reachability}
	Sei $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein NFA und $w = a_0 \ldots a_{n-1}$. Ein Zustand $q \in Q$ heißt \textit{erreichbar} von $p$ über $w$ (geschrieben $\mathcal{A}: p \reaches{w} q$), wenn es einen Lauf $(r_0, \ldots, r_n)$ gibt mit 
	\begin{itemize}
		\item $r_0 = p, r_n = q$ und
		\item $(r_i, a_i, r_{i+1}) \in \Delta$ für alle $i \in [n]$.
	\end{itemize}
\end{definition}
Diese Notation können wir auch selbstverständlich auf DFAs anwenden. Wir lassen gelegentlich, dass $\mathcal{A}$ weg, wenn der Automat aus dem Kontext klar ist. Wir schreiben außerdem verkürzend $\mathcal{A}: p \reaches{\ast} q$ und $\mathcal{A}: p \reaches{+} q$ falls Wörter $w \in \Sigma^\ast$ bzw. $\Sigma^+$ existieren mit $\mathcal{A}: p \reaches{w} q$. Für Teilmengen $P \subseteq Q$ gibt es auch die Schreibweise $\mathcal{A}: p \reaches{w} P$, wenn es ein $q \in P$ gibt mit $\mathcal{A}: p \reaches{w} q$.
\begin{definition}
	Sei $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein NFA und $w \in \Sigma^\ast$. Die \textit{Menge der in $\mathcal{A}$ über $w$ erreichbaren Zustände} ist definiert als
	$$
		E(\mathcal{A}, w) \coloneqq \{ q \in Q \mid \mathcal{A}: q_0 \reaches{w} q \}.
	$$
\end{definition}
Wir haben zwei Lemmata, die uns die Anwendung dieser Definition nahelegen.
\begin{lemma}\label{lem:reach1}
	Sei $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein NFA und $w \in \Sigma^\ast$. $w \in L(\mathcal{A})$ {g.d.w.} $E(\mathcal{A}, w) \cap F \neq \emptyset$.
	\begin{proof}
		``wenn, dann``: Sei $w \in L(\mathcal{A})$. Dann exitiert ein Lauf $(r_0, \ldots, r_n)$ von $\mathcal{A}$ auf $w$ mit $r_n \eqqcolon q \in F$, also $q_0 \reaches{w} q$. Somit ist $q \in E(\mathcal{A}, w)$. Also ist $\emptyset \neq \{q\} \subseteq E(\mathcal{A}, w) \cap F$.\\
		``nur wenn``: Es exitiert ein $q \in E(\mathcal{A}, w) \cap F$, also $q \in F$ und $q \in E(\mathcal{A}, w)$. Also gibt es einen Lauf von $\mathcal{A}$ auf $w$, der in $q_0$ startet und in $q$ endet. Dieser ist akzeptierend, also ist $w \in L(\mathcal{A})$.
	\end{proof}
\end{lemma}
\begin{lemma}\label{lem:reach2}
	Sei $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein NFA.
	\begin{enumerate}
		\item $E(\mathcal{A}, \varepsilon) = \{q_0\}$,
		\item Für alle $u \in \Sigma^\ast$ und $a \in \Sigma$ gilt
			$$
				E(\mathcal{A}, ua) = \bigcup_{p \in E(\mathcal{A}, u)} \{ q \in Q \mid (p, a, q) \in \Delta \}.
			$$
	\end{enumerate}
	\begin{proof}
		Induktionsverankerung: $q \in E(\mathcal{A}, \varepsilon)$ {g.d.w.} $q_0 \reaches{\varepsilon} q$ {g.d.w.} $q = q_0$. \checkmark\\
		Induktionsschritt: $q \in E(\mathcal{A}, ua)$, also $q_0 \reaches{ua} q$. Damit gibt es ein $p \in Q$, sodass $q_0 \reaches{u} p \reaches{a} q$. Wir folgern, dass $p \in E(\mathcal{A}, u)$ und $(p, a, q) \in \Delta$ und somit $q \in \bigcup_{p \in E(\mathcal{A}, u)} \{ r \in Q \mid (p, a, r) \in \Delta \}$. Rückrichtung analog.
	\end{proof}
\end{lemma}
Mit Hilfe der Erreichbarkeitsrelation und Lemmata~\ref{lem:reach1} und~\ref{lem:reach2}, erhalten wir für das Wortproblem für NFAs einen Algorithmus.
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\underline{NFA-Akzeptanz}{$(\mathcal{A}, w)$}\\
	\Output{Ja {g.d.w.} $\mathcal{A}$ akzeptiert $w = a_0 \ldots a_{n-1}$.}	
	$u \coloneqq \varepsilon$\\
	$E(\mathcal{A}, u) \coloneqq \{ q_0 \}$\\	
	\For{$i = 0, \ldots, n-1$}{
		Bestimme $E(\mathcal{A}, ua_i)$ aus $E(\mathcal{A}, u)$ (Lemma~\ref{lem:reach2})\\
		$u \coloneqq ua_i$
	}
	Prüfe, ob $E(\mathcal{A}, w) \cap F \neq \emptyset$.
	\caption{Wortproblem für NFAs}
	\label{alg:nfaacc}
\end{algorithm}
\begin{example}
	Wir betrachten erneut den NFA in Abbildung~\ref{fig:nfa_ex1nfa} und die Erreichbarkeitsmengen für die Präfixe von $w = abbabaa$.
	\begin{align*}
		E(\mathcal{A}, \varepsilon) &= \{ q_0 \},\\
		E(\mathcal{A}, a) &= \{ q_0, q_1 \},\\
		E(\mathcal{A}, ab) &= \{ q_0, q_2 \},\\
		E(\mathcal{A}, abb) &= \{ q_0 \},\\
		E(\mathcal{A}, abba) &= \{ q_0, q_1 \},\\
		E(\mathcal{A}, abbab) &= \{ q_0, q_2 \},\\
		E(\mathcal{A}, abbaba) &= \{ q_0, q_1 \},\\
		E(\mathcal{A}, abbabaa) &= \{ q_0, q_1, q_2 \}.
	\end{align*}
	Wegen $E(\mathcal{A}, w) \cap F \neq \emptyset$ wird $w$ akzeptiert.
\end{example}


\subsection{Äquivalenz von NFAs und DFAs}\label{sec:regular_equivalence}
Auf den ersten Blick wirkt es vermutlich so, dass NFAs ``mehr`` können als DFAs, da NFAs durch das stets richtige Raten der Transition einen Blick in die Zukunft werfen können. Dieser Abschnitt widmet sich jedoch der Äquivalenz von NFAs und DFAs, d.h. auch wenn wie in Beispiel~\ref{exp:compare_dfa_nfa} NFAs mit weniger Zuständen die gleichen Sprachen erkennen können wie DFAs, so können auch DFAs jede NFA-erkennbare Sprache erkennen. Dies ist mit einer einfachen Überlegung auch gar nicht so unintuitiv: Die Erreichbarkeitsmenge für einen NFA und ein Wort ist stets endlich, da auch ein NFA lediglich endlich viele Zustände hat. In einer Simulation eines NFA in einem DFA könnte man also also einen Zustand durch die Menge der erreichbaren Zustände wählen und würde endlich bleiben. Details dazu folgen in diesem Abschnitt.
\begin{definition}\label{def:fa_equivalence}
	Seien $\mathcal{A}, \mathcal{B}$ zwei endliche Automaten (deterministisch oder nicht-de\-ter\-mi\-nis\-tisch). $\mathcal{A}$ und $\mathcal{B}$ heißen \textit{äquivalent}, wenn $L(\mathcal{A}) = L(\mathcal{B})$.
\end{definition}
Eine Bemerkung aus dem vorigen Abschnitt greifen wir nochmal im folgendem Lemma auf.
\begin{lemma}\label{lem:dfa2nfa}
	Zu jedem DFA exitiert ein äquivalenter NFA.
	\begin{proof}
		Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein DFA. Wir definieren einen NFA
		$$
			\mathcal{A}^\prime = (Q, \Sigma, \Delta, q_0, F),
		$$
		mit $\Delta = \{ (p, a, q) \mid \delta(p, a) = q \}$. Wir zeigen, dass $\mathcal{A}^\prime$ und $\mathcal{A}$ äquivalent sind. Dazu sei $w = a_0 \ldots a_{n-1} \in L(\mathcal{A})$. Also ist der eindeutige Lauf $\varrho = (r_0, \ldots, r_n)$ akzeptierend auf $\mathcal{A}$. Der Lauf ist nach Konstruktion auch ein Lauf auf $\mathcal{A}^\prime$, also akzeptiert auch $\mathcal{A}^\prime$. Rückrichtung analog.
	\end{proof}
\end{lemma}
Diese Richtung war relativ einfach mit den eingangs genannten Bemerkungen. Man könnte den Beweis auch aufwändig wieder über Induktion führen, dies ist aber nicht sinnvoll, da das Ergbenis recht klar ist.
\begin{lemma}\label{lem:nfa2dfa}
	Zu jedem NFA existiert ein äquivalenter DFA.
	\begin{proof}
		Sei $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein NFA. Wir konstruieren einen DFA, der äquivalent ist, durch die sogenannte \textit{Potenzmengenkonstruktion}:
		$$
			\mathcal{A}^\prime = (2^Q, \Sigma, \delta, \{q_0\}, \{ P \subseteq Q \mid P \cap F \neq \emptyset \})
		$$
		mit $\delta(P, a) = \{ q \mid \text{es ex. } p \in P \text{ mit } (p, a, q) \in \Delta \}$.
		Wir zeigen, dass beide Automaten äquivalent sind durch die Behauptung, dass für alle $w \in \Sigma^\ast$ und $P \subseteq Q$ mit $\mathcal{A}^\prime: \{q_0\} \reaches{w} P$ gilt, dass $P = E(\mathcal{A}, w)$, d.h. der Zustand, den der Potenzmengenautomatauf dem Wort $w$ erreicht, entspricht der Erreichbarkeitsmenge auf dem selben Wort für den NFA. Dies zeigen wir per Induktion über alle Wortlängen $n$.\\
		Induktionsverankerung: $n = 0$. Dann ist $w = \varepsilon$ und es gilt $P = \{q_0\} = E(\mathcal{A}, \varepsilon)$ nach Lemma~\ref{lem:reach2}.\\
		Induktionsschritt: $n \rightarrow {n+1}$. Dann ist $w = ua$ für ein $u \in \Sigma^n$ und $a \in \Sigma$. Sei $P^\prime \subseteq Q$, sodass $\mathcal{A}^\prime: \{q_0\} \reaches{u} P^\prime$. Nach Induktionshypothese ist $P^\prime = E(\mathcal{A}, u)$. Somit gilt
		\begin{align*}
			P &= \delta(P^\prime, a)\\
			&= \{q \in Q \mid \text{es ex. } p \in P^\prime \text{ mit } (p, a, q) \in \Delta\}\\
			&= \bigcup_{p \in P^\prime = E(\mathcal{A}, u)} \{q \in Q \mid (p, a, q) \in \Delta\}\\
			&= E(\mathcal{A}, w)
		\end{align*}
		nach Definition von $\delta$ und Lemma~\ref{lem:reach2}. Insgesamt gilt also, dass $w \in L(\mathcal{A})$ {g.d.w.} $E(\mathcal{A}, w) \cap F \neq \emptyset$ (Lemma~\ref{lem:reach1}). Nach der oben gezeigten Behauptung gilt dies {g.d.w.} $P \cap F \neq \emptyset$ und nach Definition des Konstruktion ist $P$ genau dann akzeptierend im Potenzmengenautomaten. Somit akzeptiert $\mathcal{A}^\prime$ das Wort $w$.
	\end{proof}
\end{lemma}
In der Potenzmengenkonstruktion haben wir als Zustandsmenge stets die Potenzmenge der Zustandsmenge des NFA genutzt. Offenbar ist es aber so, dass dann einige Zustände unerreichbar sind, diese können auch weggelassen werden.
\begin{definition}\label{def:reduced_automaton}
	Sei $\mathcal{A} = (Q, \Sigma, \frac{\delta}{\Delta}, q_0, F)$ ein DFA bzw. NFA.
	\begin{enumerate}
		\item Ein Zustand $q \in Q$ ist \textit{erreichbar}, wenn es ein $w \in \Sigma^\ast$ gibt, sodass $\mathcal{A}: q_0 \reaches{w} q$.
		\item Der \textit{reduzierte Automat} (auf die erreicharen Zustände) ist:
			$$
				\mathcal{A}^\prime = (Q^\prime, \Sigma, \frac{\delta^\prime}{\Delta^\prime}, q_0, F^\prime)
			$$
			wobei
			\begin{itemize}
				\item $Q^\prime \coloneqq \{ q \in Q \mid q \text{ erreichbar} \}$,
				\item $\delta^\prime \coloneqq \left. \delta \right|_{Q^\prime \times \Sigma}$ bzw. $\Delta^\prime \coloneqq \Delta \cap (Q^\prime \times \Sigma \times Q^\prime)$,
				\item $F^\prime \coloneqq F \cap Q^\prime$.
			\end{itemize}
	\end{enumerate}
\end{definition}
Wir verwenden die Potenzmengenkonstruktion aus Lemma~\ref{lem:nfa2dfa} zur Determinisierung eines NFA. Wir können dabei schrittweise vom Anfangszustand die Zustände und Transitionen konstruieren um so direkt auf einen reduzierten Automaten gemäß Definition~\ref{def:reduced_automaton} zu kommen. Dies ist in Algorithmus~\ref{alg:nfa2dfa} beschrieben.
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\underline{Potenzmengenkonstruktion}{$(\mathcal{A} = (Q, \Sigma, \Delta, q_0, F))$}\\
	\Output{äquivalenter, reduzierter DFA $\mathcal{A}^\prime = (Q^\prime, \Sigma, \delta, q_0^\prime, F^\prime)$}	
	$Q^\prime \coloneqq \{\{q_0\}\}$\\
	$q_0^\prime \coloneqq \{q_0\}$\\
	$F^\prime \coloneqq \emptyset$\\
	Queue $P \coloneqq Q^\prime$\\
	\While{$P \neq \emptyset$}{
		$S \coloneqq P$.dequeue()\\
		\For{$a \in \Sigma$}{
			$R \coloneqq \{\bigcup_{p \in S} \{ r \mid (p, a, r) \in \Delta \}$\\
			$P$.enqueue($R$)\\
			$Q^\prime \coloneqq Q^\prime \cup R$\\
			$\delta(S, a) = R$
		}
	}
	\For{$P \in Q^\prime$}{
		\If{$P \cap F \neq \emptyset$}{
			$F^\prime \coloneqq F^\prime \cup \{ P \}$
		}
	}
	\caption{NFA-Determinisierung}
	\label{alg:nfa2dfa}
\end{algorithm}
\begin{example}
	Wir betrachten den NFA in Abbildung~\ref{fig:powersetconstruction_nfa}. In Abbildung~\ref{fig:powersetconstruction_dfa} ist ein äquivalenter, reduzierter DFA, erhalten durch Potenzmengenkonstruktion.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/powersetconstruction_nfa}
		\caption{NFA}
		\label{fig:powersetconstruction_nfa}
	\end{subfigure}\\
	\ \\
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/powersetconstruction_dfa}
		\caption{Potenzmengenautomat}
		\label{fig:powersetconstruction_dfa}
	\end{subfigure}
	\caption{Beispiel zur Potenzmengenkonstruktion.}
	\label{fig:powersetconstruction}
\end{figure}
\begin{theorem}
	Eine Sprache ist genau dann DFA-erkennbar, wenn sie NFA-erkennbar ist.
	\begin{proof}
		Die Lemmata~\ref{lem:dfa2nfa} und~\ref{lem:nfa2dfa} zusammen geben den Beweis.
	\end{proof}
\end{theorem}
In diesem Sinne ist es sinnvoll von nun an nur noch von FA-erkennbaren Sprachen zu sprechen statt zwischen DFA- und NFA-erkennbaren Sprachen zu unterscheiden.


\subsection[NFAs mit $\varepsilon$-Transitionen]{NFAs mit $\bm{\varepsilon}$-Transitionen}
Mit Blick auf spätere Abschnitte erweitern wir das Modell der NFAs um die Möglickeit den Zustand zu wechseln ohne ein Symbol dabei zu lesen.
\begin{definition}
	Ein \textit{nicht-deterministischer endlicher Automat mit} \textit{$\varepsilon$-Transitionen ($\varepsilon$-NFA)} ist ein Quintupel
	$$
		(Q, \Sigma, \Delta, q_0, F),
	$$
	mit
	\begin{itemize}
		\item $Q$ einer nicht-leeren, endlichen Menge von \textit{Zuständen},
		\item $\Sigma$ einem nicht-leeren, endlichen \textit{Eingabealphabet},
		\item $\Delta \subseteq Q \times (\Sigma \cup \{\varepsilon\}) \times Q$ einer \textit{Transitionsrelation},
		\item $q_0 \in Q$ einem \textit{Startzustand},
		\item $F \subseteq Q$ einer Menge von \textit{akzeptierenden Zustände} (oder \textit{Endzustände}).
	\end{itemize}
\end{definition}
Wir bezeichnen $\varepsilon$-NFAs wieder mit $\mathcal{A}, \mathcal{B}$ usw. Transitionsgraphen lassen sich ebenfalls analog zeichnen wie für NFAs und DFAs, wobei $\varepsilon$-Transitionen durch Kanten gekennzeichnet werden, die mit $\varepsilon$ beschriftet sind. Der Voll\-stän\-dig\-keit halber definieren wir auch wieder Läufe auf $\varepsilon$-NFAs und das Akzeptanzverhalten.
\begin{definition}
	Sei $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein $\varepsilon$-NFA.
	Ein \textit{Lauf} von $\mathcal{A}$ auf einem Wort $w = a_0 \ldots a_{n-1}$ für ein $n \in \mathbb{N}$ ist eine endliche Folge
	$$
		(r_0, \sigma_0, r_1, \sigma_1, \ldots, sigma_{m-1}, r_m),
	$$
	wobei $r_0, \ldots, r_m \in Q$ und $\sigma_0, \ldots, \sigma_{m-1} \in \Sigma \cup \{\varepsilon\}$, sodass
	\begin{enumerate}
		\item $r_0 = q_0$,
		\item Für alle $i \in [m]$ gilt, dass $(r_i, \sigma_i, r_{i+1}) \in \Delta$.
	\end{enumerate}
	Wir sagen ein Lauf ist \textit{akzeptierend}, wenn zusätzlich $r_n \in F$ gilt.
\end{definition}
\begin{definition}
\
	\begin{enumerate}
		\item Ein $\varepsilon$-NFA $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ \textit{akzeptiert} ein Wort $w \in \Sigma^\ast$, wenn es mindestens einen akzeptierenden Lauf von $\mathcal{A}$ gibt. Andernfalls \textit{verwirft} $\mathcal{A}$ das Wort $w$.
		\item Die von einem $\varepsilon$-NFA $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ \textit{erkannte Sprache} ist
			$$
				L(\mathcal{A}) \coloneqq \{ w \in \Sigma^\ast \mid \mathcal{A} \text{ akzeptiert } w \}.
			$$
		\item Eine Sprache $L$ heißt \textit{$\varepsilon$-NFA-erkennbar}, wenn es einen $\varepsilon$-NFA $\mathcal{A}$ gibt, sodass $L = L(\mathcal{A})$.
	\end{enumerate}
\end{definition}
\begin{example}\label{exp:epsnfa}
	Wir betrachten die Sprache $L = \{ a^n b^m : n, m \in \mathbb{N} \} \cup \{ a^n c^m : n, m \in \mathbb{N} \}$. Der $\varepsilon$-NFA in Abbildung~\ref{fig:epsnfa_ex1} erkennt die Sprache $L$ mit drei Zuständen.
\end{example}
\begin{figure}
	\centering
	\input{figs/epsnfa_ex1}
	\caption{$\varepsilon$-NFA, der die Sprache aus Beispiel~\ref{exp:epsnfa} erkennt.}
	\label{fig:epsnfa_ex1}
\end{figure}
Für $\varepsilon$-NFAs zeigen wir wie im vorigen Abschnitt zu NFAs, dass auch sie die Klasse der FA-erkennbaren Sprachen nicht vergrößern. Der Äquivalenzbegriff aus Definition~\ref{def:fa_equivalence} überträgt sich natürlich auf $\varepsilon$-NFAs.
\begin{lemma}
	Zu jedem NFA existiert ein äquivalenter $\varepsilon$-NFA.
	\begin{proof}
		Dies ist sofort klar, da jeder NFA als ein $\varepsilon$-NFA betrachtet werden kann, der keine $\varepsilon$-Transitionen besitzt.
	\end{proof}
\end{lemma}
Bevor wir den anderen Teil der Äquivalenz zeigen fehlt uns eine Definition.
\begin{definition}\label{def:eps_closure}
	Sei $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein $\varepsilon$-NFA. Der \textit{$\varepsilon$-Abschluss} (auch \textit{$\varepsilon$-Hülle}) eines Zustands $p \in Q$ ist
	$$
		\comp{\varepsilon}(p) \coloneqq \{ q \in Q \mid \text{es ex. } p_1, \ldots, p_k \text{ mit } (p, \varepsilon, p_1), (p_i, \varepsilon, p_{i+1}), (p_k, \varepsilon, q) \in \Delta \}.
	$$
	Für Mengen $P \subseteq Q$ ist der \textit{$\varepsilon$-Abschluss} dann erweitert durch
	$$
		\comp{\varepsilon}(P) \coloneqq \bigcup_{p \in P} \comp{\varepsilon}(p).
	$$
\end{definition}
Auf diese Weise lässt sich die Erreichbarkeitsrelation, die wir schon aus Definition~\ref{def:reachability} kennen erweitern.
\begin{definition}\label{def:reachability2}
	Sei $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein NFA und $w = a_0 \ldots a_{n-1}$. Ein Zustand $q \in Q$ heißt \textit{erreichbar} von $p$ über $w$ (geschrieben $\mathcal{A}: p \reaches{w} q$), wenn es ein $m \geq n$, Indizes $0 \leq i_0 < \ldots < i_{n-1} \leq m$ und Zustände $r_0, \ldots, r_m \in Q$ gibt, sodass
	\begin{itemize}
		\item $r_0 = p, r_m = q$,
		\item $(r_{i_j}, a_j, r_{i_j+1}) \in \Delta$ für alle $j \in [n]$ und
		\item $(r_i, \varepsilon, r_{i+1}) \in \Delta$ für alle $i \in [m] \setminus \{i_1, \ldots, i_{n-1}\}$.
	\end{itemize}
\end{definition}
Man beachte, dass $p \reaches{\varepsilon} q$ nicht $p = q$ bedeutet wie bei NFAs.
\begin{lemma}\label{lem:epsnfa2nfa}
	Zu jedem $\varepsilon$-NFA existiert ein äquivalenter NFA.
	\begin{proof}
		Wir betrachten einen $\varepsilon$-NFA $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ und überführen ihn in einen äquivalenten NFA $\mathcal{A}^\prime = (Q, \Sigma, \Delta^\prime, q_0, F^\prime)$. Dabei ist
		$$
			\Delta^\prime \coloneqq \{(p, a, q) \in Q \times \Sigma \times Q \mid \mathcal{A}: p \reaches{a} q\}
		$$
		und $F^\prime \coloneqq \{q \in Q \mid q \in \comp{\varepsilon}(F)\}$.
		Die Korrektheit ist einfach einzusehen. Sei $w = a_0 \ldots a_{n-1} \in L(\mathcal{A})$. Dann gibt es ein $q \in F$ mit $q_0 \reaches{w} q$. Also gibt es Zustände $r_0 = q_0, \ldots, r_m = q$ wie in Definition~\ref{def:reachability2}. Nach Konstruktion von $\Delta^\prime$ gibt es somit Zustände $r_{i_0} = q_0, \ldots, r_{i_n} = q$, sodass $r_{i_j} \reaches{a_j} r_{i_{j+1}}$ für alle $j \in [n]$. Damit ist $w \in L(\mathcal{A}^\prime)$. Rückrichtung analog.
	\end{proof}
\end{lemma}
\begin{example}
	Wir betrachten den $\varepsilon$-NFA in Abbildung~\ref{fig:epsnfa_ex2_eps}. In Abbildung~\ref{fig:epsnfa_ex2_noeps} sehen wir den NFA den wir nach Lemma~\ref{lem:epsnfa2nfa} erhalten, wobei die neu eingefügten Transitionen rot markiert sind. Beachte auch, dass sich die Menge der akzeptierenden Zustände geändert hat, da sonst das leere Wort nicht mehr akzeptiert werden würde.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/epsnfa_ex2_eps}
		\caption{mit $\varepsilon$-Transitionen}
		\label{fig:epsnfa_ex2_eps}
	\end{subfigure}~
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/epsnfa_ex2_noeps}
		\caption{ohne $\varepsilon$-Transitionen}
		\label{fig:epsnfa_ex2_noeps}
	\end{subfigure}
	\caption{Eliminieren von $\varepsilon$-Transitionen}
	\label{fig:epsnfa_ex2}
\end{figure}
\begin{remark*}
	Beachte, dass ein Zusammenziehen der Zustände, die durch $\varepsilon$-Transitionen verbunden sind nicht funktioniert, da das im Allgemeinen die erkannte Sprache verändert. Ein solches Vorgehen würde zum Beispiel dazu führen, dass der entstehende Automat zu Beispiel~\ref{fig:epsnfa_ex1} nur noch einen Zustand besitzt mit einem durch $a, b, c$ beschrifteten Loop. Die erkannte Sprache wäre dann $\{a, b, c\}^\ast$.
\end{remark*}
\begin{corollary}
	Zu jedem $\varepsilon$-NFA existiert ein äquivalenter DFA.
	\begin{proof}
		Lemma~\ref{lem:epsnfa2nfa} liefert zunächst einen NFA, der mit Lemma~\ref{lem:nfa2dfa} in einen DFA umgewandelt werden kann.
	\end{proof}
\end{corollary}
Wir werden also auch weiterhin von FA-erkennbaren Sprachen sprechen, solange sie von einem DFA, NFA oder $\varepsilon$-NFA erkannt werden, da alle drei Modelle gleich mächtig sind. In mancher Fachliteratur wird wegen ihrer Äqui\-va\-lenz auch zwischen NFAs und $\varepsilon$-NFAs gar nicht unterschieden -- wir werden im Folgenden die Unterscheidung dennoch weiterhin machen.


\subsection{Reguläre Ausdrücke}\label{sec:regular_regexp}
Bisher haben wir Sprachen betrachtet, die sich von endlichen Automaten erkennen lassen. Diese Art von Sprachen liefern uns eine eigene Klasse von Sprachen, die unter verschiedenen Operationen abgeschlossen ist. Auch wenn bisher noch nicht alles gezeigt wurde (Konkatenation, Kleene'sche Hülle) lässt sich mit einiger Berechtigung sagen, dass die Klasse der FA-erkennbaren Sprachen gute Eigenschaften hat. Wir untersuchen nun welche Sprache wir mit sogenannten \textit{regulären Ausdrücken} beschreiben können.
\begin{definition}\label{def:regexsyntax}
	Sei $\Sigma$ ein endliches Alphabet. Ein \textit{regulärer Ausdruck} ist induktiv definiert mit:
	\begin{itemize}
		\item $\bm{\emptyset}$ ist ein regulärer Ausdruck.
		\item Für jedes $a \in \Sigma$ ist $\bm{a}$ ein regulärer Ausdruck.
		\item Falls $r, r^\prime$ reguläre Ausdrücke sind, dann ist auch $(r \bm{+} r^\prime)$ ein regulärer Ausdruck.
		\item Falls $r, r^\prime$ reguläre Ausdrücke sind, dann ist auch $(r \bm{\cdot} r^\prime)$ ein regulärer Ausdruck.
		\item Falls $r$ reguläre Ausdrücke sind, dann ist auch $r \overset{\bm{\ast}}{}$ ein regulärer Ausdruck.
	\end{itemize}
	Die \textit{Menge aller regulären Ausdrücke} über $\Sigma$ bezeichnen mit $\mathsf{RE}_\Sigma$.
\end{definition}
Das ist bisher lediglich die Syntax der regulären Ausdrücke gewesen. Nun definieren wir eine Sematik für diese Ausdrücke, d.h. wir ordnen jedem regulären Ausdruck eine Sprache zu.
\begin{definition}
	Sei $\Sigma$ ein endliches Alphabet. Die \textit{Interpretation eines regulären Ausdrucks} ist die Abbildung
	$$
		\llbracket \cdot \rrbracket \colon \mathsf{RE}_\Sigma \to 2^{\Sigma^\ast}
	$$
	mit
	\begin{itemize}
		\item $\llbracket \bm{\emptyset} \rrbracket = \emptyset$,
		\item $\llbracket \bm{a} \rrbracket = \{ a \}$ für jedes $\bm{a} \in \mathsf{RE}_\Sigma$,
		\item $\llbracket (r \bm{+} r^\prime) \rrbracket = \llbracket r \rrbracket \cup \llbracket r^\prime \rrbracket$,
		\item $\llbracket (r \bm{\cdot} r^\prime) \rrbracket = \llbracket r \rrbracket \cdot \llbracket r^\prime \rrbracket$,
		\item $\llbracket r \overset{\bm{\ast}}{} \rrbracket = \llbracket r \rrbracket^\ast$.
	\end{itemize}
	Eine Sprache $L \subseteq \Sigma^\ast$ heißt \textit{regulär}, wenn ein regulärer Ausdruck $r \in \mathsf{RE}_\Sigma$ existiert mit $\llbracket r \rrbracket = L$.
\end{definition}
Wir erlauben in der Regel auch als Abkürzung die Ausdrücke $\bm{\varepsilon}$ für $\bm{\emptyset}\overset{\bm{\ast}}{}$ und $r \overset{\bm{+}}{}$ für $r \bm{\cdot} r \overset{\bm{\ast}}{}$. Außerdem lassen wir den Punkt ($\bm{\cdot}$) weg, es sei denn er dient der Lesbarkeit. Die Klammern können wir auch weglassen unter der Konvention, dass $\overset{\bm{\ast}}{}$ stärker bindet als $\bm{\cdot}$, was wiederum stärker bindet als $\bm{+}$. Statt $\llbracket r \rrbracket$ ist auch die Schreibweise $L(r)$ gebräuchlich.\par
Reguläre Ausdrücke kennt man auch für Computerprogramme wie \texttt{grep}, \texttt{awk} oder \texttt{sed}. Diese sind syntaktisch etwas anders aufgebaut, jedoch lässt sich jeder POSIX-Ausdruck (das sind die regulären Ausdrücke für oben genannte Programme) auch als regulärer Ausdruck wie in Definition~\ref{def:regexsyntax} umschreiben.
\begin{example}
	Wir geben die wichtigsten Spezifikationen der POSIX-Ausdrücke an.
	\begin{itemize}
		\item $r$\verb$|$$r^\prime$ für $r \bm{+} r^\prime$,
		\item Für $(\bm{a + b +} \ldots \bm{+ z})$ geht auch \verb$[a-z]$\\
			(analog: \verb$[A-Z]$,  \verb$[0-9]$, \verb$[a-z0-9,\.:;\?!]$ usw.),
		\item \verb$.$ für ein beliebiges Zeichen,
		\item $r$\verb$?$ für $r \bm{+ \varepsilon}$,
		\item $r$\verb$+$ und $r$\verb$*$ für $r\overset{\bm{+}}{}, r\overset{\bm{\ast}}{}$,
		\item $r$\verb${m,n}$ für $r^m \bm{+} r^{m+1} \bm{+} \ldots \bm{+} r^n$.
	\end{itemize}
	Wir können zum Beispiel die Sprache aller E-Mail-Adressen als Ausdruck
	$$
		\verb$[a-zA-Z0-9\.-]+@[a-zA-Z0-9\.-]+\.(de|com|net)$
	$$
	darstellen.
\end{example}
Für unsere Analyse von regulären Sprachen benutzen wir nur reguläre Ausdrücke der Form wie in Definition~\ref{def:regexsyntax} vorgestellt. Dadurch lassen sich viele Beweise über den minimalistischen induktiven Aufbau der Ausdrücke führen.
\begin{example}
	Wir betrachten das Alphabet $\Sigma = \{a, b\}$.
	\begin{itemize}
		\item $\llbracket \bm{((a+b)(a+b))} \overset{\bm{\ast}}{} \rrbracket = \{ w \in \Sigma^\ast \mid |w| \text{ gerade} \}$.
		\item $\llbracket \bm{(a+b)(a+b)(a+b)} \overset{\bm{\ast}}{} \rrbracket = \{ w \in \Sigma^\ast \mid |w| \geq 2 \}$.
		\item $\llbracket \bm{a} \overset{\bm{\ast}}{} \bm{+ b} \overset{\bm{\ast}}{} \rrbracket = \{ w \in \Sigma^\ast \mid |w|_a = 0 \text{ oder } |w|_b = 0 \}$.
	\end{itemize}
\end{example}
Aus Gründen der Einfachheit werden wir von nun an bei der Notation von regulären Ausdrücken auf eine Unterscheidung zu Alphabetsymbolen, Kleene-Stern usw. verzichten, d.h. wir verwenden $\emptyset, \varepsilon, a, +, \cdot, ^\ast$ statt $\bm{\emptyset}, \bm{\varepsilon}, \bm{a}, \bm{+}, \bm{\cdot}, \overset{\bm{\ast}}{}$. Außerdem verwenden wir Shortcuts, z.B. $\sum_{a\in\Sigma} a = \Sigma^\ast$. Formal ist aber wichtig weiterhin eine Unterscheidung zwischen Sprachen und regulären Ausdrücken zu haben.
\begin{definition}
	Zwei reguläre Ausdrücke $r, e \in \mathsf{RE}_\Sigma$ heißen \textit{äquivalent} (geschrieben $r \equiv e$), wenn $\llbracket r \rrbracket = \llbracket e \rrbracket$.\\
	Ein regulärer Ausdruck $r \in \mathsf{RE}_\Sigma$ und ein endlicher Automat $\mathcal{A}$ (DFA, NFA) heißen \textit{äquivalent}, wenn  $\llbracket r \rrbracket = L(\mathcal{A})$.
\end{definition}
\begin{example}
	Die regulären Ausdrücke $(a + b)^\ast$ und $(a^\ast b^\ast)^\ast$ sind äquivalent.
\end{example}
Wir kommen nun zu einem der wichtigsten Ergebnisse der Automatentheorie und dieser Vorlesung.
\begin{theorem}[Äquivalenzsatz von Kleene]\label{thm:kleene_equivalence}
	Eine Sprache ist genau dann regulär, wenn sie FA-erkennbar ist.
\end{theorem}
Aus Gründen der Übersichtlichkeit werden wir den Beweis des Satzes aufteilen auf zwei Lemmata.
\begin{lemma}\label{lem:regex2nfa}
	Jede reguläre Sprache ist FA-erkennbar.
	\begin{proof}
		Wir stellen im Rahmen dieses Beweises eine Variante der sogenannten Thompson-Konstruktion vor, die einen regulären Ausdruck in einen äquivalenten $\varepsilon$-NFA umwandelt. Die Konstruktion nutzt den induktiven Aufbau von regulären Ausdrücken aus.\\
		Basisfälle (s. Abbildung~\ref{fig:thompson_basic}): 
		Wir geben drei $\varepsilon$-NFAs für die drei Basisfälle für reguläre Ausdrücke: 
		\begin{align*}
			\mathcal{A}_\emptyset &= (\{q_0\}, \Sigma, \emptyset, q_0, \emptyset),\\
			\mathcal{A}_\varepsilon &= (\{q_0\}, \Sigma, \emptyset, q_0, \{q_0\}),\\
			\mathcal{A}_a &= (\{q_0, q_1\}, \Sigma, \{(q_0, a, q_1)\}, q_0, \{q_1\}).
		\end{align*}
		\begin{figure}
			\centering
			\begin{subfigure}[b]{.25\textwidth}
				\centering
				\input{figs/thompson_empty}
				\caption{$r = \emptyset$}
				\label{fig:thompson_empty}
			\end{subfigure}~
			\begin{subfigure}[b]{.25\textwidth}
				\centering
				\input{figs/thompson_eps}
				\caption{$r = \varepsilon$}
				\label{fig:thompson_eps}
			\end{subfigure}~
			\begin{subfigure}[b]{.4\textwidth}
				\centering
				\input{figs/thompson_symbol}
				\caption{$r = a$}
				\label{fig:thompson_symbol}
			\end{subfigure}
			\caption{Basisfälle der Thompson-Konstruktion}
			\label{fig:thompson_basic}
		\end{figure}\\
		Rekursive Fälle (s. Abbildung~\ref{fig:thompson_recursive}):
		Seien für $e, e^\prime \in \mathsf{RE}_\Sigma$ die $\varepsilon$-NFAs $\mathcal{A}_e = (Q, \Sigma, \Delta, q_0, F)$ und $\mathcal{A}_{e^\prime} = (Q^\prime, \Sigma, \Delta^\prime, q_0^\prime, F^\prime)$ gegeben mit $q_{-1} \notin Q \cup Q^\prime$ und $Q \cap Q^\prime = \emptyset$. Dann erhalten wir $\varepsilon$-NFAs für die Ausdrücke $e+e^\prime, e \cdot e^\prime$ und $e^\ast$:
		\begin{align*}
			\mathcal{A}_{e+e^\prime} &= (Q \cup Q^\prime \cup \{q_{-1}\}, \Sigma, \Delta \cup \Delta^\prime \cup \{(q_{-1}, \varepsilon, q_0), (q_{-1}, \varepsilon, q_0^\prime)\}, q_{-1}, F \cup F^\prime),\\
			\mathcal{A}_{e \cdot e^\prime} &= (Q \cup Q^\prime, \Sigma, \Delta \cup \Delta^\prime \cup \{(q, \varepsilon, q_0^\prime) \mid q \in F \}, q_0, F^\prime),\\
			\mathcal{A}_{e^\ast} &= (Q \cup \{q_{-1}\}, \Sigma, \Delta \cup \{(q_{-1}, \varepsilon, q_0)\} \cup \{(q, \varepsilon, q_{-1}) \mid q \in F \}, q_{-1}, \{q_{-1}\}).
		\end{align*}
		
		\begin{figure}
			\centering
			\begin{subfigure}[b]{.9\textwidth}
				\centering
				\input{figs/thompson_plus}
				\caption{$r = e + e^\prime$}
				\label{fig:thompson_plus}
			\end{subfigure}\\
			\begin{subfigure}[b]{.9\textwidth}
				\centering
				\input{figs/thompson_cat}
				\caption{$r = e \cdot e^\prime$}
				\label{fig:thompson_cat}
			\end{subfigure}\\
			\begin{subfigure}[b]{.9\textwidth}
				\centering
				\input{figs/thompson_star}
				\caption{$r = e^\ast$}
				\label{fig:thompson_star}
			\end{subfigure}
			\caption{Rekursive Fälle der Thompson-Konstruktion}
			\label{fig:thompson_recursive}
		\end{figure}
		Wir zeigen nun noch die Korrektheit der Konstruktion. Dies geschieht per Induktion über den Aufbau des regulären Ausdrucks.\\
		Induktionsverankerung: $e \in \{\emptyset, \varepsilon, a\}$. Der Automat $\mathcal{A}_e$ wie in Abbildung~\ref{fig:thompson_basic} erkennt offensichtlich $\llbracket e \rrbracket$.\checkmark\\
		Induktionshypothese: Für die regulären Ausdrücke $e, e^\prime \in \mathsf{RE}_\Sigma$ erkennen die Automaten $\mathcal{A}_e = (Q, \Sigma, \Delta, q_0, F)$ und $\mathcal{A}_{e^\prime} = (Q^\prime, \Sigma, \Delta^\prime, q_0^\prime, F^\prime)$ die Sprachen $\llbracket e \rrbracket$ bzw. $\llbracket e^\prime \rrbracket$.\\
		Induktionsschritt: Wir zeigen nur den Fall für $e + e^\prime$, die anderen beiden gehen analog.
		Sei $w \in \llbracket e + e^\prime \rrbracket = \llbracket e \rrbracket \cup \llbracket e^\prime \rrbracket$. OBdA sei $w \in \llbracket e \rrbracket$. Demnach existiert ein Lauf $(r_0, \ldots, r_n)$ mit $r_n \in F$. Dann ist $(q_{-1}, r_0, \ldots, r_n)$ akzeptierender Lauf auf $\mathcal{A}_{e+e^\prime}$ und damit $w \in L(\mathcal{A}_{e+e^\prime})$.\\
		Sei umgekehrt $w \in L(\mathcal{A}_{e+e^\prime})$. Das heißt es gibt einen Lauf $(q_{-1}, r_0, \ldots, r_n)$ mit $r_n \in F \cup F^\prime$. Da $q_{-1}$ nur zwei $\varepsilon$-Transitionen hat muss $(r_0, \ldots, r_n)$ ebenfalls ein akzeptierender Lauf sein mit Startzustand $r_0$. Nach Konstruktion ist $r_0$ Startzustand von oBdA $\mathcal{A}_e$ und da $Q \cap Q^\prime = \emptyset$ muss $r_n \in F$ sein. Damit ist $w \in \llbracket e \rrbracket \subseteq \llbracket e+e^\prime \rrbracket$.
	\end{proof}
\end{lemma}
\begin{remark*}
	Wir sehen, dass die Thompson-Konstruktion einige $\varepsilon$-Tran\-si\-tio\-nen verwendet und auch gelegentlich den Zustandsraum unnötig vergrößert. Abseits von einigen Optimierungen dazu gibt es auch Verfahren, die ganz ohne $\varepsilon$-Transitionen auskommt (s. Glushkov-Konstruktion). In dieser Vorlesung wird diese aber nicht betrachten.
\end{remark*}
\begin{remark*}
	Der ursprüngliche Beweis von Lemma~\ref{lem:regex2nfa} von Stephen {C.} Kleene ging übrigens von einem regulären Ausdruck zu einem DFA, das Konzept ''Nichtdeterminismus`` wurde erst später eingeführt. Man kann sich vorstellen, dass der Beweis um einiges schwieriger war als diese recht anschauliche Konstruktion, u.a. waren auch algebraische Konzepte und Halbgruppentheorie involviert.
\end{remark*}
Ein Nebenprodukt der Thompson-Konstruktion aus dem Beweis ist folgendes Korollar.
\begin{corollary}
	Seien $L, K \subseteq \Sigma^\ast$ FA-erkennbare Sprachen. Dann sind auch $L \cdot K$ und $L^\ast$ FA-erkennbar.
\end{corollary}
\begin{example}
	Wir betrachten den regulären Ausdruck $r = (a+b)^\ast c$ und bauen nun einen $\varepsilon$-NFA für $r$. Die Automaten für die atomaren Teilausdrücke $a, b, c$ sind klar. In Abbildungen~\ref{fig:thompson_example_1} bis~\ref{fig:thompson_example_3} gehen wir Schritt für Schritt die Konstruktion durch. Man sieht wie der Automat aus dem vorigen Schritt in den Automaten aus dem aktuellen Schritt eingebettet ist. In Abbildung~\ref{fig:thompson_example_4} finden wir außerdem einen zustandsminimalen NFA, der sich recht schnell nur durch Ablesen des Ausdrucks hinschreiben lässt. Dies zeigt, dass auch bei einem solchen Minimalbeispiel bereits viele unnötige Zustände und Transitionen eingefügt werden in der Thompson-Konstruktion. Der Vorteil ist aber natürlich, dass ein Algorithmus immer funktioniert.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}{.99\textwidth}
		\centering
		\input{figs/thompson_example_1}
		\caption{Automat für $(a+b)$.}
		\label{fig:thompson_example_1}
	\end{subfigure}\\
	\begin{subfigure}{.99\textwidth}
		\centering
		\input{figs/thompson_example_2}
		\caption{Automat für $(a+b)^\ast$.}
		\label{fig:thompson_example_2}
	\end{subfigure}\\
	\begin{subfigure}{.99\textwidth}
		\centering
		\input{figs/thompson_example_3}
		\caption{Automat für $(a+b)^\ast c$.}
		\label{fig:thompson_example_3}
	\end{subfigure}\\
	\begin{subfigure}{.99\textwidth}
		\centering
		\input{figs/thompson_example_4}
		\caption{Minimaler NFA für $(a+b)^\ast c$.}
		\label{fig:thompson_example_4}
	\end{subfigure}
	\caption{Thompson-Konstruktion für $r = (a+b)^\ast c$.}
	\label{fig:thompson_example}
\end{figure}

Wir fahren nun fort mit dem zweiten Teil von Satz~\ref{thm:kleene_equivalence}. Wir schauen uns hier ein Verfahren an, was aus jedem endlichen Automaten einen regulären Ausdruck macht. Auch dazu gibt es verschiedene Varianten, u.a. auch grafische, die grundlegende Idee hinter diesen Verfahren ist jedoch meist die selbe.
\begin{lemma}\label{lem:nfa2regex}
	Jede FA-erkennbare Sprache ist regulär.
	\begin{proof}
		Wir gehen von einem NFA $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ aus und erzeugen einen ä\-qui\-va\-len\-ten regulären Ausdruck $r_\mathcal{A} \in \mathsf{RE}_\Sigma$. Die Idee des Verfahrens ist recht einfach. Wir suchen einen regulären Ausdruck der alle Wörter beschreibt mit denen man vom Startzustand $q_0$ zu einem der akzeptierenden Zustände $q \in F$ kommt ($\ast$). Wir benutzen nun folgende Notation: Für $P \subseteq Q$ und $p, q \in Q$ ist $r_P(p, q)$ der reguläre Ausdruck, der alle Wörter $w \in \Sigma^\ast$ beschreibt mit den man von $p$ nach $q$ kommt und dazwischen nur Zustände aus $P$ benutzt (für die Korrektheit später schreiben wir dafür $p \reachess{w}{P} q$). Anders gesagt ist $r_P(p, q)$ der reguläre Ausdruck zum Automaten $\mathcal{A} = (P, \Sigma, \Delta \cap ((P \cup \{p\}) \times \Sigma \times (P \cup \{q\})), p, \{q\})$. Bzgl. ($\ast$) suchen wir also den regulären Ausdruck
		$$
			r_\mathcal{A} = \sum_{q \in F} r_Q(q_0, q).
		$$
		Das Verfahren lässt sich rekursiv beschreiben nun mit folgender Überlegung: Ein regulärer Ausdruck $r_\emptyset(p, q)$ ist $\sum_{(p, a, q) \in \Delta} a$, also alle Symbole $a \in \Sigma$ mit denen eine direkte Transition von $p$ nach $q$ möglich ist (oder noch anschaulicher: Alle Symbole die im Transitionsgraphen auf der Kante von $p$ nach $q$ stehen). Sollte keine Transition existieren, dann ist der reguläre Ausdruck entsprechend $\emptyset$ für $p \neq q$ und $\varepsilon$ für $p = q$. Einmal formal aufgeschrieben:
		$$
			r_\emptyset(p, q) = \left\lbrace
				\begin{array}{*3{>{\displaystyle}l}p{5cm}}
					\ &\sum_{(p, a, q) \in \Delta} a, & p \neq q\\
					\varepsilon + &\sum_{(p, a, q) \in \Delta} a, & p = q.
				\end{array}
			\right.
		$$
		Der Rekursionsschritt funktioniert nun folgendermaßen: Für ein $P \neq \emptyset$ wählen wir einen Zustand  $s \in P$ und entfernen diesen mit folgender Umformung
		$$
			r_P(p, q) = r_{P^\prime}(p, q) + r_{P^\prime}(p, s) r_{P^\prime}(s, s)^\ast r_{P^\prime}(s, q),
		$$
wobei $P^\prime \coloneqq P \setminus \{ s \}$. Auch dies einmal in einfache Worte gefasst: Um von $p$ nach $q$ zu kommen über Zustände ausschließlich aus $P$ können wir entweder (links vom +) von $p$ nach $q$ gehen ohne einen Zustand $s$ zu besuchen oder (rechts vom +) von $p$ nach $s$ gehen, Pfade von $s$ nach $s$ benutzen und schließlich von $s$ nach $q$ gehen, wobei auch hier nur Zustände in $P \setminus \{ s \}$ benutzt werden (s. Abbildung~\ref{fig:state_elimination}).
		\begin{figure}
			\centering
			\input{figs/state_elimination}
			\caption{Rekursionsschritt um einen Zustand rauszuwerfen. Die blau getönte Fläche ist $P^\prime \coloneqq P \setminus \{ s \}$. Die größere Fläche ist $P$. Wichtig: Die Kanten in diesem Graphen sind keine Transitionen, sondern sind mit einem regulären Ausdruck beschriftet, der die Sprache zwischen den Knoten beschreibt mit Zuständen aus dem Index!}
			\label{fig:state_elimination}
		\end{figure}
	\end{proof}
\end{lemma}
\begin{remark*}
	Das Verfahren könnte einem auch aus Algorithmik-Vor\-le\-sung\-en bekannt vorkommen. Es handelt sich dabei um eine (zugegeben umständlich aufgeschriebene) Variante des Floyd-Warshall-Algorithmus mit dem sich zum Beispiel auch kürzeste Wege in gerichteten Graphen finden lassen.
\end{remark*}
\begin{example}\label{exp:fw_example}
	Wir betrachten den NFA in Abbildung~\ref{fig:fw_example}. Wir suchen also einen regulären Ausdruck, der die Worte beschreibt mit denen man von $q_0$ nach $q_0$ ($q_0$ ist einziger akzeptierender Zustand) kommt mit allen Zuständen, also den Ausdruck $r_Q(q_0, q_0)$. Wir wählen zunächst $s = q_1$ (eliminieren $q_1$):
	$$
		r_\mathcal{A} = r_Q(q_0, q_0) = r_{\{ q_0 \}}(q_0, q_0) + r_{\{ q_0 \}}(q_0, q_1) r_{\{ q_0 \}}(q_1, q_1)^\ast r_{\{ q_0 \}}(q_1, q_0)
	$$
	Noch zu berechnen sind $r_{\{ q_0 \}}(q_0, q_0), r_{\{ q_0 \}}(q_0, q_1), r_{\{ q_0 \}}(q_1, q_1), r_{\{ q_0 \}}(q_1, q_0)$. Wir berechnen $r_{\{ q_0 \}}(q_0, q_0)$ und eliminieren jetzt auch $q_0$ und können dann direkt den Basisfall einsetzen. Dabei vereinfachen wir die regulären Ausdrücke noch:
	\begin{align*}
		r_{\{ q_0 \}}(q_0, q_0) &= r_\emptyset(q_0, q_0) + r_\emptyset(q_0, q_0) r_\emptyset(q_0, q_0)^\ast r_\emptyset(q_0, q_0)\\
		&= \varepsilon + \varepsilon \varepsilon^\ast \varepsilon\\
		&\equiv \varepsilon.
	\end{align*}
	Wir berechnen nun $r_{\{ q_0 \}}(q_0, q_1)$ und eliminieren wieder $q_0$:
	\begin{align*}
		r_{\{ q_0 \}}(q_0, q_1) &= r_\emptyset(q_0, q_1) + r_\emptyset(q_0, q_0) r_\emptyset(q_0, q_0)^\ast r_\emptyset(q_0, q_1)\\
		&= c + \varepsilon \varepsilon^\ast c\\
		&\equiv c.
	\end{align*}
	Jetzt berechnen wir: $r_{\{ q_0 \}}(q_1, q_1)$ und eliminieren wieder $q_0$:
	\begin{align*}
		r_{\{ q_0 \}}(q_1, q_1) &= r_\emptyset(q_1, q_1) + r_\emptyset(q_1, q_0) r_\emptyset(q_0, q_0)^\ast r_\emptyset(q_0, q_1)\\
		&= (a + b + \varepsilon) + a \varepsilon^\ast c\\
		&\equiv a + b + \varepsilon + ac.
	\end{align*}
	Zuletzt berechnen wir $r_{\{ q_0 \}}(q_1, q_0)$ und eliminieren wieder den einzigen verbleibenden Zustand $q_0$:
	\begin{align*}
		r_{\{ q_0 \}}(q_1, q_0) &= r_\emptyset(q_1, q_0) + r_\emptyset(q_1, q_0) r_\emptyset(q_0, q_0)^\ast r_\emptyset(q_0, q_0)\\
		&= a + a \varepsilon^\ast \varepsilon\\
		&\equiv a.
	\end{align*}
	Diese Ausdrücke können wir nun rückwärts wieder einsetzen:
	\begin{align*}
		r_\mathcal{A} = r_Q(q_0, q_0) &= \varepsilon + c (a + b + \varepsilon + ac)^\ast  a\\
		&\equiv \varepsilon + c(a+b+ac)^\ast a.
	\end{align*}
	Da der Automat recht einfach ist, lässt sich hier natürlich auch ein einfacher Ausdruck einfach ablesen:
	$$
		r_\mathcal{A}^\prime = (c (a+b)^\ast a)^\ast \equiv r_\mathcal{A}.
	$$
\end{example}
\begin{figure}
	\centering
	\input{figs/fw_example}
	\caption{NFA $\mathcal{A}$ für Beispiel~\ref{exp:fw_example}.}
	\label{fig:fw_example}
\end{figure}
Die beiden Lemmata~\ref{lem:regex2nfa} und~\ref{lem:nfa2regex} ergeben zusammen den Äquivalenzsatz von Kleene. Mit den Sätzen~\ref{thm:regular_complement} und~\ref{cor:regular_intersection} können wir auch schließen, dass sich die regulären Ausdrücke auch um Operatoren für Komplement und Schnitt erweitern ließen, ohne dadurch zusätzliche Sprachen zu beschreiben. Dies ist nicht offensichtlich: Dazu versuche man einmal Komplement und Schnitt nur mit Hilfe von Vereinigung ($+$), Konkatenation ($\cdot$) und Iteration ($^\ast$) zu simulieren.\par
Wir werden nun anstelle von FA-erkennbaren Sprachen auch nur noch von regulären Sprachen sprechen.


\subsection{Weitere Abschlusseigenschaften}\label{sec:regular_closure2}
Wir haben bis hier hin reguläre Sprachen eine umfangreiche Charakterisierung gegeben, nämlich als Sprachen, die sich durch reguläre Ausdrücke oder Sprachen von endliche Automaten beschreiben lassen -- auf die Äquivalenz dieser Charakterisierungen haben wir lange hingearbeitet und mit dem Ä\-qui\-va\-lenz\-satz von Kleene einen schweren Brocken der theoretischen Informatik bewiesen. Dieser Abschnitt wird nun wieder deutlich einfacher. Wir haben bereits einige Abschlusseigenschaften regulärer Sprachen bewiesen: Schnitt, Vereinigung, Komplement, Konkatenation, Iteration usw., aber auch unter ''erfundenen`` Operatoren wie \textit{Perfect Shuffle}. Diese Abschlüsse allein zusammen mit dem Äquivalenzsatz von Kleene ergeben bereits genug Gründe um reguläre Sprachen als eine in wissenschaftlicher Hinsicht \textit{sinnvolle} Sprachklasse zu sehen. Wir werden in diesem Abschnitt nun ein paar weitere sinnvolle Abschlusseigenschaften beweisen.\par
\begin{definition}\label{def:stringhomomorphism}
	Seien $\Sigma, \Gamma$ Alphabete. Die  Abbildung $h\colon \Sigma^\ast \to \Gamma^\ast$ heißt \textit{(Wort-)Ho\-mo\-mor\-phis\-mus}, wenn für alle $a_0 \ldots a_{n-1} \in \Sigma^\ast$ gilt, dass
	$$
		h(a_0 \ldots a_{n-1}) = h(a_0) \ldots h(a_{n-1}).
	$$
\end{definition}
Insbesondere gilt bei einem Homomorphismus $h$, dass $h(uv) = h(u)h(v)$ und somit auch $h(\varepsilon) = \varepsilon$.
\begin{definition}
	Sei $L \subseteq \Sigma^\ast$ eine Sprache und $h\colon \Sigma^\ast \to \Gamma^\ast$ ein Homomorphismus. Dann ist
	$$
		h[L] \coloneqq \{h(w) : w \in L\}
	$$
	die \textit{Sprache unter dem Homomorphismus} $h$ von $L$ (über $\Gamma$).
\end{definition}
\begin{theorem}
	Sei $L \subseteq \Sigma^\ast$ eine reguläre Sprache und $h\colon \Sigma^\ast \to \Gamma^\ast$ ein Homomorphismus. Dann ist auch $h[L] \subseteq \Gamma^\ast$ regulär.
	\begin{proof}
		Sei $L \subseteq \Sigma^\ast$ regulär und $h$ Homomorphismus auf $\Sigma$. Da $L$ regulär existiert ein regulärer Ausdruck $e \in \mathsf{RE}_\Sigma$ mit $\llbracket e \rrbracket = L$. Wir können auf dem regulären Ausdruck den Homomorphismus anwenden, wobei wir Klammern, Punkte und Sterne ignorieren, dazu definieren wir die Erweiterung $\hat{h}$ von $h$ mit $\hat{h} \vert_\Sigma \equiv h$ und $\hat{h}(\sigma) = \sigma$ für $\sigma \in \{(,),\bm{+}, \bm{\cdot}, \overset{\bm{\ast}}{}\}$. Aus der Definition~\ref{def:stringhomomorphism} geht hervor, dass es genügt den Homomorphismus auf den einzelnen Symbolen $a \in \Sigma$ anzuwenden. Im Detail heißt das: 
		\begin{itemize}
			\item $\hat{h}(\bm{\emptyset}) = \bm{\emptyset}$,
			\item $\hat{h}(\bm{a}) = \bm{b}_0 \ldots \bm{b}_{n-1}$, wenn $h(a) = b_0 \ldots b_{n-1}$,
			\item $\hat{h}((r \bm{+} r^\prime)) = ((\hat{h}(r)) + (\hat{h}(r^\prime)))$,
			\item $\hat{h}((r \bm{\cdot} r^\prime)) = ((\hat{h}(r)) \bm{\cdot} (\hat{h}(r^\prime)))$,
			\item $\hat{h}((r)\overset{\bm{\ast}}{}) = (\hat{h}(r))\overset{\bm{\ast}}{}$.
		\end{itemize} 
		Es bleibt zu zeigen, dass $\llbracket \hat{h}(e) \rrbracket = h[L]$.
	\end{proof}
\end{theorem}
Wir betrachten nun eine weitere Abbildung:
\begin{definition}
	Die \textit{Reverse}-Operation ist definiert als
	$$
		\cdot^\mathcal{R} \colon \Sigma^\ast \to \Sigma^\ast, \quad\quad a_0 a_1 \ldots a_{n-1} \mapsto a_{n-1} \ldots a_1 a_0.
	$$
	Wir können sie auf Sprachen $L \subseteq \Sigma^\ast$ natürlich erweitern durch
	$$
		L^\mathcal{R} \coloneqq \{ w^\mathcal{R} : w \in L \}.
	$$
\end{definition}
Beachte, dass $w^\mathcal{R}$ wohldefiniert ist für jedes $w \in \Sigma
^\ast$ da jedes $w$ eine eindeutige Darstellung besitzt im freien Monoid $(\Sigma^\ast, \cdot)$.
\begin{remark*}
	Die Reverse-Operation ist kein Homomorphismus für $|\Sigma| > 1$, denn es gilt für z.B: $abb = (bba)^\mathcal{R}$. Wäre $\cdot^\mathcal{R}$ Homomorphismus müsste aber gelten $(bba)^\mathcal{R} = b^\mathcal{R} b^\mathcal{R} a^\mathcal{R} = bba \neq abb$. Eine zeichenweise Anwendung von $\cdot^\mathcal{R}$ ist also nicht möglich.
\end{remark*}
\begin{theorem}
	Sei $L \subseteq \Sigma^\ast$ regulär. Dann ist auch $L^\mathcal{R}$ regulär.
	\begin{proof}
		Das ist jedes Sommerstemester an der RWTH eine Übungsaufgabe.
	\end{proof}
\end{theorem}


\subsection{Nicht-reguläre Sprachen}\label{sec:regular_nonregular}
Bisher haben wir nur sehr einfache Sprachen betrachtet, die alle regulär waren. D.h. wir konnten zu jeder bisher betrachteten Sprache einen regulären Ausdruck angeben oder einen endlichen Automaten. Man könnte auf die Idee kommen, dass alle formalen Sprachen regulär sind. Dies ist aber nicht der Fall. Dass es mehr Sprachen als nur reguläre Sprachen geben muss folgt bereits aus kombinatorischen Argumenten. Dies heben wir uns aber für ein späteres Kapitel noch auf. Schauen wir uns zunächst ein Beispiel für eine nicht-reguläre Sprache an:
$$
	L_\mathbb{P} \coloneqq \{ a^p : p \in \mathbb{P} \}.
$$
$L_\mathbb{P}$ ist die Sprache aller (mit $a$) unär codierten Primzahlen, d.h. $L_\mathbb{P} = \{a^2, a^3, a^5, a^7, \ldots\}$. Dass $L_\mathbb{P}$ nicht regulär ist, ist auch nicht so schwer einzusehen: Intuitiv, wie würde ein regulärer Ausdruck aussehen? Natürlich könnten wir eine unendliche Vereinigung bilden (dies wäre jedoch nicht regulär, da reguläre Ausdrücke endlich sind):
$$
	r_\mathbb{P} = \sum_{p \in \mathbb{P}} a^p = a^2 + a^3 + a^5 + a^7 + \ldots
$$
Es erscheint klar, dass es jedoch keine Möglichkeit gibt diesen Ausdruck zu vereinfachen. In einem Automaten wird es noch deutlicher. Wenn wir mit endlich vielen Zuständen auskämen um genau alle unendlich vielen Primzahlcodierungen zu erkennen würden wir Kreise im Transitionsgraphen und somit eine gewisse Regelmäßigkeit in der Wiederholung von Primzahlen bekommen. Dies würde bedeuten, dass die Generierung neuer Primzahlen viel einfacher wäre als bisher angenommen, da wir ab einer bestimmten Primzahl einfach immer eine Konstante addieren könnten um die nächste zu bekommen. Das ist natürlich nicht der Fall. Eine schematische Erklärung dieses Arguments ist in Abbildung~\ref{fig:dfa_primes}. Egal an welcher und an wie vielen Stellen wir eine Zustandwiederholung zulassen (durch gestrichtelte Kante angedeutet), wir würden stets Codierungen akzeptieren, die nicht zu Primzahlen gehören können. In diesem Abschnitt werden wir uns ansehen, wie wir dieses Argument formalisieren und auf weitere nicht-reguläre Sprachen anwenden können.
\begin{figure}
	\centering
	\input{figs/dfa_primes}
	\caption{Wenn $L_\mathbb{P}$ regulär wäre gäbe es unendlich viele Primzahlen, die sich als $p + ck$ darstellen lassen, wobei $p \in \mathbb{P}, c \in \mathbb{N}, k = 0, 1, 2, \ldots$.}
	\label{fig:dfa_primes}
\end{figure}
Vorab: Mit diesem Abschnitt der Vorlesung haben viele Studenten Probleme, da das später vorgestellte Lemma etwas sperrig ist. Tatsächlich basiert dieses Lemma jedoch auf dem wahrscheinlich einfachsten Prinzip der Mathematik. Wir führen die Idee des Lemmas anhand dieses Prinzips einmal vor. Man sollte dieses Beispiel zunächst verstehen, dann sollte der Schritt zum Lemma geradezu trivial wirken.
\begin{example}\label{exp:pumping1}
	Die Sprache $L = \{a^n b^n : n \in \mathbb{N} \}$ ist nicht regulär.
	\begin{proof}
		Angenommen $L$ wäre regulär. Dann existiert ein endlicher Automat $\mathcal{A}$ mit $L(\mathcal{A}) = L$. Wir nehmen an, dass $\mathcal{A}$ $n$ Zustände besitzt. Betrachten wir nun das Wort $w = a^n b^n$ Offenbar ist $w \in L$, also gibt es einen akzeptierenden Lauf $\varrho = (r_0, \ldots, r_{2n})$. Das Wort hat die Länge $|w| = 2n \geq n$. Das heißt aber, dass spätestens nach Lesen des letzten $a$s sich im Lauf von $\mathcal{A}$ ein Zustand wiederholt haben muss. Dies folgt aus dem sogenannten \textit{pigeonhole principle} (oder deutsch: \textit{Schubfachprinzip}): Der Lauf auf den ersten $n$ Zeichen ist $(r_0, \ldots, r_n)$, d.h. er besteht aus $n+1$ Zuständen. Da $\mathcal{A}$ nur $n$ Zustände hat, muss mindestens einer doppelt im Lauf vorkommen. Wir nehmen an, dass $r_i = r_j$ mit oBdA $i < j$. Es ist auch klar, dass bis zu dieser Wiederholung im $j$-ten Schritt nur $a$s gelesen wurden, da die ersten $n$ Symbole von $w$ alle $a$ sind (d.h. $j \leq n$). Betrachten wir nun folgenden Lauf
		$$
			\varrho^\prime = (r_0, \ldots, r_i, r_{j+1}, \ldots, r_{2n})
		$$
		auf dem Wort $w^\prime = a^{n-k}b^n$ mit $k \coloneqq j-i > 0$. Der Lauf entsteht also durch das Weglassen der $a$s die sonst zwischen der Zustandswiederholung gelesen worden wären. Dieser Lauf ist gültig auf dem Automaten, denn $r_i = r_j$, d.h. die selben Transitionen die im $j$-ten Schritt möglich waren sind auch bereits im $i$-ten Schritt möglich. Außerdem gilt, dass $\varrho^\prime$ ein akzeptierender Lauf ist, da er auf $r_{2n} \in F$ endet. Also akzeptiert $\mathcal{A}$ das Wort $w^\prime$, welches aber echt weniger $a$s als $b$s besitzt (denn $k > 0$, weil $i < j$), also ist $w^\prime \notin L$. Der Automat $\mathcal{A}$ erkennt also nicht $L$. Da $\mathcal{A}$ und die Anzahl seiner Zustände $n$ belieibig gewählt wurde, existiert also kein endlicher Automat für $L$. Somit ist $L$ nicht regulär.
	\end{proof}
\end{example}
\begin{remark*}
	Statt die $a$s zwischen der Zustandswiederholung wegzulassen hätten wir natürlich auch beliebig viele weitere Wiederholungen hinzufügen können. Wir würden also den Lauf
	$$
		\varrho^{\prime\prime} = (r_0, \ldots, r_i, \ldots, r_j, r_{i+1} \ldots r_j, \ldots, r_{j+1}, \ldots, r_{2n})
	$$
	betrachten zu dem Wort $w^{\prime\prime} = a^{n+hk}b^n$ für ein beliebiges $h > 0$.
\end{remark*}
Dieses Beispiel ist sehr ausführlich jetzt besprochen worden. Sobald man diese Ü\-ber\-le\-gung\-en verstanden hat, hat man diesen gesamten Abschnitt eigentlich auch schon verstanden und sollte auch beim folgendem Lemma nicht mehr allzu große Schwierigkeiten haben. Tatsächlich bildet das Beispiel den Beweis des Lemmas auf einen einzelnen Automaten bzw. eine einzelne Sprache ab.
\begin{lemma}[Pumping-Lemma]\label{lem:pumping-lemma}
	Sei $L \subseteq \Sigma^\ast$ eine reguläre Sprache. Dann existiert ein $n \in \mathbb{N}_+$, sodass für alle $w \in L$ mit $|w| \geq n$ eine Zerlegung $w = xyz$ existiert mit den Eigenschaften:
	\begin{enumerate}
		\item $|xy| \leq n$,
		\item $y \neq \varepsilon$,
		\item $xy^iz \in L$ für alle $i \in \mathbb{N}$. 
	\end{enumerate}
	\begin{proof}
		Sei $L \subseteq \Sigma^\ast$ eine beliebige reguläre Sprache und $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$ ein NFA mit $L(\mathcal{A}) = L$. Wir setzen $n \coloneqq |Q|$. Betrachte nun ein belibiges Wort $w = a_0 \ldots a_{m-1} \in L$ mit Länge $|w| \eqqcolon m \geq n$. Falls kein solches Wort existiert sind wir bereits fertig. Ansonsten betrachten wir den Lauf $\varrho = (r_0, \ldots, r_m)$ von $\mathcal{A}$ auf $w$. Da $w \in L = L(\mathcal{A})$ ist $r_m \in F$. Wegen $m \leq n$ muss eine Zustandswiederholung in $\varrho$ vorkommen, spätestens nach Lesen des $n$-ten Zeichens. Seien also die Zustände $r_k$ und $r_j$ mit $0 \leq k < j \leq n$ gleich. Wir wählen die Zerlegung $x \coloneqq a_0 \ldots a_{k-1}, y \coloneqq a_k \ldots a_{j-1}, z \coloneqq a_j \ldots a_m$ und zeigen, dass diese die gewünschten Eigenschaften hat:
		\begin{enumerate}
			\item $|xy| = |a_0 \ldots a_{j-1}| = j \leq n$. \checkmark
			\item $y = a_k \ldots a_{j-1} \neq \varepsilon$, da nach Annahme $j-k > 0$. \checkmark
			\item Die Aussage gilt offensichtlich für $i = 1$. Für $i = 0$ betrachten wir nun den Lauf für das Wort $xz = xy^0z$: $(r_0, \ldots, r_k, r_{j+1}, \ldots, r_m)$. Dieser Lauf ist tatsächlich korrekt, denn $r_k = r_j \eqqcolon r$ und somit gilt $(r_k, a_j, r_{j+1}) = (r_j, a_j, r_{j+1}) \in \Delta$ (alle anderen Transitionen sind nach Annahme trivialerweise ebenfalls in $\Delta$). Außerdem gilt weiterhin $r_m \in F$. Also akzeptiert $\mathcal{A}$ das Wort $xz$. Ansonsten haben wir, dass $r \reaches{y} r$ und somit auch $r \reaches{y^i} r$ für $i \geq 2$.\checkmark \qedhere
		\end{enumerate}
	\end{proof}
\end{lemma}
\begin{figure}
	\centering
	\input{figs/pumping}
	\caption{Zustandswiederholung in $p$. Es gibt also einen Lauf von $q_0$ nach $p$ und einen Lauf von $p$ nach $q \in F$. Den Zwischenlauf von $p$ nach $p$ kann man also weglassen (oder auch beliebig oft wiederholen).}
	\label{fig:pumping}
\end{figure}
\begin{remark*}
	Wir stellen wieder fest, dass der Beweis des Pumping-Lemmas eine Verallgemeinerung des Vorgehens aus Beispiel~\ref{exp:pumping1} ist. Wir betrachten auch wieder einen Automaten mit $n$ Zuständen und akzeptierte Wörter, die zu lang sind, als dass sie dann ohne Wiederholung erkannt werden könnten. Das Weglassen oder Wiederholen von Infixen (\textit{pumpen}) entspricht dem Weglassen oder Wiederholen im Lauf.
	Eine Visualisierung des Prinzip ist in Abbildung~\ref{fig:pumping}.
\end{remark*}
\begin{remark*}
	Wir können mit dem Pumping-Lemma nicht zeigen, dass eine Sprache regulär ist. Es ist lediglich eine notwendige Bedingung für Regularität, aber keine hinreichende Bedingung. D.h. es gibt Sprachen, die nicht regulär sind, aber dennoch dem Pumping-Lemma genügen, z.B.
	$$
		K = \{a^m b^n c^n : m \in \mathbb{N}_+, n \in \mathbb{N} \} \cup \{ b^m c^n : m, n \in \mathbb{N} \}.
	$$
\end{remark*}
\begin{example}
	Wir betrachten die Sprache
	$$
		L = \{ ww^\mathcal{R} : w \in \Sigma^\ast \}
	$$
	aller Palindrome gerader Länge über $\Sigma = \{a, b\}$. $L$ ist nicht regulär.
	\begin{proof}
		Angenommen $L$ wäre regulär. Dann gilt das Pumping-Lemma. Sei also $n \in \mathbb{N}_+$ beliebig und wir betrachten das Wort $w = a^n bb a^n \in L$. Es hat die Länge $|w| = 2n+2 \geq n$. Wir betrachten nun Zerlegungen, die die Eigenschaften (i) und (ii) erfüllen und zeigen, dass (iii) dann nicht gelten kann. Wegen (i) ist $y = a^k$ für ein $k \leq n$ ($k = |y| \leq |xy| \leq n$, da die ersten $n$ Zeichen alle $a$ sind, kommt also auch kein $b$ in $y$ vor). Wegen (ii) gilt außerdem, dass $k > 0$. Betrachte nun das Wort $w^\prime \coloneqq xyyz = a^{n+k}bba^n$. Es liegt nicht in $L$, da $w^\prime$ kein Palindrom ist. Damit gilt die Eigenschaft (iii) für $i = 2$ nicht (gilt sogar für gar kein $i \neq 1$). Dies ist ein Widerspruch zum Pumping-Lemma, also kann $L$ nicht regulär sein. 
	\end{proof}
\end{example}

Es ist auch möglich, das Pumping-Lemma als Spiel aufzufassen. Das Vorgehen ist dabei identisch zu Model-Checking-Spielen für First Order Logic (\textit{Prädikatenlogik}), die man in der Vorlesung Mathematische Logik (normalerweise im 4. Semester) sieht. Wir beginnen mit den Spielern:
\begin{description}
	\item[Alice] versucht die Beweisführung der Nicht-Regualarität zu sabotieren,\\
		\textit{(Verifiziererin, Defender)}
	\item[Bob] möchte zeigen, dass eine Sprache nicht regulär ist.\\
		\textit{(Falsifizierer, Attacker)}
\end{description}
Die Spielregeln gehen wie folgt:
\begin{enumerate}[label=\arabic*)]
	\item Alice wählt eine Zahl $n \in \mathbb{N}_+$.
	\item Bob wählt ein Wort $w \in L$ mit $|w| \geq n$.
	\item Alice wählt eine Zerlegung $xyz = w$, mit $|xy| \leq n$ und $y \neq \varepsilon$.
	\item Bob wählt eine Zahl $i \in \mathbb{N}$.
\end{enumerate}
Alice gewinnt, wenn $xy^iz \in L$ oder Bob in Schritt 2 nicht ziehen kann (das wäre der Fall wenn jedes Wort aus $L$ kürzer ist als das $n$, welches Alice vorgegeben hat -- die Sprache wäre dann endlich und somit regulär). Bob gewinnt, wenn $xy^iz \notin L$.\\
Falls $L$ regulär ist, so hat Alice eine Gewinnstrategie, d.h. sie kann ein $n$ wählen, sodass sie für jedes Wort, mit dem Bob in Schritt 2 antworten könnte eine Zerlegung findet, die Bob in Schritt 4 nicht aus der Sprache rauspumpen kann. Äquivalent dazu: Falls Bob eine Gewinnstrategie hat, so ist $L$ nicht regulär.
\begin{remark*}
	Beachte, dass in Schritt 3 \textit{Alice} diejenige ist, die eine Zerlegung wählt. Wenn wir zeigen wollen, dass Bob eine Gewinnstrategie hat, heißt das, dass wir für \textit{alles} was Alice wählen könnte im 4. Schritt noch aus der Sprache herausgepumpt werden kann. Es wäre keine Gewinnstrategie, wenn Bob nur auf manche Züge von Alice eine passende Antwort findet.
\end{remark*}
\begin{example*}
	Wir kehren einmal zur Sprache $L_\mathbb{P}$ vom Anfang des Abschnitts zurück und geben eine Gewinnstrategie für Bob an um zu zeigen, dass $L_\mathbb{P}$ nicht regulär ist:
	\begin{enumerate}[label=\arabic*)]
	\item Alice wählt eine Zahl $n \in \mathbb{N}_+$.
	\item Bob wählt das Wort $a^q$ mit $q \geq n$ und $q \in \mathbb{P}$ (ein solches $q$ existiert wegen Satz~\ref{thm:primes}).
	\item Alice wählt eine Zerlegung $xyz = a^q$, mit $|xy| \leq n$ und $y \neq \varepsilon$, d.h. $y = a^k$ für ein $0 < k \leq n$.
	\item Bob wählt die Zahl $i = q+1$.
\end{enumerate}
Wir zeigen, dass $xy^{q+1}z \notin L_\mathbb{P}$:
\begin{align*}
	|xy^{q+1}z| &= |xyz| + |y^q|\\
	&= |a^q| + q |y|\\
	&= q + qk\\
	&= q(1 + k).
\end{align*}
Da $k > 0$ ist $1+k > 1$ und somit $q(1+k)$ eine zusammengesetzte Zahl (d.h. nicht prim). Also ist $xy^{q+1}z \notin L_\mathbb{P}$. Wir erhalten also eine Gewinnstrategie für Bob und somit ist $L_\mathbb{P}$ nicht regulär.
\end{example*}
Eine weitere Möglichkeit um zu zeigen, dass eine Sprache nicht regulär ist, ist zu zeigen, dass man mit Operationen von denen man weiß, dass sie Regularität erhalten (z.B. Schnitt, Homomorphismen, usw.) aus einer Sprache $L$ eine Sprache $L^\prime$ zu konstruieren, von der man weiß, dass sie nicht regulär ist. Dann kann auch $L$ schon nicht regulär gewesen sein.
\begin{example}
	Wir wollen wissen, ob die Sprache $L = \{w \in \{a, b\}^\ast \mid |w|_a = |w|_b\}$ regulär ist. Wir verwenden als Hilfssprache $\llbracket a^\ast b^\ast \rrbracket$. Dann erhalten wir mit $L \cap \llbracket a^\ast b^\ast \rrbracket = \{a^n b^n : n \in \mathbb{N}\} \eqqcolon L^\prime$. Von $L^\prime$ wissen wir bereits aus Beispiel~\ref{exp:pumping1}, dass sie nicht regulär ist. Also ist auch $L$ nicht regulär, denn reguläre Sprachen sind unter Schnitt abgeschlossen (Satz~\ref{thm:regular_intersection}) und wir haben $L$ mit einer regulären Sprache geschnitten um $L^\prime$ zu erhalten.
\end{example}


\subsection{Myhill-Nerode-Äquivalenz}\label{sec:regular_myhill-nerode}
Bisher haben wir reguläre Sprachen durch Formalismen wie reguläre Ausdrücke und endliche Automaten charakterisiert. Nun wollen wir eine Eigenschaft finden, die reguläre Sprachen auf einer stark mathematisch-strukturellen Ebene charakterisiert. Dazu erinnern wir uns an unser freies Monoid $(\Sigma^\ast, \cdot, \varepsilon)$, welches bereits in Abschnitt~\ref{sec:regular_closure2} uns einmal angesehen haben. Wir definieren uns eine Äquivalenzrelation (siehe Definition~\ref{def:relations}), d.h. wir teilen zu einem Alphabet $\Sigma$ die Wörter aus $\Sigma^\ast$ in Äquivalenzklassen ein. Tatsächlich definiert die gleich eingeführte Relation sogar noch etwas mehr, deshalb schieben wir eine weitere Definition hier ein.
\begin{definition}
	Sei $A$ eine Menge und $\bullet$ eine zweistellige Funktion auf $A$. Eine Relation $\sim \subseteq A \times A$ heißt \textit{rechtsseitige Kongruenz} (bezüglich $\bullet$), wenn
	\begin{enumerate}
		\item $\sim$ eine Äquivalenzrelation ist und
		\item $\bullet$ diese Relation respektiert (d.h. für alle $a \sim b$ und alle $c \in A$ gilt $a \bullet c \sim b \bullet c$).
	\end{enumerate}
\end{definition}
Wir definieren nun wie angekündigt unsere Äquivalenzrelation auf Wörtern.
\begin{definition}
	Sei $\Sigma$ ein Alphabet und $L \subseteq \Sigma^\ast$. Seien $u, v \in \Sigma^\ast$. $u$ und $v$ heißen \textit{Myhill-Nerode-äquivalent bezüglich $L$} ($u \sim_L v$) genau dann, wenn für alle $w \in \Sigma^\ast$ gilt, dass $uw \in L$ g.d.w. $vw \in L$.
\end{definition}
Diese Relation ist tatsächlich sogar nicht nur eine Äquivalenz sondern auch eine rechtsseitige Kongruenz bezüglich $\cdot$, deswegen sind auch die Begriffe \textit{Myhill-Nerode-Rechtskongruenz} und -- nicht ganz korrekterweise -- \textit{Myhill-Nerode-Kongruenz} gebräuchlich. Dies wollen wir im folgenden zeigen.
Eine einfacher Folgerung aus der Definition ist
\begin{lemma}\label{lem:mncongruence}
	Sei $L \subseteq \Sigma^\ast$ eine Sprache. Für alle $u, v, w \in \Sigma^\ast$ mit $u \sim_L v$ gilt $uw \sim_L vw$.
	\begin{proof}
		Wir zeigen zunächst, dass $\sim_L$ eine Äquivalenzrelation ist. Offensichtlich ist $\sim_L$ reflexiv und symmetrisch. Angenommen nun $u \sim_L v$ und $v \sim_L w$, aber $u \not\sim_L w$, d.h. es gibt ein trennendes Wort $x$ mit oBdA $u \cdot x \in L$ und $w \cdot x \notin L$. Dann ist $v \cdot x \notin L$ wegen $v \sim_L w$. Daraus folgt aber auch, dass $u \not\sim_L v \lightning$.\\
		Sei nun $u \sim_L v$ und $w \in \Sigma^\ast$ beliebig. Angenommen $uw \not\sim_L vw$, d.h. es gibt ein trennendes Wort $x \in \Sigma^\ast$ mit oBdA $uw \cdot x \in L$ und $vw \cdot x \notin L$. Dann ist aber $wx$ ein trennendes Wort für $u$ und $v$ und somit $u \not\sim_L v \lightning$.\\
		Ingesamt gilt also, dass $\sim_L$ rechtsseitige Kongruenzrelation bezüglich $\cdot$ ist.
	\end{proof}
\end{lemma}
Wir verwenden etwas vereinfachte Schreibweisen für Myhill-Nerode-Äquivalenz: Anstatt $[w]_{\sim_L}$ schreiben wir nur $[w]_L$ und für $\Sigma^\ast /_{\sim_L}$ schreiben wir $\Sigma^\ast /_L$. Außerdem schreiben wir $\ind(L)$ für $\ind(\sim_L)$.
\begin{example}\label{exp:mnf2reg}
	Wir bestimmen die Myhill-Nerode-Äquivalenzklassen und den Index der Sprache 
	$$
		L = \{w \in \{a, b\}^\ast \mid w \text{ hat Infix } ab\}.
	$$
	Wir betrachten $\sim_L$ und die Wörter über $\{a, b\}$:
	\begin{itemize}
		\item $\varepsilon \not\sim_L a$, denn $b$ ist trennendes Wort: $\varepsilon \cdot b = b \notin L$, aber $a \cdot b = ab \in L$.
		\item $\varepsilon \sim_L b$, denn es gilt $\varepsilon \cdot w \in L$  g.d.w. $b \cdot w \in L$, nämlich genau dann, wenn $w$ das Infix $ab$ beinhaltet.
		\item $a \sim_L aa$, denn es gilt $a \cdot w \in L$ g.d.w. $aa \cdot w \in L$, nämlich genau dann, wenn $w$ mit $b$ beginnt oder das Infix $ab$ beinhaltet.
		\item $a \not\sim_L ab$, denn $\varepsilon$ ist trennendes Wort: $a \cdot \varepsilon = a \notin L$, aber $ab \cdot \varepsilon = ab \in L$.
		\item $\varepsilon \not\sim_L ab$, denn $\varepsilon$ ist trennendes Wort: $\varepsilon \cdot \varepsilon = \varepsilon \notin L$, aber $ab \cdot \varepsilon = ab \in L$.
	\end{itemize}
	Wir behaupten nun, dass $[\varepsilon]_L, [a]_L, [ab]_L$ alle Myhill-Nerode-Äquivalenzklassen sind (und damit, dass $\ind(L) = 3$ ist).
	\begin{proof}
		Bereits gezeigt haben wir, dass diese drei Myhill-Nerode-Äquivalenzklassen jeweils verschieden sind. Es bleibt zu zeigen, dass $[\varepsilon]_L \cup [a]_L \cup [ab]_L = \{a, b\}^\ast$, also dass $\{[\varepsilon]_L, [a]_L, [ab]_L\}$ eine Partition von $\{a, b\}^\ast$ ist. Wir zeigen, dass für jedes Wort $v \in \{a, b\}^\ast$ eine der drei Möglichkeiten gilt $v \sim_L \varepsilon, v \sim_L a, v \sim_L ab$.
		\begin{enumerate}[label=\arabic*)]
			\item $v$ enthält $ab$ als Infix. Dann gilt $v \cdot x \in L$ für alle $x \in\{a, b\}^\ast$. Auch $ab \cdot x \in L$ gilt für alle $x \in \{a, b\}^\ast$, somit $v \sim_L ab$.
			\item $v$ enthält $ab$ nicht als Infix und endet mit $a$. Dann $v \cdot x \in L$ g.d.w. $x$ mit $b$ beginnt oder $ab$ als Infix enthält. Für genau die selben $x$ gilt $a \cdot x \in L$. Also ist $v \sim_L a$.
			\item Die oberen beiden Fälle treffen nicht zu, d.h. $v$ enthält $ab$ nicht als Infix und endet auch nicht mit $a$ (d.h. $v \in \llbracket b^\ast \rrbracket$). Dann ist $v \cdot x \in L$ g.d.w. $x$ das Infix $ab$ enthält. Offensichtlich sind das genau die selben $x$ mit denen man auch von $\varepsilon$ in $L$ landet, somit $v \sim_L \varepsilon$.
		\end{enumerate}
		Jedes Wort fällt in eines dieser drei Fälle, also haben wir die Behauptung gezeigt. Außerdem liefert uns diese Betrachtung eine Beschreibung der Äquivalenzklassen:
		\begin{enumerate}[label=\arabic*)]
			\item $[ab]_L = L = \llbracket (a+b)^\ast ab (a+b)^\ast \rrbracket$,
			\item $[a]_L = \{w \mid w \text{ enthält kein } ab \text{ und endet auf } a\} = \llbracket b^\ast a^+ \rrbracket$,
			\item $[\varepsilon]_L = \{w \mid w \text{ enthält kein } ab \text{ und endet nicht auf } a\} = \llbracket b^\ast \rrbracket$.
		\end{enumerate}
		Außerdem folgt natürlich, dass $\ind(L) = 3$.
	\end{proof}
\end{example}
In dem Beispiel haben wir bereits auf eine sehr algorithmische Weise die Myhill-Nerode-Äquivalenzklassen bestimmt. Auch wenn wir nochmal bewiesen haben, dass die drei angegebenen Äquivalenzklassen tatsächlich die geforderte Partition bilden, so war dies nicht mehr wirklich nötig mit der vorangegangenen Betrachtung. Denn Lemma~\ref{lem:mncongruence} lieferte bereits die Aussage, dass es nicht mehr Äquivalenzklassen geben kann:
\begin{itemize}
	\item $\varepsilon \sim_L bb$, denn $\varepsilon \sim_L b$ und (wegen Lemma~\ref{lem:mncongruence}) $\varepsilon \cdot b \sim_L b \cdot b$. Mit Transitivität folgt auch $\varepsilon \sim_L bb$.
	\item $\varepsilon \not\sim_L aa$, denn $aa \sim_L a \not\sim_L \varepsilon$. Aus Symmetriegründen folgt dann auch $aa \not\sim_L \varepsilon$.
	\item $a \sim_L aaa$, denn $a \sim_L aa$ und damit auch $a \cdot a \sim_L aa \cdot a$. Mit Transitivität erhalten wir dann auch wieder $a \sim_L aaa$ und induktiv auch $a \sim_L a^k$ für beliebige $k \in \mathbb{N}_+$.
\end{itemize}
Das bedeutet, wenn man bereits für alle Wörter bis zu einer bestimmten Länge alle Myhill-Nerode-Äquivalenzklassen gefunden hat und für Wörter der nächstgrößeren Länge keine neuen mehr hinzugekommen sind, so hat man bereits alle Klassen gefunden. Alle noch längeren Wörter sind dann bereits ebenfalls äquivalent zu einem kürzeren Wort. 
\begin{lemma}
	Sei $L \subseteq \Sigma^\ast$ eine Sprache und $w \in \Sigma^\ast$. Falls $w$ kleinster Repräsentant (bzgl. $\canon$) einer Myhill-Nerode-Äquivalenzklasse $[w]_L$ ist, so ist bereits jedes Präfix von $w$ ebenfalls kleinster Repräsentant seiner Myhill-Nerode-Äquivalenzklasse.
	\begin{proof}
		Sei $u \sqsubseteq w$, d.h. es gibt ein $x \in \Sigma^\ast$ mit $ux = w$, sodass $u \in [v]_L$ für ein $v \canon u$. Dann ist aber nach Lemma~\ref{lem:cn} $vx \in [w]_L$, aber $vx \canon ux = w \lightning$.
	\end{proof}
\end{lemma}
\begin{corollary}\label{cor:mn_allfound}
	Sei $\mathcal{P}_L$ eine Menge von Myhill-Nerode-Äquivalenzklassen bzgl. einer Sprache $L \subseteq \Sigma^\ast$. Falls für alle $[u]_L \in \mathcal{P}_L$ und alle $a \in \Sigma$ gilt, dass $ua \sim_L w$ für ein $w \canon ua$ mit $[w]_L \in \mathcal{P}_L$, so gilt bereits für alle $v \in a\Sigma^+, a \in \Sigma$, dass $uv \sim_L w$ für ein $w \canon uv$ mit $[w]_L \in \mathcal{P}_L$. (Und somit gilt, dass $\bigcup_{w \in \mathcal{P}} [w]_L = \Sigma^\ast$.)
	\begin{proof}
		Angenommen für alle $[u]_L\in \mathcal{P}_L$ und alle $a \in \Sigma$ ist $ua$ bereits Myhill-Nerode-äquivalent zu einem kleineren Repräsentanten $w \in \mathcal{P}$, aber es gibt ein $v \in a\Sigma^+$ mit $uv \notin [w]_L$ für alle $[w]_L \in \mathcal{P}$. Wähle $v$ dabei minimal. $uv$ repräsentiert also eine neue Myhill-Nerode-Äquivalenzklasse, d.h. es gibt kein kanonisch kleineres Wort, welches zu $[uv]_L$ gehört. Nach vorigem Lemma ist dann aber auch jedes Präfix von $uv$ kleinster Repräsentant einer Myhill-Nerode-Äquivalenzklasse. Insbesonder gilt das für $ua$. $\lightning$ zur Annahme, dass $[ua]_L$ einen kleineren Repräsentanten bereits hat.
	\end{proof}
\end{corollary}
Dieses Korollar liefert intuitiv den Algorithmus~\ref{alg:mnc} zum Finden von Myhill-Nerode-Ä\-qui\-va\-lenz\-klas\-sen, denn wir wissen nun, dass wir alle Klassen gefunden haben, wenn ein Anhängen eines(!) beliebigen weiteren Symbols keine neue Klasse bringt.
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\underline{Myhill-Nerode-Klassifizierer}{($L \subseteq \Sigma^\ast$)}\\
	\Output{Myhill-Nerode-Äquivalenzklassen $\mathcal{P} = \{[w]_L : w \in \Sigma^\ast\}$.}
	$\mathcal{P} \coloneqq \{[\varepsilon]_L\}$\\
	\For{$[u]_L \in \mathcal{P}$}{
		\For{$a \in \Sigma$}{
			\If{$ua \not\sim_L v$ für alle $[v]_L \in \mathcal{P}$}{
				$\mathcal{P} \coloneqq \mathcal{P} \cup \{[ua]_L\}$\\
			}
		}
	}
	\caption{Induktives Finden von Myhill-Nerode-Äquivalenzklassen}
	\label{alg:mnc}
\end{algorithm}
Beachte, dass dieses Verfahren nur für reguläre Sprachen terminiert. Nicht-reguläre Sprachen haben unendlich viele Äquivalenzklassen. Dies ist ein sehr starkes Resultat, denn es gibt uns eine notwendige und hinreichende Bedingung für Regularität und ist neben dem Äquivalenzsatz von Kleene sicher das schönste Theorem in dieser Vorlesung. Die Details geben wir im folgenden Satz.
\begin{theorem}[Satz von Nerode]\label{thm:nerode}
	Eine Sprache ist genau dann regulär, wenn die Anzahl der Myhill-Nerode-Äquivalenzklassen endlich ist.
\end{theorem}
Die Sprache $L$ im Beispiel~\ref{exp:mnf2reg} hat $\ind(L) = 3 < \infty$ und ist somit regulär. Wir betrachten nun ein Beispiel für eine nicht-reguläre Sprache.
\begin{example}
	Sei $L = \{w \in \{(, )\} \mid w \text{ korrekt geklammert}\}$ Korrekt geklammert meint, dass $|w|_( = |w|_)$ und für jedes Präfix $u \sqsubseteq w$ gilt $|u|_( \geq |u|_)$. Wir zeigen, dass $L$ unendlich viele Myhill-Nerode-Äquivalenzklassen hat. Dann folgt mit dem Satz~\ref{thm:nerode}, dass $L$ nicht regulär ist.
	Sei $u_k \coloneqq (^k$ und betrachte für zwei beliebige $i \neq j$ die Wörter $u_i, u_j$. Es gilt $u_i \not\sim_L u_j$, denn $v = )^i$ trennt die beiden Worte: $u_i \cdot v = (^i)^i \in L$, aber $u_j \cdot v = (^j)^i \notin L$. Also liegen $u_i$ und $u_j$ in zwei verschiedenen Myhill-Nerode-Äquivalenzklassen. Da es unendlich viele $i, j \in \mathbb{N}$ gibt mit $i \neq j$, gibt es demnach auch unendlich viele Myhill-Nerode-Äquivalenzklassen für $L$. Also ist $L$ nicht regulär.
\end{example}
Den Beweis von Satz~\ref{thm:nerode} teilen wir auf zwei Lemmata auf.
\begin{lemma}\label{lem:dfa2mnf}
	Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein DFA und $L = L(\mathcal{A})$. Dann gilt
	$$
		\ind(L) \leq |Q|.
	$$
	\begin{proof}
		Für alle $v \in \Sigma^\ast$ sei $q_v \in Q$ der (eindeutige) Zustand mit $\mathcal{A}: q_0 \reaches{v} q_v$. Nun gilt für alle $v, w \in \Sigma^\ast$ mit $q_v = q_w$, dass $v \sim_L w$. Sonst existiert ein $x \in \Sigma^\ast$ mit oBdA $vx \in L$ und $wx \notin L$. Dann wäre aber $\mathcal{A}: q_0 \reaches{v} q_v \reaches{x} q \in F$ und $\mathcal{A}: q_0 \reaches{w} q_w = q_v \reaches{x} q \notin F \lightning$. Mit Kontraposition gilt äquivalent, dass wenn $v \not\sim_L w$, dann gilt $q_v \neq q_w$. Also folgt, dass es maximal so viele Myhill-Nerode-Äquivalenzklassen gibt wie Zustände in $\mathcal{A}$.
	\end{proof}
\end{lemma}
\begin{corollary}\label{cor:reg2mnf}
	Für jede reguläre Sprache $L$ gilt $\ind(L) < \infty$.
\end{corollary}
Wir müssen nun noch zeigen, dass ein endlicher Index zu einer Sprache impliziert, dass $L$ regulär ist.
\begin{lemma}\label{lem:mnf2reg}
	Sei $L \subseteq \Sigma^\ast$ eine Sprache mit $\ind(L) < \infty$. Dann gibt es einen DFA für $L$.
	\begin{proof}
		Wir geben den DFA (\textit{Myhill-Nerode-DFA}) explizit an:
		$$
			\mathcal{A}_L = (\Sigma^\ast/_L, \Sigma, \delta_L, [\varepsilon]_L, F_L),
		$$
		mit $\delta_L([u]_L, a) = [ua]_L$ und $F_L = \{[u]_L : u \in L\}$. Beachte, dass $\Sigma^\ast/_L$ wegen $\ind(L) < \infty$ endlich ist, aber nicht leer, da $\Sigma^\ast/_L$ eine Partition von $\Sigma^\ast$ ist. Außerdem ist $\Sigma$ selbst ebenfalls endlich und nicht leer. Zusätzlich gilt $[\varepsilon]_L \in \Sigma^\ast/_L$ und $F_L \subseteq \Sigma^\ast/_L$. Zuletzt ist $\delta_L$ wohldefiniert wegen Lemma~\ref{lem:mncongruence}, denn 
		$$
			\delta_L([u]_L, a) = [ua]_L = [u^\prime a]_L = \delta([u^\prime]_L, a)
		$$
		für alle $u \sim_L u^\prime$ und alle $a \in \Sigma$. Also ist $\mathcal{A}_L$ wirklich ein DFA. Es bleibt zu zeigen, dass $L(\mathcal{A}_L) = L$.\\
		Zunächst zeigen wir per Induktion, dass für alle $w \in \Sigma^\ast$ gilt: $\delta_L^\ast([\varepsilon]_, w) = [w]_L$.\\
		Induktionsverankerung: Für $w = \varepsilon$ gilt offensichtlich $\delta_L^\ast([\varepsilon]_L, \varepsilon) = [\varepsilon]_L$. \checkmark\\
		Induktionsschritt: Sei $[u]_L \in \Sigma^\ast/_L$, sodass $\delta_L^\ast([\varepsilon]_L, u) = [u]_L$. Nach Definition von $\delta_L$ gilt dann für alle $a \in \Sigma$, dass $\delta_L([u]_L, a) = [ua]_L$ und damit insgesamt
		\begin{align*}
			\delta_L^\ast([\varepsilon]_L, ua) &= \delta_L(\delta_L^\ast([\varepsilon]_L, u), a)\\
			&\overset{\text{IH}}{=} \delta_L([u]_L, a)\\
			&= [ua]_L.
		\end{align*}
		Also gilt die Aussage für alle $w \in \Sigma^\ast$. Nach Konstruktion des Myhill-Nerode-DFA ist außerdem $[w]_L$ akzeptierender Zustand g.d.w. $w \in L$. Beachte, dass in dem Fall auch kein $v \in [w]_L$ mit $v \notin L$ existiert, denn für $v, w$ wäre dann $\varepsilon$ ein trennendes Wort. Also akzeptiert $\mathcal{A}_L$ genau $L$.
	\end{proof}
\end{lemma}
Die beiden Lemma~\ref{lem:mnf2reg} und Korollar~\ref{cor:reg2mnf} ergeben zusammen den Satz von Nerode (Satz~\ref{thm:nerode}). Wir werden auch in Zukunft den Myhill-Nerode-DFA zu einer Sprache $L$ stets mit $\mathcal{A}_L$ bezeichnen. In Abbildung~\ref{fig:mndfa} sehen wir den Myhill-Nerode-DFA für die Sprache $L$ aus Beispiel~\ref{exp:mnf2reg}.
\begin{figure}
	\centering
	\input{figs/mndfa}
	\caption{Myhill-Nerode-DFA für die Sprache aus Beispiel~\ref{exp:mnf2reg}.}
	\label{fig:mndfa}
\end{figure}


\subsection{Minimierung von Automaten}\label{sec:regular_minimization}
Bisher haben wir uns die Theorie regulärer Sprachen und endlicher Automaten angesehen. In den übrigen Abschnitten dieses Kapitels kommen wir zu etwas angewandteren Themen. Wir schauen uns also an, wozu die Theorie, die wir bisher gelernt haben nützlich ist. Einige Konzepte wie der Myhill-Nerode-DFA hat natürlich eine \textit{intrinsische} Anwendung in der Automatentheorie: Wir haben gesehen, dass es keine kleineren DFAs (bzgl. Anzahl der Zustände) als $\mathcal{A}_L$ gibt. Mit Blick auf Speichereffizienz ist es also in unserem Interesse zu jeder Sprache den Myhill-Nerode-DFA zu finden -- im Optimalfall mit einem effizientem Algorithmus. Dieser Abschnitt beschäftigt sich mit genau diesem Thema. \textit{Extrinsische} Anwendungen, d.h. wozu das ganze dann am Ende \textit{wirklich} zu gebrauchen ist, folgt im Abschnitt darauf.
\begin{remark*}
	Wir nennen einen Algorithmus \textit{effizient}, wenn seine Laufzeit höchstens polynomiell in der Länge der Eingabe wächst.
\end{remark*}
\begin{remark*}
	Ein effizienter Algorithmus zur Minimierung von NFAs ist bisher nicht bekannt.
\end{remark*}
Im folgenden verwenden wir für einen DFA $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ die verkürzende Schreibweise $|\mathcal{A}| \coloneqq |Q|$ für die Größe des Automaten.
\begin{definition}
	Ein DFA $\mathcal{A}$ heißt \textit{minimal}, wenn für alle $\mathcal{A}^\prime$ mit $L(\mathcal{A}) = L(\mathcal{A}^\prime)$ gilt $|\mathcal{A}| \leq |\mathcal{A}^\prime|$.
\end{definition}
\begin{theorem}\label{thm:mndfamin}
	Der Myhill-Nerode-DFA aus Lemma~\ref{lem:mnf2reg} ist minimal.
	\begin{proof}
		Für den Myhill-Nerode-DFA $\mathcal{A}_L$ zu einer Sprache $L$ gilt $|\mathcal{A}_L| = \ind(L)$. Außerdem gilt $\ind(L) \leq |\mathcal{A}|$ für alle $\mathcal{A}$ mit $L(\mathcal{A}) = L$ nach Korollar~\ref{cor:reg2mnf}. Insgesamt also $|\mathcal{A}_L| = \ind(L) \leq |\mathcal{A}|$ für alle $\mathcal{A}$ mit $L(\mathcal{A}) = L = L(\mathcal{A}_L)$. Also ist $\mathcal{A}_L$ minimal für $L$.
	\end{proof}
\end{theorem}
Im einführenden Abschnitt~\ref{sec:regular_dfa} zur DFAs wurde bereits erwähnt, dass sich Automaten auch als Strukturen auffassen lassen wie in Abschnitt~\ref{sec:pre_structures} auffassen lassen, ohne im Detail darauf einzugehen. Natürlich lässt sich dann auch ein entsprechender Isomorphiebegriff für Automaten definieren, was wir an dieser Stelle einmal explizit machen.
\begin{definition}
	Seien $\mathcal{A}_1 = (Q_1, \Sigma, \delta_1, q_0^1, F_1)$ und $\mathcal{A}_2 = (Q_2, \Sigma, \delta_2, q_0^2, F_2)$ zwei DFAs. Ein \textit{Isomorphismus} von $\mathcal{A}_1$ auf $\mathcal{A}_2$ ist eine bijektive Abbildung $f\colon Q_1 \to Q_2$ mit
	\begin{itemize}
		\item $f(\delta_1(q, a)) = \delta_2(f(q), a)$ für alle $q \in Q_1, a \in \Sigma$,
		\item $f(q_0^1) = q_0^2$,
		\item $f[F_1] = F_2$.
	\end{itemize}
	Man schreibt dann auch $f\colon \mathcal{A}_1 \cong \mathcal{A}_2$. Zwei DFAs $\mathcal{A}_1, \mathcal{A}_2$ heißen \textit{isomorph} ($\mathcal{A}_1 \cong \mathcal{A}_2$), wenn ein Isomorphismus zwischen ihnen existiert.
\end{definition}
Wenn zwei Automaten (oder auch zwei andere beliebige Strukturen) isomorph sind, dann sind sie strukturell gleich, d.h. die Elemente oder Funktionssymbole sind bloß umbenannt. Es ist auch in der Mathematik nicht unüblich Strukturen nur \textit{bis aus Isomorphie} hin zu untersuchen.
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/dfa_iso_a}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/dfa_iso_b}
	\end{subfigure}
	\caption{Zwei zueinander isomorphe DFAs.}
	\label{fig:dfa_iso}
\end{figure}
\begin{example}
	Betrachte die DFAs $\mathcal{A}_1$ und $\mathcal{A}_2$ in Abbildung~\ref{fig:dfa_iso}. Folgende Abbildung ist ein Isomorphismus von $\mathcal{A}_1$ auf $\mathcal{A}_2$:
	$$
		f(q_0) = q_0^\prime, \quad f(q_1) = q_2^\prime, \quad f(q_2) = q_3^\prime, \quad f(q_3) = q_1^\prime.
	$$
\end{example}
\begin{theorem}\label{thm:mindfaunique}
	Sei $\mathcal{A}$ ein minimaler DFA für $L$. Dann ist $\mathcal{A} \cong \mathcal{A}_L$.
	\begin{proof}
		Sei $L \subseteq \Sigma^\ast$. Wir betrachten wieder den Myhill-Nerode-DFA aus Lemma~\ref{lem:mnf2reg} $\mathcal{A}_L = (\Sigma^\ast/_L, \Sigma, \delta_L, [\varepsilon]_L, F_L)$. Sei außerdem $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein minimaler DFA für $L$. Wie im Beweis von Lemma~\ref{lem:dfa2mnf} sei für alle $v \in \Sigma^\ast$, $q_v \in Q$ der eindeutige Zustand mit $\mathcal{A}: q_0 \reaches{v} q_v$ (oder $\delta^\ast(q_0, v) = q_v$). Wir haben schon gezeigt, dass wenn $q_v = q_w$ für zwei $v, w \in \Sigma^\ast$, dass $v \sim_L w$ gilt. Beachte außerdem, dass für alle $q \in Q$ ein $v \in \Sigma^\ast$ mit $q_v = q$ existiert (wenn nicht, wäre dieses $q$ unerreichbar von $q_0$ aus -- dann wäre aber $\mathcal{A}$ nicht minimal, da man $q$ einfach weglassen könnte und einen kleineren, äquivalenten DFA erhalten würde). Also ist $Q = \{q_v : v \in \Sigma^\ast\}$. Wir definieren nun einen Isomorphismus von $\mathcal{A}$ auf $\mathcal{A}_L$ durch:
		$$
			f\colon Q \to \Sigma^\ast/_L, \quad q_v \mapsto [v]_L.
		$$
		Weil für $q_v = q_w$ impliziert, dass $v \sim_L w$ ist $f(q_v) = [v]_L = [w]_L = f(q_w)$ (d.h. $f(q_v)$ hängt nur vom Zustand $q_v$, nicht vom tatsächlichen Wort $v$ ab), damit ist $f$ wohldefiniert. Wir müssen nun noch zeigen, dass $f$ ein Isomorphismus ist:
		\begin{itemize}
			\item $f$ is bijektiv:
				\begin{itemize}
					\item $f$ ist surjektiv, denn für alle $[v]_L \in \Sigma^\ast/_L$ ist $f(q_v) = [v]_L$.
					\item $f$ is injektiv, weil $|Q| \leq |\Sigma^\ast/_L|$ (Zustandsmenge von $\mathcal{A}_L$), da $\mathcal{A}$ minimal für $L$ ist.
				\end{itemize}
			\item $f(\delta(q, a)) = \delta_L(f(q), a)$ für alle $q \in Q, a\in \Sigma$: Sei $q \in Q$ und $a \in \Sigma$. Sei $v \in \Sigma^\ast$, sodass $q = q_v$. Dann ist $\delta(q, a) = q_{va}$, denn
				$$
					\mathcal{A}: q_0 \reaches{v} q_v = q \reaches{a} \delta(q, a), 
				$$
				und $\mathcal{A}: q_0 \reaches{va} q_va$. Wegen Determinismus von $\mathcal{A}$ gilt also auch $\delta(q, a) = q_{va}$. Alles zusammen ergibt:
				$$
					f(\delta(q, a)) = f(q_{va}) = [va]_L = \delta([v]_L, a) = \delta_L(f(q_v), a).
				$$
			\item $f(q_0) = [\varepsilon]_L$ ist klar, denn $q_\varepsilon = q_0$ für jeden DFA.
			\item $f[F] = F_L$: Für alle $w \in \Sigma^\ast$ ist $[w]_L$ ein akzeptierender Zustand in $\mathcal{A}_L$ g.d.w. $w \in L$. Außerdem ist $w \in L$ g.d.w. ein $q \in F$ existiert mit $\delta^\ast(q_0, w) = q$. Mit $q = q_w$ erhalten wir dann:
				\begin{align*}
					q \in F &\text{ g.d.w. } w \in L\\
					&\text{ g.d.w. } f(q) = [w]_L \text{ mit } w \in L. \quad \text{(also } [w]_L \text{ akzeptierend in } \mathcal{A}_L \text{)}\qedhere
				\end{align*} 
		\end{itemize}
	\end{proof}
\end{theorem}
Was bedeutet dieser Satz? Zunächst einmal haben wir als Ergebnis, dass zu jeder regulären Sprache ein (bis auf Isomorphie) eindeutiger minimaler DFA existiert. Außerdem gibt es eine 1:1-Korrespondenz der Zustände im minimalen DFA zu den Myhill-Nerode-Äquivalenzklassen der zueghörigen Sprache. Das sollte ohne jeden Zweifel da tollste Ergebnis sein, das man bisher im Studium gelernt hat $\heartsuit$.
\begin{remark*}
	Für NFAs gilt dieses Ergebnis nicht. Beispielsweise sind die drei NFAs in Abbildung~\ref{fig:3nfas} alle äquivalent (Sprache $\llbracket a^+ \rrbracket$) und minimal (bzgl. Anzahl der Zustände), aber nicht isomorph zueinander.
\end{remark*}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.3\textwidth}
		\centering
		\input{figs/3nfas_a}
	\end{subfigure}
	\begin{subfigure}[b]{.3\textwidth}
		\centering
		\input{figs/3nfas_b}
	\end{subfigure}
	\begin{subfigure}[b]{.3\textwidth}
		\centering
		\input{figs/3nfas_c}
	\end{subfigure}
	\caption{Drei äquivalente NFAs, die minimal aber nicht isomorph zueinander sind.}
	\label{fig:3nfas}
\end{figure}
Im Folgenden schauen wir uns an, wie wir zu einem gegebenen DFA einen äquivalenten minimalen DFA berechnet. Dazu seien alle von nun an betrachteten DFAs reduziert auf ihre erreichbaren Zustände, d.h. alle Zustände sollen erreichbar sein.\par
Wir haben bereits gesehen, dass Myhill-Nerode-Äquivalenz und Zustände in DFAs miteinander korrespondieren. 
\begin{definition}
	Sei $\sim$ eine Äquivalenzrelation über $A$. $\sim^\prime$ ist eine \textit{Verfeinerung} von $\sim$, wenn $\sim^\prime$ eine Äquivalenzrelation über $A$ ist und jede Äquivalenzklasse von $\sim$ eine Vereinigung von Äquivalenzklassen von $\sim^\prime$ ist.
\end{definition}
\begin{remark*}
	Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein DFA. Sei
	$$
		\Sigma^\ast_q \coloneqq \{w \in \Sigma^\ast \mid q_w = q\}.
	$$
	Dann ist $\Sigma^\ast_Q \coloneqq \{\Sigma^\ast_q : q \in Q\}$ eine Partition von $\Sigma^\ast$ und die dadurch induzierte Ä\-qui\-va\-lenz\-re\-la\-tion 
	$$
		\sim_{\Sigma^\ast_Q} \,= \{(u, v) \in \Sigma^\ast \times \Sigma^\ast \mid u, v \in \Sigma^\ast_q \text{ für ein } q \in Q\}
	$$
	ist eine Verfeinerung von $\sim_{L(\mathcal{A})}$.
\end{remark*}
\begin{definition}
	Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein DFA. Zwei Zustände $p$ und $q$ sind \textit{äquivalent} (geschrieben $p \sim_\mathcal{A} q$), wenn für alle $w \in \Sigma^\ast$ gilt
	$$
		\delta^\ast(p, w) \in F \quad\text{ g.d.w. }\quad \delta^\ast(q, w) \in F.
	$$
\end{definition}
\begin{lemma}\label{lem:state_equivalence}
	$\sim_{\Sigma^\ast_Q}$ und $\sim_\mathcal{A}$ sind rechtsseitige Kongruenzrelationen bzgl. Konkatenation bzw $\delta(\cdot, a)$ für alle $a \in \Sigma$.
	\begin{proof}
		Das ist eine schöne Übung.
	\end{proof}
\end{lemma}
\begin{example}\label{exp:state_equivalence}
	Wir betrachtenn den Automaten $\mathcal{A}$ in Abbildung~\ref{fig:state_equivalence_o}. Es gibt 6 Zustände in $\mathcal{A}$, aber nur 3 $\sim_\mathcal{A}$-Äquivalenzklassen: $\{q_0, q_1, q_5\}, \{q_2, q_3\}, \{q_4\}$.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/state_equivalence_o}
		\caption{DFA $\mathcal{A}$}
		\label{fig:state_equivalence_o}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/state_equivalence_q}
		\caption{Quotientautomat $\mathcal{A}/_{\sim_\mathcal{A}}$}
		\label{fig:state_equivalence_q}
	\end{subfigure}
	\caption{Ein DFA $\mathcal{A}$ mit 6 Zuständen, aber nur 3 $\sim_\mathcal{A}$-Äquivalenzklassen und der zugehörige Quotientautomat $\mathcal{A}/_{\sim_\mathcal{A}}$.}
	\label{fig:state_equivalence}
\end{figure}
Wir wissen bereits, dass Wörter, die im selben Zustand landen äquivalent sind (Lemma~\ref{lem:dfa2mnf}). Zusätzlich wissen wir nun, dass die $\sim_{\Sigma^\ast_Q}$-Äquivalenz, wie oben definiert, eine Verfeinerung der Myhill-Nerode-Äquivalenz $\sim_L \coloneqq \sim_{L(\mathcal{A})}$ ist und dass $\sim_L$ einen minimalen DFA induziert (Satz~\ref{thm:mndfamin}), der auch noch bis auf Umbenennung der Zustände eindeutig ist (Satz~\ref{thm:mindfaunique}). Wir müssen also nun nur noch einen Algorithmus angeben, der die Verfeinerung von $\sim_\mathcal{A}$ auflöst und uns wieder die Myhill-Nerode-Äquivalenz $\sim_L$ liefert.
\begin{lemma}\label{lem:mn_se}
	Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein DFA und $L = L(\mathcal{A})$. Für alle $w \in \Sigma^\ast$ sei $q_w \in Q$ der eindeutige Zustand mit $\mathcal{A}: q_0 \reaches{w} q_w$. Dann gilt für alle $u, v \in \Sigma^\ast$:
	$$
		q_u \sim_\mathcal{A} q_v \quad\text{ g.d.w. }\quad u \sim_L v.
	$$
	\begin{proof}
		Seien $u, v \in \Sigma^\ast$.
		\begin{align*}
			q_u \sim_\mathcal{A} q_v &\quad\text{ g.d.w. }\quad \text{f.a. } w \in \Sigma^\ast \text{ gilt } \delta^\ast(q_u, w) \in F \iff \delta^\ast(q_v, w) \in F\\
			&\quad\text{ g.d.w. }\quad \text{f.a. } w \in \Sigma^\ast \text{ gilt } \delta^\ast(q_0, uw) \in F \iff \delta^\ast(q_0, vw) \in F\\
			&\quad\text{ g.d.w. }\quad \text{f.a. } w \in \Sigma^\ast \text{ gilt } uw \in L \iff vw \in L\\
			&\quad\text{ g.d.w. }\quad u \sim_L v.\qedhere
		\end{align*}
	\end{proof}
\end{lemma}
\begin{theorem}\label{thm:se2mindfa}
	Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein DFA, $\sim \coloneqq \sim_\mathcal{A}$ seine zugehörige Zu\-stands\-ä\-qui\-va\-lenz\-re\-la\-tion, und $L = L(\mathcal{A})$. Sei
	$$
		\mathcal{A}/_\sim \coloneqq (Q/_\sim, \Sigma, \delta_\sim, [q_0]_\sim, F_\sim)
	$$
	mit 
	\begin{itemize}
		\item $\delta_\sim([q]_\sim, a) = [\delta(q, a)]_\sim$,
		\item $F_\sim = \{[q]_\sim \in Q/_\sim \mid [q]_\sim \cap F \neq \emptyset\}$.
	\end{itemize}
	Dann ist $\mathcal{A}/_\sim$ ein DFA und es gilt: $\mathcal{A}/_\sim \cong \mathcal{A}_L$.
	\begin{proof}
		$\mathcal{A}/_\sim$ ist ein DFA, denn $Q/_\sim$ und $\Sigma$ sind endlich und nicht leer. $\delta_\sim$ ist wohldefiniert, denn für alle $p, q \in Q$ mit $p \sim q$ ist nach Lemma~\ref{lem:state_equivalence} $[\delta(p, a)]_\sim = [\delta(q, a)]_\sim$. Außerdem ist $[q_0]_\sim \in Q/_\sim$ und $F_\sim \subseteq Q/_\sim$.
		Nun geben wir einen Isomorphismus $f\colon \mathcal{A}/_\sim \cong \mathcal{A}_L$ an, wobei $\mathcal{A}_L = (\Sigma^\ast/_L, \Sigma, \delta_L, [\varepsilon]_L, F_L)$ wieder der Myhill-Nerode-DFA (Lemma~\ref{lem:mnf2reg}) ist. Dazu sei wieder $q_v$ der eindeutige Zustand mit $\delta(q_0, v) = q_v$ für alle $v \in \Sigma^\ast$. Wir setzen dann $f([q_v]_\sim) = [v]_L$. Nach Lemma~\ref{lem:mn_se} ist $f$ wohldefiniert und bijektiv. Es bleibt zu zeigen, dass $f$ wirklich ein isomorphismus ist:
		\begin{itemize}
			\item $f(\delta_\sim([q]_\sim, a)) = \delta_L(f([q]_\sim), a)$ für alle $q \in Q, a\in \Sigma$: Sei $q \in Q, a \in \Sigma$. Sei $v \in \Sigma^\ast$ mit $q = q_v$ (existiert, weil $\mathcal{A}$ reduziert auf die erreichbaren Zustände ist). Dann ist $\delta_\sim([q]_\sim, a) = [q_{va}]_\sim$, weil $\delta(q_v, a) =q_{va}$. Es folgt:
				$$
					f(\delta_\sim([q]_\sim, a)) = f([q_{va}]_\sim) = [va]_L = \delta_L([v]_L, a) = \delta_L(f([q_v]_\sim), a).
				$$
			\item $f([q_0]_\sim) = f([q_\varepsilon]_L) = [\varepsilon]_L$.
			\item $f[F_\sim] = F_L$: Für $p, q \in Q$ impliziert $p \sim q$, dass $\delta^\ast(p, \varepsilon) \in F$ g.d.w. $\delta^\ast(q, \varepsilon) \in F$. Das ist genau dann der Fall, wenn $p \in F$ g.d.w. $q \in F$ gilt. Also gilt für alle $q \in Q$, dass $q \in F$ g.d.w. $[q]_\sim \in F_\sim$. Sei nun $w \in \Sigma^\ast$ beliebig. Dann gilt
				$$
					w \in L \quad\text{ g.d.w. }\quad q_w \in F \quad\text{ g.d.w. }\quad [q_w]_\sim \in F_\sim.
				$$
				Aus der Definition von $F_L$ ergibt sich außerdem $w \in L$ g.d.w. $[w]_L \in F_L$. Also insgesamt für $q = q_w$:
				\[
					[q]_\sim \in F_\sim \quad\text{ g.d.w. }\quad w \in L \quad\text{ g.d.w. }\quad f([q]_\sim) = [w]_L \in F_L.\qedhere
				\]
		\end{itemize}
	\end{proof}
\end{theorem}
Den entstehenden Automaten $\mathcal{A}/_\sim$ wie im Satz nennen wir auch den \textit{Quotientautomaten} von $\mathcal{A}$ unter $\sim$. 
\begin{remark*}
	Das Konzept des Quotientautomaten existiert auch für NFAs und funktioniert auch für beliebige Äquivalenzrelationen. Beachte aber, dass im Allgemeinen dann nicht gilt, dass der entstehende Automat ein DFA ist (selbst wenn der ursprüngliche Automat deterministisch war). Außerdem ist die erkannte Sprache im Allemeinen nicht die selbe, sondern eine Obermenge.
\end{remark*}
In Abbildung~\ref{fig:state_equivalence_q} ist der Quotientautomat zum Automaten aus Beispiel~\ref{exp:state_equivalence}.
\begin{corollary}
	Der Quotientautomat $\mathcal{A}/_\sim = \mathcal{A}/_{\sim_\mathcal{A}}$ aus Satz~\ref{thm:se2mindfa} ist minimal.
\end{corollary}
Ähnlich wie Korollar~\ref{cor:mn_allfound} uns ein Kriterium zum Suchen für Myhill-Nerode-Ä\-qui\-va\-lenz\-klas\-sen (siehe Algorithmus~\ref{alg:mnc}) liefert, hilft uns folgende Beobachtung beim Finden der $\sim_\mathcal{A}$-Klassen.
\begin{remark*}
	Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ ein DFA und seien $p, q \in Q$.
	\begin{enumerate}[label=\arabic*)]
		\item Wenn $p \in F$ und $q \notin F$, dann $p \not\sim_\mathcal{A} q$.
		\item Wenn ein $a \in \Sigma$ existiert mit $\delta(p, a) \not\sim_\mathcal{A} \delta(q, a)$, dann $p \not\sim_\mathcal{A}$.
	\end{enumerate}
\end{remark*}
Wir haben nun alles nötige gesammelt für einen DFA-Minimierungsalgorithmus, den wir in Algorithmus~\ref{alg:dfaminimization} aufschreiben.
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwComment{Comment}{\texttt{// }}{}
	\underline{DFAMinimization}{($\mathcal{A}$)}\\
	\Output{minimaler DFA $\mathcal{A}/_{\sim_\mathcal{A}}$}	
	Berechne Menge der erreichbaren Zustände (z.B. mit DFS) und reduziere $\mathcal{A}$.\\
	Setze $P_1 = F, P_2 = Q \setminus F$ und $\mathcal{P} = \{P_1, P_2\}$.\\
	\While{ex. $P_i \in \mathcal{P}$, $p, q \in P_i$, $P_j \in \mathcal{P}$, $a \in \Sigma$ mit $\delta(p, a) \in P_j$ und $\delta(q, a) \notin P_j$}{
		$P_{i_1} = \{r \in P_i \mid \delta(r, a) \in P_j\}$,\\
		$P_{i_2} = \{r \in P_i \mid \delta(r, a) \notin P_j\}$,\\
		$\mathcal{P} = (\mathcal{P} \setminus \{P_i\}) \cup \{P_{i_1}, P_{i_2}\}$.
	}
	\Comment{$\mathcal{P}$ enthält nun die $\sim_\mathcal{A}$-Äquivalenzklassen (Partition von $Q$)}
	Konstruiere $\mathcal{A}/_{\sim_\mathcal{A}}$ gemäß Satz~\ref{thm:se2mindfa}.
	\caption{DFA-Minimierungsalgorithmus (\textit{Blockverfeinerung})}
	\label{alg:dfaminimization}
\end{algorithm}
\begin{example}
	Betrachte erneut den DFA $\mathcal{A}$ aus Abbildung~\ref{fig:state_equivalence_o}. Wir beschreiben die Ausführung des Algorithmus auf $\mathcal{A}$:
	Zunächst sind alle Zustände erreichbar. Es muss also keine Reduktion auf erreichbare Zustände vorgenomnen werden. Wir initialisieren $\mathcal{P}$ indem wir $Q$ auf akzeptierende und nicht-akzeptierende Zustände aufteilen $P_1 = \{q_2, q_3\}$ und $P_2 = \{q_0, q_1, q_4, q_5\}$. Die Elemente von $\mathcal{P}$ bezeichnen wir als \textit{Blöcke} von $Q$. Wir sehen bei der Iteration über bestehende Blöcke und Alphabetsymbole, dass wir den Block $P_2$ aufteilen können mit Hilfe des Symbols $b$ und des Blocks $P_1$, denn $\delta(q_0, b) = q_3 \in P_1$, aber $\delta(q_4, b) = q_5 \notin P_1$. Dann kann $P_2$ aus $\mathcal{P}$ gelöscht und dafür die Blöcke $P_3 = \{q_0, q_1, q_5\}$ und $P_4 = \{q_4\}$ hinzugefügt werden. In einer weiteren Iteration über Blöcke und Alphabetsymbole finden wir keine weiteren Verfeinerungen. Damit ist $\sim_\mathcal{A}$ gefunden: Die Äquivalenzklassen sind die verbleibenden Mengen in $\mathcal{P}$. Wir können nun den Quotientenautomaten bauen und erhalten den DFA in Abbildung~\ref{fig:state_equivalence_q}. In Abbildung~\ref{fig:blockrefinement} befindet sich noch eine Veranschaulichung des Algorithmus als Baum. 
\end{example}
\begin{figure}
	\centering
	\input{figs/blockrefinement}
	\caption{Blockverfeinerungsalgorithmus als Baum visualiert. Die Äquivalenzklassen von $\sim_\mathcal{A}$ befinden sich an den Blättern des Baumes.}
	\label{fig:blockrefinement}
\end{figure}
Wir zeigen nun noch, dass Algorithmus~\ref{alg:dfaminimization} auch wirklich das richtige macht und betrachten noch kurz die Laufzeit des Algorithmus.
\begin{theorem}
	Algorithmus~\ref{alg:dfaminimization} liefert zu jedem DFA den äquivalenten, minimalen DFA.
	\begin{proof}
		Das reduzieren auf erreichbare Zustände verändert die erkannte Sprache des Automaten nicht. Sei $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ reduzierter DFA und $n \coloneqq |\mathcal{A}|$.
		Wir teilen den Beweis auf 3 Behauptungen auf:
		\begin{enumerate}
			\item Der Algorithmus führt die Zeilen 4 bis 7 (Verfeinerung) maximal $n-2$ mal auf. 
				\begin{subproof}
					Wir starten die Verfeinerung mit zwei Blöcken. In jedem Durchlauf der Verfeinerung erhöht sich die Anzahl der Blöcke um 1. Da die Blöcke eine Partition von $Q$ bilden können wir höchstens $n$ Blöcke enthalten (dann, wenn $\mathcal{A}$ bereits minimal war). Insgesamt können wir also höchstens $n-2$ mal verfeinern.
				\end{subproof}
			\item Falls $p \in P_i$ und $q \in P_j$ mit $i \neq j$ (am Ende des Algorithmus), so ist $p \not\sim_\mathcal{A} q$.
				\begin{subproof}
					Wir machen eine Induktion über die Anzahl der Verfeinerungen $s$.\\
					Induktionsverankerung: $s = 0$. Zustände $p \in F$ und $q \in Q \setminus F$ sind nicht äquivalent.\checkmark\\
					Induktionsschritt: Wir betrachten den $s+1$. Verfeinerungsschritt von $P_i$ zu $P_{i_1}$ und $P_{i_2}$ vermöge Block $R$ und $a \in \Sigma$. Sei $p \in P_{i_1}$ und $q \in P_{i_2}$. Dann gilt $\delta(p, a) \in R$ und $\delta(q, a) \notin R$. Nach Induktionshypothese ($R$ entstand ja in einer früheren Verfeinerung oder ist initialer Block) ist damit $\delta(p, a) \not\sim_\mathcal{A} \delta(q, a)$ und dann folgt unmittelbar vermöge der Definition von $\sim_\mathcal{A}$, dass $p \not\sim_\mathcal{A} q$.
				\end{subproof}
			\item Falls $p, q \in P_i$ (am Ende des Algoritmus), so ist $p \sim_\mathcal{A} q$.
				\begin{subproof}
					Wir zeigen per Induktion über Wortlänge $m$, dass wenn ein $w \in \Sigma^\ast$ existiert, sodass $\delta^\ast(p, w) \in F$ und $\delta^\ast(q, w) \notin F$ (oder umgekehrt) mit $|w| = m$, so gilt $p \in P_i$ und $q \in P_j$ für $i \neq j$. Mittels Kontraposition erhalten wir dann die Aussage.\\
					Induktionsverankerung: $m = 0$. Betrachte $w = \varepsilon$. Dann ist $p \in F$ und $q \notin F$ (oder umgekehrt). Also sind $p$ und $q$ schon bei der Initialisierung in verschiedenen Blöcken.\checkmark\\
					Induktionsschritt: Sei $w \in \Sigma^{m+1}$, sodass $\delta^\ast(p, w) \in F$ und $\delta^\ast(q, w) \notin F$ (oder umgekehrt). Dann ist $w = av$ für ein $a \in \Sigma$ und $v \in \Sigma^m$. Dann ist aber $\delta^\ast(\delta(p, a), v) \in F$ und $\delta^\ast(\delta(q, a), v) \notin F$ (oder umgekehrt). Also sind $\delta(p, a)$ und $\delta(q, a)$ in verschiedenen Blöcken. Dann müssen auch $p$ und $q$ in verschiedenen Blöcken liegen, denn sonst ließe sich eine weitere Verfeinerung finden vermöge der Blöcke von $\delta(p, a)$ und $\delta(q, a)$ und $a$.
				\end{subproof}
		\end{enumerate}
		Also berechnet die Blockverfeinerung $\sim_\mathcal{A}$. Mit Satz~\ref{thm:se2mindfa} folgern wir, dass der entstandene DFA minimal ist.
	\end{proof}
\end{theorem}
\begin{theorem}
	Algorithmus~\ref{alg:dfaminimization} hat bei Eingabe $\mathcal{A} = (Q, \Sigma, \delta, q_0, F)$ mit $n \coloneqq |Q|$ und $m \coloneqq |\Sigma|$ eine Laufzeit von $\mathcal{O}(mn \log n)$.
	\begin{proof}
		Einer der Großmeister der Automatentheorie, John E. Hopcroft, sagt, es geht.
	\end{proof}
\end{theorem}


\subsection{Weitere Algorithmische Probleme für reguläre Sprachen}\label{sec:regular_algorithms}
Wir betrachten ein paar zusätzliche algorithmische Fragen: Wird ein Wort von einem bestimmten Automaten akzeptiert? Akzeptieren zwei Automaten die selbe Sprache? Ist die erkannte Sprache von einem Automaten unendlich? Die erste Frage scheint recht einfach zu sein, die anderen beiden hingegen wirken nicht mehr ganz so trivial. In der theoretischen Informatik treten diverse Probleme auf, die womöglich gar nicht algorithmisch lösbar sind. Das heißt wir müssen womöglich auf solche destruktive Ergebnisse vorbereitet sein (Spoiler: In diesem Kapitel noch nicht). Wir starten mit einer Übersicht über die Probleme, die wir in diesem Abschnitt betrachten. Danach beweisen wir einige Sätze zu diesen Problemen.
\begin{description}
	\item[Wortproblem (Matching)] Gegeben NFA $\mathcal{A}$, Wort $w$. Ist $w \in L(\mathcal{A})$?
	\item[Leerheitsproblem (Emptiness)] Gegeben NFA $\mathcal{A}$. Ist $L(\mathcal{A}) = \emptyset$?
	\item[Universalitätsproblem (Universality)] Gegeben NFA $\mathcal{A}$. Ist $L(\mathcal{A}) = \Sigma^\ast$?
	\item[Unendlichkeitsproblem (Infinity)] Gegeben NFA $\mathcal{A}$. Ist $|L(\mathcal{A})| = \infty$?
	\item[Inklusionsproblem (Inclusion)] Gegeben NFAs $\mathcal{A}, \mathcal{B}$. Ist $L(\mathcal{A}) \subseteq L(\mathcal{B})$?
	\item[Äquivalenzprolem (Equivalence)] Gegeben NFAs $\mathcal{A}, \mathcal{B}$. Ist $L(\mathcal{A}) = L(\mathcal{B})$?
\end{description}
\begin{remark*}
	Wir sagen ein Problem (formal ist ein Problem selbst eine Sprache) ist \textit{entscheidbar}, wenn es einen Algorithmus gibt, der zu jeder Eingabe die richtige Antwort ausgibt. Eine präzisere Definition gibt es in der Vorlesung Berechenbarkeit und Komplexitätstheorie.
\end{remark*}
\begin{theorem}\label{thm:nfa_matching}
	Das Wortproblem für NFAs ist entscheidbar.
	\begin{proof}
		Für DFAs ist dies trivial: Wir müssen lediglich den Automaten über das Wort laufen lassen und antworten gemäß Zustand nach vollständigem Lesen des Wortes. Für NFAs haben wir mit Algorithmus~\ref{alg:nfaacc} ein Verfahren über die sukzessive Berechnung der Erreichbarkeitsmenge, was einer impliziten Potenzmengenkonstruktion (siehe Algorithmus~\ref{alg:nfa2dfa}) entspricht.
	\end{proof}
\end{theorem}
\begin{theorem}\label{thm:nfa_emptiness}
	Das Leerheitsproblem für NFAs ist entscheidbar.
	\begin{proof}
		Wir müssen nur überprüfen, ob es einen Zustand $q \in F$ gibt, der von $q_0$ aus erreichbar ist, zum Beispiel über Wort $w$. Dann gilt $\delta^\ast(q_0, w) = q \in F$ und somit $w \in L(\mathcal{A}) \neq \emptyset$. Existiert kein solcher Zustand, so ist $L(\mathcal{A})$ leer. Es genügt also eine einfache Tiefensuche im Transitionsgraphen von $\mathcal{A}$ nach akzeptierenden Zuständen.
	\end{proof}
\end{theorem}
\begin{theorem}\label{thm:nfa_universality}
	Das Universalitätsproblem für NFAs ist entscheidbar.
	\begin{proof}
		Bemerke zunächst, dass für DFAs gilt, dass $L(\mathcal{A}) = \Sigma^ast$ g.d.w. jeder (erreichbare) Zustand akzeptierend ist (für reduzierte Automaten gilt also $Q = F$). Für NFAs ist das nicht der Fall, da es beispielsweise Wörter $w \in \Sigma^\ast$ geben kann, sodass kein $q \in Q$ existiert mit $\mathcal{A}: q_0 \reaches{w} q$. Dann ist $w \notin L(\mathcal{A}) \neq \Sigma^\ast$. Auch möglich wärenWörter $w \in \Sigma^\ast$ und Zustände $q \in Q \setminus F$ mit $\mathcal{A}: q_0 \reaches{w} q$, aber für jedes dieser $w$ auch Zustände $q^\prime \in F$ mit $\mathcal{A}: q_0 \reaches{w} q^\prime$. Dann ist $L(\mathcal{A})$ doch $\Sigma^\ast$. Um das Universalitätsproblem für NFAs zu entscheiden wandeln wir den NFA mit Algorithmus~\ref{alg:nfa2dfa} zu einem DFA um und reduzieren diesen auf seine erreichbaren Zustände. Sind dann die verbleibenden Zustände alle akzeptierend, so war der ursprügliche NFA auch universal.
	\end{proof}
\end{theorem}
\begin{remark*}
	Beachte, dass $L = \Sigma^\ast$ g.d.w. $\comp{L} = \emptyset$. Wir können also auch das Leerheitsproblem auf dem NFA lösen und die Ausgabe des Verfahrens aus Satz~\ref{thm:nfa_emptiness} umkehren um das Universalitätsproblem zu lösen.
\end{remark*}
\begin{theorem}\label{thm:nfa_infinity}
	Das Unendlichkeitsproblem für NFAs ist entscheidbar.
	\begin{proof}
		$L(\mathcal{A})$ ist unendlich g.d.w. ein $q \in Q$ existiert mit 
		$$
			\mathcal{A}: q_0 \reaches{\ast} q \reaches{+} q \reaches{\ast} q^\prime \in F.
		$$
		``wenn, dann``: Sei $L(\mathcal{A})$ unendlich. Dann gibt es ein Wort $w = a_0 \ldots a_{n-1} \in L(\mathcal{A})$ mit $|w| = n \geq |Q|$. Wir betrachten nun ein akzeptierenden Lauf auf $\mathcal{A}$:
		$$
			\varrho = (q_0, a_0, q_1, \ldots, a_{n-1}, q_n)
		$$
		Wegen $n \geq |Q|$ gibt es $0 \leq i < j \leq n$, sodass $q_i = q_j \eqqcolon q$ (vgl. Pumping-Lemma~\ref{lem:pumping-lemma}, pigeonhole principle), sodass
		$$
			\mathcal{A}: q_0 \reaches{a_0 \ldots a_{i-1}} q \reaches{a_i \ldots a_{j-1}} q \reaches{a_j \ldots a_{n-1}} q_n \in F
		$$
		und damit $\mathcal{A}: q_0 \reaches{\ast} q \reaches{+} q \reaches{\ast} q^\prime \in F$.\\
		``nur wenn``: Es gelte $\mathcal{A}: q_0 \reaches{\ast} q \reaches{+} q \reaches{\ast} q^\prime \in F$. Seien $x, y, z \in \Sigma^\ast$ mit $y \neq \varepsilon$, sodass $\mathcal{A}: q_0 \reaches{x} q \reaches{y} q \reaches{z} q^\prime \in F$. Dann gilt $xy^iz \in L(\mathcal{A})$ für alle $i \in \mathbb{N}$ (vgl. wieder mit Pumping-Lemma). Also ist $|L(\mathcal{A})| = \infty$.\\
		Beachte, dass für jeden Zustand $q \in Q$ die Probleme $\mathcal{A}: q_0 \reaches{\ast} q$, $\mathcal{A}: q \reaches{+} q$, $\mathcal{A}: q \reaches{\ast} q^\prime$ für ein $q^\prime \in F$ entscheidbar sind mit einer Tiefensuche im Transitionsgraphen von $\mathcal{A}$. Also ist auch das Unendlichkeitsproblem für NFAs entscheidbar.
	\end{proof}
\end{theorem}
\begin{remark*}
	Das Unendlichkeitsproblem für NFAs ist sogar in Polynomialzeit entscheidbar. Bei den anderen Problemen ist implizit immer eine Potenzmengenkonstruktion notwendig.
\end{remark*}
\begin{theorem}\label{thm:nfa_inclusion}
	Das Inklusionsproblem für NFAs ist entscheidbar.
	\begin{proof}
		Wir benutzen folgende Hilfsaussage:
		\begin{itemize}
			\item[] Seien $L_1, L_2 \subseteq \Sigma^\ast$. Dann gilt $L_1 \subseteq L_2$ g.d.w. $L_1 \cap \comp{L_2} = \emptyset$.
				\begin{subproof}
					Dieses ganze Fach würde lächerlich aussehen, wenn wir hier jedes kleinste Detail beweisen würden. Aber eine ganz nette Übung.
				\end{subproof}
		\end{itemize}
		Wir können also einfach bei Eingabe $\mathcal{A}, \mathcal{B}$ den NFA $\mathcal{B}$ erst in einen DFA umwandeln (Algorithmus~\ref{alg:nfa2dfa}) und dann die akzeptierenden Zustände nicht-akzeptierend machen und umgekehrt um das Komplement zu erkennen (Satz~\ref{thm:regular_complement}). Dann können wir mit der Produktkonstruktion (Satz~\ref{thm:regular_intersection}) den Schnitt von $L(\mathcal{A})$ und $\comp{L(\mathcal{B})}$ erkennen. Zuletzt wenden wir den Leerheitstest (Satz~\ref{thm:nfa_emptiness}) an und erhalten ein Verfahren für das Inklusionsproblem.
	\end{proof}
\end{theorem}
\begin{theorem}\label{thm:nfa_equivalence}
	Das Äquivalenzproblem für NFAs ist entscheidbar.
	\begin{proof}
		$L(\mathcal{A}) = L(\mathcal{B})$ g.d.w. $L(\mathcal{A}) \subseteq L(\mathcal{B})$ und $L(\mathcal{B}) \subseteq L(\mathcal{A})$. Also zwei Mal Satz~\ref{thm:nfa_inclusion} anwenden.
	\end{proof}
\end{theorem}


\subsection{*Anwendung: Model-Checking}\label{sec:regular_model-checking}
Ein wichtigese, sehr grundlegendes, algorithmisches Problem der Informatik ist das sogenannte \textit{Model-checking-Problem}. Es tritt überall auf, wo automatische Verfikation von Hard- oder Software benötigt wird. Wir betrachten ein System (oder eine Modellierung eines Systems, zum Beispiel durch einen endlichen Automaten). $\mathcal{S}$ und eine Spezifikation einer Systemeigenschaft $\varphi$. In der Praxis kann $\varphi$ eine Formel in Modallogik oder Monadic Second Order Logic sein, manchmal reicht auch schon ein regulärer Ausdruck. Das Problem ist nu zu entscheiden, ob $\mathcal{S}$ die Eigenschaft $\varphi$ besitzt.\par
Wir nehmen an, das $\mathcal{S}$ ein NFA ist und $\varphi$ ein regulärer Ausdruck. Wir sagen das System $\mathcal{S}$ erfüllt die Spezifikation $\varphi$, wenn jeder akzeptierende Lauf von $\mathcal{S}$ die Eigenschaft, die durch $\varphi$ spezifiziert wird, besitzt. D.h. wir wollen prüfen ob $L(\mathcal{A}) \subseteq \llbracket \varphi \rrbracket$. In Algorithmus~\ref{alg:model-checking} beschreiben wir das Vorgehen um das Model-Checking-Problem, wie oben beschrieben, zu lösen.
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwComment{Comment}{\texttt{// }}{}
	\underline{Model-Checking}{($\mathcal{S}, \varphi$)}\\
	\Output{Ist $L(\mathcal{S}) \subseteq \llbracket \varphi \rrbracket$?}
	Wandle alle $\varphi$ mit Thompson-Konstruktion in $\varepsilon$-NFA.\\
	Eliminiere $\varepsilon$-Transitionen.\\
	Determinisiere mit Potenzmengenkonstruktion und erhalte $\mathcal{A}_\varphi$.\\
	Vertausche akzeptierende und nicht-akzeptierende Zustände in $\mathcal{A}_\varphi$ (erhalte $\comp{\mathcal{A}_\varphi}$).\\
	Wende Produktkonstruktion auf $\mathcal{S}$ und $\comp{\mathcal{A}_\varphi}$ an.\\
	Löse das Leerheitsproblem auf dem Produktautomaten, übernehme die Ausgabe.
	\caption{Model-Checking-Algorithmus für System $\mathcal{S}$ (als NFA modelliert) und Spezifikation $\varphi$ (regulärer Ausdruck)}
	\label{alg:model-checking}
\end{algorithm}
\begin{remark*}
	Die Zeilen 2-4 in Algorithmus~\ref{alg:model-checking} beschreiben nur die Umwandlung von einem regulären Ausdruck $\varphi$ in einen DFA. Der Algorithmus funktioniert natürlich genauso, wenn man für $\varphi$ eine andere Formalisierung (z.B. in Logiken) wählt, die sich in Automaten umwandeln lässt. Dann ersetzt man einfach diese Zeilen 2-4 durch eine geeignete Transformation.
\end{remark*}
\begin{example}\label{exp:mc}
	Wir betrachten den NFA $\mathcal{S}$ in Abbildung~\ref{fig:mc_system}, der einen System modelliert mit Aktionen $\{a, b, c\}$. Ein Wort entspricht also einer Aktionsfolge. Uns interssiert nun ob dieses System folgende \textit{Request-Response-Bedingung} $\varphi$ efüllt: 
	\begin{center}
		``Wenn die Aktion $c$ ausgeführt wird, wird später auch $a$ ausgeführt.``
	\end{center}
	Wir können $\varphi$ als regulären Ausdruck hinschreiben: $\varphi = ((a+b)^\ast c (b+c)^\ast a (a+b)^\ast)^\ast$. Ein einfacher DFA, der $\llbracket \varphi \rrbracket$ erkennt ist in Abbildung~\ref{fig:mc_spec}. In Abbildung~\ref{fig:mc_prod} ist der Produktautomat, der $L(\mathcal{S}) \cap \comp{\llbracket \varphi \rrbracket}$ erkennt. Die Zustände $(p_1, q_0), (p_1, q_1), (p_1, q_3)$ sind unerreichbar von $(p_0, q_0)$ aus, also ist kein akzeptierender Zustand erreichbar. Somit erfüllt $\mathcal{S}$ die Request-Response-Bedingung $\varphi$.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/mc_system}
		\caption{System $\mathcal{S}$ als NFA.}
		\label{fig:mc_system}
	\end{subfigure}\\
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/mc_spec}
		\caption{DFA $\mathcal{A}_\varphi$ zu Spezifikation $\varphi$.}
		\label{fig:mc_spec}
	\end{subfigure}\\
	\begin{subfigure}[b]{.9\textwidth}
		\centering
		\input{figs/mc_prod}
		\caption{Produkt-Automat aus $\mathcal{S}$ und $\comp{\mathcal{A}_\varphi}$.}
		\label{fig:mc_prod}
	\end{subfigure}
	\caption{Die drei Automaten aus Beispiel~\ref{exp:mc}.}
\end{figure}
\begin{example}
	Weitere geläufige Eigenschaften sind zum Beispiel: \textit{Liveness} ``Eine ge\-wünsch\-te Aktion wird irgendwann eintreten`` oder \textit{Safety} ``Ein bestimmte Aktionsfolge (ein Infix) kann nicht auftreten``. Als Übung zeige man, dass $\mathcal{S}$ aus Abbildung~\ref{fig:mc_system} die Liveness-Eigenschaft bzgl. $a$ hat, aber die Safety-Eigenschaft bzgl. $cc$ verletzt.
\end{example}


\subsection{*Anwendung: First-Longest-Match-Analyse}\label{sec:regular_first-longest-match}
Wir schauen uns nun (endlich) eine praktische Anwendung von regulären Ausdrücken und endlichen Automaten an. Bisher haben wir stets das \textit{einfache Matching-Problem} für reguläre Ausdrücke betrachtet. Dabei ging es darum zu prüfen, ob ein gegebenes Wort $w \in \Sigma^\ast$ zur Sprache eines regulären Ausdrucks $r \in \mathsf{RE}_\Sigma$ gehört. Dies ließ sich mit Hilfe der Thompson-Konstruktion (Lemma~\ref{lem:regex2nfa}) einfach als das Wortproblem eines $\varepsilon$-NFA betrachten. In der Praxis ist das einfach Matching-Problem aber zu primitiv um echte Probleme lösen zu können. Vorallem im Compilerbau benötigen wir etwas mehr, wenn wir Programmcode in seine Bestandteile zerlegen wollen um später Syntax-Checks darauf durchführen zu können und schließlich eine Semantik für das Programm festzulegen. In der Fachsprache nennt man diesen Teil eines Compilers auch \textit{Scanner} oder \textit{Lexer} (siehe auch die Programme \texttt{lex}, \texttt{flex}).\par
Das \textit{erweiterte (extended) Matching-Problem} kommt direkt aus der Anwendung des Compilerbaus. Gegeben ist ein Wort $w \in \Sigma^\ast$ und eine endliche Menge von regulären Ausdrücken $\{r_1, \ldots, r_n\} \subseteq \mathsf{RE}_\Sigma$. (In der Praxis ist die Annahme $\varepsilon \notin \llbracket r_i \rrbracket \neq \emptyset$ für alle $i$ sinnvoll). Gesucht ist nun eine Zerlegung des Wortes $w = u_1 \ldots u_k$, wobei für jedes $u_j$ ein $r_{i_j}$ existieren muss, sodass $u_j \in \llbracket r_{i_j} \rrbracket$. Man nennt die Zerlegung in $(u_1, \ldots, u_k)$ auch \textit{Dekomposition} und die zugehörigen Indizes der regulären Ausdrücke $(i_1, \ldots, i_k)$ \textit{Analyse} von $w$ bezüglich $r_1, \ldots, r_n$.\par
Wie folgendes Beispiel zeigt, müssen weder Dekomposition noch Analyse eindeutig sein (insbesondere dann, wenn wir $\varepsilon \in \llbracket r_i \rrbracket$ zulassen).
\begin{example}
	Wir betrachten Wörter und reguläre Ausdrücke über dem Alphabet $\Sigma = \{a, b, c\}$.
	\begin{enumerate}[label=\arabic*)]
		\item $r_1 = a^+, w = aa$. Ergibt Dekompositionen $(aa)$ und $(a, a)$ mit zugehöriger (eindeutiger) Analyse $(1)$ bzw. $(1, 1)$.
		\item $r_1 = a+b, r_2 = a+c, w = a$. Ergibt eindeutige Dekomposition $(a)$ mit zwei verschiedenen Analysen $(1)$ und $(2)$.	
	\end{enumerate}
	Im zweiten Fall stellen wir uns vor, dass $r_1$ mögliche Schlüsselwörter einer Programmiersprache (\texttt{while}, \texttt{true}, \texttt{if}) beschreibt und $r_2$ mögliche Variablennanem (Identifier). In diesem Anwendungsfall ist es natürlich zu sagen, dass der Ausdruck $r_1$ \textit{wichtiger} ist als $r_2$ (deshalb verbieten die mesiten Programmiersprachen auch Schlüsselwörter als Identifier). Wir müssen also Analysen mit $1$ höher gewichten. Ähnliche Beispiele lassen sich auch für die Dekomposition finden.
\end{example}
Das Ziel ist es nun also, Dekomposition und zugehörige Analyse eindeutig zu machen, um Konflikte wie oben zu vermeiden. Ein erster Ansatz wäre zunächst sicherzustellen, dass $\llbracket r_i \rrbracket \cap \llbracket r_j \rrbracket = \emptyset$ für alle $i \neq j$, indem wir den gemeinsamen Schnitt von zwei Ausdrücken aus dem weniger gewichteten Ausdruck entfernen. Dadurch würde zumindest die Analyse eindeutig werden. Diese Idee ist jedoch nicht sinnvoll, u.a. deshalb, da das Entfernen des Schnittes -- also das Erkennen von $\llbracket r_j^\prime \rrbracket = \llbracket r_j \rrbracket \setminus \llbracket r_i \rrbracket$ -- eine Produktkonstruktion erfordert und somit tendenziell teuer ist. Stattdessen werden wir folgende zwei Prinzipien implementieren:
\begin{description}
	\item[Longest Match] Mache jedes $u_i$ der Zerlegung so lang wie möglich.
	\item[First Match] Wähle aus den matchenden regulären Ausdrücken denjenigen mit dem kleinesten Index.
\end{description}
\begin{definition}
	Eine Dekomposition $(u_1, \ldots, u_k)$ von $w \in \Sigma^\ast$ bezüglich der regulären Ausdrücke $r_1, \ldots, r_n \in \mathsf{RE}_\Sigma$ heißt \textit{Longest-Match-Dekomposition} (LMD), wenn für jedes $i \in \{1, \ldots, k\}$, $x \in \Sigma^+$, $y \in \Sigma^\ast$ mit $w = u_1 \ldots u_i x y$ gilt, dass kein $j \in \{1, \ldots, n\}$ existiert, sodass $u_i x \in \llbracket r_j \rrbracket$. 
\end{definition}
Umgangssprachlich bedeutet das: Wenn wir einen Teil der Komposition $u_i$ noch erweitern würden mit folgenden Symbolen aus $w$, so würde keiner der regulären Ausdrücke mehr matchen. $u_i$ ist also das länstmögliche Wort in Schritt $i$ mit dem sich das erweiterte Matching-Problem noch lösen lässt.
\begin{theorem}
	Sei $\{r_1, \ldots, r_n\} \subseteq \mathsf{RE}_\Sigma$ und $w \in \Sigma^\ast$. Falls eine LMD von $w$ bzgl. $r_1, \ldots, r_n$ existiert, so ist sie eindeutig.
	\begin{proof}
		Denk nach!
	\end{proof}
\end{theorem}
Eine LMD muss nicht existieren wie folgendes Beispiel zeigt.
\begin{example}
	$r_1 = a^+, r_2 = ab, w = aab$ hat Dekomposition $(a, ab)$ aber keine LMD.
\end{example}
\begin{definition}
	Sei $(u_1, \ldots, u_k)$ eine LMD von $w \in \Sigma^\ast$ bezüglich der regulären Ausdrücke $r_1, \ldots, r_n \in \mathsf{RE}_\Sigma$. Die zugehörige \textit{First-Longest-Match-Analyse} (FLM-Analyse) $(i_1, \ldots, i_k)$ ist gegeben durch
	$$
		i_j \coloneqq \min \{ \ell \mid u_j \in L(r_\ell) \}
	$$
	für jedes $j \in \{1, \ldots, k\}$.
\end{definition}
\begin{theorem}
	Eine FLM-Analyse ist eindeutig, sofern sie existiert. Sie existiert genau dann, wenn die zugehörige LMD existiert.
	\begin{proof}
		Der Beweis ist ziemlich offensichtlich, es sei denn, der Leser schläft.
	\end{proof}
\end{theorem}
Es gibt viele Möglichkeiten eine FLM-Analyse durchzuführen. Die hier vorgestellte Art in Algorithmus~\ref{alg:flmlmd} ist nicht unbedingt die populärste (man würde sie vermutlich nicht so implementieren), aber sie ist insofern intuitiv, als dass sie nur Konzepte aus dieser Vorlesung verwendet.
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwComment{Comment}{\texttt{// }}{}
	\underline{FLM+LMD}{($w, (r_i)_{i=1}^n$)}\\
	\Output{FLM-Analyse $(i_j)_{j=1}^k$ und LMD $(u_j)_{j=1}^k$}
	\Comment{Vorbereitung}
	Wandle alle $r_i$ mit Thompson-Konstruktion in $\varepsilon$-NFAs $\mathcal{A}_i = (Q_i, \Sigma, \delta_i, q_0^i, F_i)$ um.\\
	Baue neuen $\varepsilon$-NFA $\mathcal{A} = (\biguplus_i Q_i \uplus \{q_0, q_f\}, \Sigma \uplus \{1, \ldots, n\}, \delta, q_0, \{q_f\})$ mit $\delta(p, a) = \delta_i(p, a)$ falls $p \in Q_i$ und $\delta(q_0, \varepsilon) = \{q_0^i \mid i \in \{1, \ldots, n\}\}$ und $\delta(q, i) = \{q_f\}$, falls $q \in F_i$.\\
	Eliminiere $\varepsilon$-Transitionen.\\
	Determinisiere mit Potenzmengenkonstruktion.\\
	Minimiere mit Markierungsalgorithmus.\\
	\tcc{Wir haben jetzt einen DFA mit folgender Eigenschaft: Vom Zustand $\delta^\ast(q_0, u)$ ist genau dann eine Transition mit $i \in \{1, \ldots, n\}$ in einen Endzustand möglich, wenn $u \in \llbracket r_i \rrbracket$.}
	\Comment{Löse nun das erweiterte Matching-Problem}	
	Setze $\ell = 1, w_\ell = w$.\\
	\While{$w_\ell \neq \varepsilon$}{
		\texttt{A} = leeres Array der Länge $|w_\ell|$.\\
		Simuliere $\mathcal{A}$ auf $w_\ell$, prüfe in jedem Schritt $j$, ob eine $i$-Transition in einen akzeptierenden Zustand möglich ist.
		Falls ja, setze \texttt{A[$j$]} auf das kleinste mögliche $i$.\\
		Nach der Simulation laufe \texttt{A} von hinten nach vorne durch bis wir den ersten nicht-leeren Eintrag an Stelle $h$ finden. Sollte es keinen geben gebe einen Error aus, ansonsten ist $u_\ell = w_{\ell_1} \ldots w_{\ell_h}$ und $i_\ell = \texttt{A[}h\texttt{]}$.\\
		Setze $w_{\ell+1} = w_{\ell_h} \ldots w_{\ell_{|w_\ell|}}, \ell = \ell+1$.
	}
	Gebe $(u_1, \ldots, u_{\ell-1})$ und $(i_1, \ldots, i_{\ell-1})$ aus.
	\caption{First-Longest-Match-Analyse und Longest-Match-Dekomposition}
	\label{alg:flmlmd}
\end{algorithm}
In Abbildung~\ref{fig:flmlmd} sehen wir wie der $\varepsilon$-NFA der nach Zeile 3 entsteht aussieht.
\begin{figure}
	\centering
	\input{figs/flmlmd}
	\caption{Skizze des NFAs aus Algorithmus~\ref{alg:flmlmd}.}
	\label{fig:flmlmd}
\end{figure}



\newpage
\section{Kellerautomaten und Kontextfreie Sprachen}\label{sec:contextfree}
Wir haben gesehen, dass reguläre Sprachen eine wichtige Rolle sowohl in der theoretischen als auch in der angewandten Infromatik spielen. Allerdings haben wir auch festgestellt, dass es Sprachen gibt, die nicht regulär sind. Es ist auch leider nicht so, dass man die Existenz nicht-regulärer Sprachen einfach ignorieren könnte, weil sie nicht wichtig wären oder nur eine mathematisch-informatische Spielerei sind. Wir haben beispielsweise gesehen, dass die Menge der unär codierten Primzahlen nicht regulär ist. Das Finden von immer neuen größeren Primzahlen ist bekanntlich ein viel betrachtetes Problem und -- mit Blick auf kryptographische Algorithmen wie \texttt{RSA} -- auch ein sehr wichtiges. Aber auch ein für die Informatik sehr grundlegendes Anwendungsgebiet, der Definition von Programmiersprachen, tauchen bereits nicht-reguläre Sprachen auf. Wir haben zum Beispiel die Sprache $L \coloneqq \{a^n b^n : n \in \mathbb{N}\}$ gesehen und festgestellt, dass sie nicht regulär ist. Identifiziert man nun das Symbol $a$ beispielsweise mit einer öffnenden Klammer '$($' und $b$ mit einer schließenden Klammer '$)$' (quasi durch den Homomorphismus $h\colon \{a, b\} \to \{(, )\}, a \mapsto (, b \mapsto )$, so erhalten wir die Sprache der Klammerausdrücke mit gleich vielen lffnenden wie schließenden Klammern $\{(^n)^n : n \in \mathbb{N}\}$. Eine Anwendung in der Informatik ist bei Ausdrücken wie \texttt{foo(bar(), (a+((b+c)*d)))}, die von einem Compiler korrekt aufgeschlüsselt werden müssen offensichtlich. In diesem Kapitel betrachten wir die Sprachen, die sich induktiv, d.h. mit einfachen Basisfällen und Rekursionsregeln, definieren lassen. Zum Beispiel lässt sich die eingangs besprochende nicht-reguläre Sprache $L$ induktiv definieren durch:
\begin{description}
	\item[Basisfall] $\varepsilon \in L$,
	\item[rekursiver Fall] Falls $u \in L$, so ist auch $aub \in L$.
\end{description}
Solche induktiven Definitionen von Sprachen geben wir durch \textit{Grammatiken} an. In diesem Kapitel betrachten wir die sogenannten \textit{kontextfreien Grammatiken}, die zum Beispiel zur Beschreibung der Progarmmiersprache C ausreichen.

\subsection{Kontextfreie Grammatiken}\label{sec:contextfree_grammars}
%TODO Einleitender Absatz
\begin{definition}
	Eine \textit{kontextfreie Grammatik (CFG)} (von engl.: context-free grammar) ist ein Quadrupel
	$$
		(N, \Sigma, P, S)
	$$
	mit
	\begin{itemize}
		\item $N$ einer nicht-leeren, endlichen Menge von \textit{Nichtterminalsymbolen},
		\item $\Sigma$ einer nicht-leeren, endlichen Menge von \textit{Terminalsymbolen} (mit $N \cap \Sigma = \emptyset$),
		\item $P$ einer eindlichen Menge von \textit{Produktionsregeln} der Form $A \to \alpha$ mit $A \in N, \alpha \in (N \cup \Sigma)^\ast$,
		\item $S \in N$ einem \textit{Startsymbol}.
	\end{itemize}
\end{definition}
Wir bezeichnen Grammatiken mit $\mathcal{G}, \mathcal{G}^\prime, \ldots$. Lateinische Großbuchstaben $A, B, \ldots$ stehen für Nichtterminale, lateinische Kleinbuchstaben $a, b, \ldots$ für Terminale bzw. $u, v, w, \ldots$ für \textit{Terminalwörter} (kurz: \textit{Wörter}). Griechische Kleinbuchstaben $\alpha, \beta, \ldots$ hingegen stehen für \textit{Satzformen}, das sind Wörter über $(N \cup \Sigma)^\ast$. Wenn nicht weiter benötigt, geben wir zu einer Grammatik $(N, \Sigma, P, S)$ nur die Produktionsregeln $P$ explizit an mit folgender Konvention:
\begin{itemize}
	\item Die zuerst gelistete Regel hat das Startysmbol auf der linken Regelseite,
	\item Regeln mit gleicher linker Seite werden zusammengefasst, indem rechte Seiten durch $|$ getrennt werden,
	\item gesunder Menschenverstand bei der Unterscheidung zwischen Terminal- und Nichtterminalsymbolen.
\end{itemize}
\begin{example}\label{exp:cfg_art}
	Wir geben eine Grammatik an, die die Sprache der syntaktisch korrekt formulierten arithmetischen Terme mit $+, -, \ast$ über Binärzahlen $\mathcal{G}_\text{art} = (N, \Sigma, P, S)$ mit $N = \{S, B, B^\prime, O\}, \Sigma = \{0, 1, +, - ,\ast, (, )\}$ und
	\begin{align*}
		P = \{ & S \to B \mid -S \mid (SOS)\\
		& B \to 0 \mid 1B^\prime\\
		& B^\prime \to \varepsilon \mid 0B^\prime \mid 1B^\prime\\
		& O \to + \mid - \mid \ast \}
	\end{align*}
	Was genau passiert hier? Mit $B$ und $B^\prime$ lassen sich Binärzahlen ohne führende $0$en erzeugen. $O$ lässt sich durch einen beliebigen Operator als Terminalsymbol ersetzen. Mit $S$ lässt sich eine Binärzahl (über $B, B^\prime$) erzeugen, oder induktiv ein $-$ vor einen Term setzen, oder zwei Terme mit einem Operator (über $O$) zu verbinden. Betrachten wir nur die Regeln mit $B$ und $B^\prime$ als linker Regelseite so bekommen wir eine Grammatik (mit Startsymbol $B$) für die Sprache aller Binärzahlen ohne führenden $0$en.
\end{example}
Nun haben wir bereits eine kontextfreie Grammatik einmal angegeben und sie mit einer Sprache assoziiert. Allerdings haben wir bisher noch gar nicht erklärt, wie genau eine Grammatik eine Sprache definiert -- wir müssen noch einen Formalismus finden, der zu einer kontextfreien Grammatik eine Semantik festlegt, ähnlich wie in Abschnitt~\ref{sec:regular_regexp} für die regulären Ausdrücke.
\begin{definition}
	Sei $\mathcal{G} = (N, \Sigma, P, S)$ eine kontextfreie Grammatik und seien $\alpha, \beta \in (N \cup \Sigma)^\ast$ Satzformen.
	\begin{itemize}
		\item $\beta$ ist \textit{direkt ableitbar} aus $\alpha$ in $\mathcal{G}$ ($\alpha \to_\mathcal{G} \beta$), wenn eine Regel $A \to \delta \in P$ gibt und Satzformen $\gamma_1, \gamma_2 \in (N \cup \Sigma)^\ast$ sodass
			$$
				\alpha = \gamma_1 A \gamma_2 \quad\text{ und }\quad \beta = \gamma_1 \delta \gamma_2. 
			$$
		\item Eine \textit{Ableitung von $\beta$ aus $\alpha$} ist eine Folge $(\alpha_0, \ldots, \alpha_n)$ von Satzformen mit $\alpha_0 = \alpha, \alpha_n = \beta$, wobei für alle $i \in [n]$ gilt, dass $\alpha_i \to_\mathcal{G} \alpha_{i+1}$.
		\item Eine \textit{Ableitung von $\beta$} ist eine Ableitung von $\beta$ aus $S$.
		\item $\beta$ ist \textit{ableitbar aus $\alpha$ in $\mathcal{G}$} ($\alpha \to_\mathcal{G}^\ast \beta$), wenn eine Ableitung von $\beta$ aus $\alpha$ existiert.
		\item $\beta$ ist \textit{ableitbar}, wenn eine Ableitung von $\beta$ existiert.
	\end{itemize}
\end{definition}
\begin{definition}
	Sei $\mathcal{G} = (N, \Sigma, P, S)$ eine kontextfreie Grammatik. Die von $\mathcal{G}$ \textit{erzeugte Sprache} ist
	$$
		L(\mathcal{G}) \coloneqq \{w \in \Sigma^\ast \mid S \to_\mathcal{G}^\ast w\},
	$$
	also die Menge ableitbarer Terminalwörter in $\mathcal{G}$.
\end{definition}
Falls wir auf die Länge von Ableitungen eingehen mösten, schreiben wir $\alpha \to_\mathcal{G}^n \beta$ bzw. $\alpha \to_\mathcal{G}^{[n+1]} \beta$ für ableitbar in $n$ bzw. höchstens $n$ Schritten. Wir lassen den Index $\mathcal{G}$ weg, wenn die Grammatik aus dem Kontext klar ist.
\begin{definition}
	Eine Sprache heißt \textit{kontextfrei}, wenn es eine kontextfreie Grammatik git, die sie erzeugt.
\end{definition}
\begin{example}
	Wir betrachten die Sprache $L = \{a^n b^n : n \in \mathbb{N}\}$ (vgl. Beispiel~\ref{exp:pumping1}). Folgende kontextfreie Grammatik erzeugt $L$:
	\begin{align*}
		S \to aSb \mid \varepsilon.
	\end{align*}
	Wir leiten einige Wörter aus $L$ ab:
	\begin{itemize}
		\item $S \to \varepsilon$,
		\item $S \to aSb \to a \varepsilon b = ab$,
		\item $S \to aSb \to aaSbb \to^{n-2} a^nSb^n \to a^n b^n$.
	\end{itemize}
\end{example}


\subsection{Normalformen für kontextfreie Grammatiken}\label{sec:contextfree_normalforms}


\subsection{Ableitungsbäume}\label{sec:contextfree_derivationtrees}


\subsection{Abschlusseigenschaften kontextfreie Sprachen}\label{sec:contextfree_closure}


\subsection{Nicht-kontextfreie Sprachen}\label{sec:contextfree_noncontextfree}


\subsection{Kellerautomaten}\label{sec:contextfree_pda}


\subsection{Deterministische Kellerautomaten}\label{sec:contextfree_dpda}


\subsection{Algorithmische Probleme für kontextfreie Sprachen}\label{sec:contextfree_algorithms}


\subsection{*Anwendung: Parsing}\label{sec:contextfree_parsing}



\newpage
\section{Allgemeine Grammatiken}\label{sec:grammars}
\subsection{Kontextsensitive Grammatiken}\label{sec:grammars_csg}


\subsection{Normalformen für kontextsensitive Grammatiken}\label{sec:grammars_normalforms}


\subsection{Linear-beschränkte Automaten}\label{sec:grammars_lba} % or rational graphs?


\subsection{Algorithmische Probleme für kontextsensitive Sprachen}\label{sec:grammars_algorithms}


\subsection{Phrasenstrukturgrammatiken}\label{sec:grammars_unrestricted}


\subsection{Turingmaschinen}\label{sec:grammars_tm}


\subsection{Die Chomsky-Hierarchie}\label{sec:grammars_chomsky}



\newpage
\section{Prozesskalküle und Petri-Netze}\label{sec:process}
\subsection{Nebenläufige Systeme}\label{sec:process_concurring}


\subsection{Synchronisierte und unsynchronsierte Produkte}\label{sec:process_products}


\subsection{Algebraische Beschreibung von Transitionssystemen}\label{sec:process_algebraic}


\subsection{Petri-Netze}\label{sec:process_petrinets}


\subsection{*Anwendung: Mutual Exclusion}\label{sec:process_peterson}



\newpage
\bibliography{sources}\addcontentsline{toc}{section}{Literatur}
\nocite{*}
\bibliographystyle{alpha}



\newpage
\begin{appendix}
\section{Englische Fachbegriffe}


\newpage
\section{Exkurs: Unentscheidbarkeit}
\end{appendix}

% eof
\end{document}
