\documentclass[11pt, a4paper]{article}
\usepackage[left=3cm, right=3cm]{geometry}

% packages
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{stmaryrd}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{mathrsfs}
\usepackage{mathdots}
\usepackage{mathtools}
\usepackage{listings}
\usepackage[linesnumbered, ruled, vlined, ngerman]{algorithm2e}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{array}
\usepackage[justification=centering]{caption}
\usepackage{subcaption}
\usetikzlibrary{arrows, automata, graphs, shapes, petri, decorations.pathmorphing}

% meta
\clubpenalty = 10000
\widowpenalty = 10000
\displaywidowpenalty = 10000
\parindent = 0pt

% define environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}[definition]{Beispiel}
\newtheorem*{example*}{Beispiel}
\newtheorem*{remark*}{Bemerkung}

\theoremstyle{plain}
\newtheorem{theorem}[definition]{Satz}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Korollar}

\numberwithin{equation}{section}

\renewcommand{\labelenumi}{(\roman{enumi})}
\makeatletter
\newcommand*{\shifttext}[2]{
	\settowidth{\@tempdima}{#2}
	\makebox[\@tempdima]{\hspace*{#1}#2}
}
\makeatother
\def\Rho{\mathrm{P}}

\newenvironment{problem}[1]{\begin{tabular}{|p{.96\textwidth}|} \hline \textsc{#1}\\}{\\ \hline \end{tabular}}
\newcommand{\comp}[1]{\overline{#1}}
\newcommand{\qedw}{\hfill\(\square\)}
\newcommand{\qedb}{\hfill\(\blacksquare\)}
\newcommand{\shuffle}{\mathrel{\shifttext{5pt}{$\equiv$}\shifttext{-5pt}{$=$}}}
\newcommand{\reaches}[1]{\overset{#1}{\rightarrow}}
\newcommand{\reachess}[2]{\overset{#1}{\underset{#2}{\rightarrow}}}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\bin}{bin}
\DeclareMathOperator{\xor}{xor}
\let\emptyset\varnothing

\lstset{numbers=left, xleftmargin=.2\textwidth, xrightmargin=.2\textwidth, basicstyle=\ttfamily\bfseries}

% Here we go...
\begin{document}

% title page
\pagestyle{empty}
\begin{center}
    Rheinisch-Westf\"alische Technische Hochschule Aachen\\[10em]

    \begin{LARGE}
    		Skript zur Vorlesung\\[1.5em]
		\textbf{Formale Systeme, Automaten, Prozesse}
    \end{LARGE}
	\vfill
    \begin{Large}
    		Letzte Änderung:\\
    		\today\\[2em]
		Autor:\\
		Niklas Rieken\\
	\end{Large}
	\vfill
	\includegraphics[scale=.4]{icons/cc.png}
	\includegraphics[scale=.4]{icons/by.png}
	\includegraphics[scale=.4]{icons/sa.png}\\
	Dieses Werk ist lizenziert unter einer Creative Commons Namensnennung -- Weitergabe unter gleichen Bedingungen 4.0 International Lizenz.
\end{center}


\newpage
% acknowledgements
\vspace*{\fill}
\section*{Hinweise}
Dieses Skript entstand aus der Vorlesung Formale Systeme, Automaten, Prozesse an der RWTH Aachen von Prof. Dr. Wolfgang Thomas und Prof. Dr. Martin Grohe vom Lehrstuhl Informatik~7 in den Sommersemestern 2015 und 2016. Ein paar Notationen und Definitionen sind außerdem adaptiert aus dem Skript zu den Diskreten Strukturen und Lineare Algebra I für Informatiker von Dr. Timo Hanke und Prof. Dr. Gerhard Hiß vom Lehrstuhl~D für Mathematik.
\vspace*{\fill}


\newpage
% table of contents
\tableofcontents


\newpage
\pagestyle{headings}
\section{Mathematisches Vorwissen}\label{sec:pre}
In diesem ersten Kapitel fixieren wir einige mathematischen Notationen und geben elementare Sätze aus der diskreten Mathematik, die im weiteren verlauf der Vorlesung benötigt werden. In der Regel sollten sämtliche Begriffe und Notationen aus dem ersten Semester bereits bekannt sein. Deshalb ist dieses Kapitel eher nur für Sommersemesteranfänger bestimmt.


\subsection{Mengen}\label{sec:pre_sets}
Der Begriff Menge geht auf Georg Cantor aus dem 19. Jahrhundert zurück und wurde (verglichen mit späteren Definitionen in diesem Skript) informell beschrieben.
\begin{quote}
	Unter einer ``Menge`` verstehen wir jede Zusammenfassung \( M \) von bestimmten wohlunterschiedenen Objekten \( m \) unserer Anschauung oder unseres Denkens (welche die ``Elemente`` von \( M \) genannt werden) zu einem Ganzen.
\end{quote}

Wir definieren eine Menge wie folgt
\begin{definition}
	Eine \textit{Menge} \( M \) ist etwas, zu dem jedes beliebige Objekt \( x \) entweder \textit{Element} der Menge ist (\( x \in M \)), oder nicht (\( x \notin M \)).
\end{definition}
Mengen selbst können auch wieder als Objekte aufgefasst werden, also Elemente anderer Mengen sein. Wir vermeiden jedoch Aussagen über ``Mengen, die sich selbst enthalten``, da so schnell Widersprüche entstehen können (vgl. Russel'sche Antinomie). Wir schließen uns der weit verbreiteten \textit{Zermelo-Fraenkel-Mengenlehre} an, dazu geben wir jedoch keine Details (diese findet man zum Beispiel in der Logik 2-Vorlesung im Wahlpflichtbereich). Wir schauen uns nur an, wie wir Mengen im allgemeinen betrachten können. Folgende Definition sind dabei elementar.
\begin{definition}\label{def:subsets}
	Seien \( M, N \) zwei Mengen. \( N \) ist eine \textit{Teilmenge} von \( M \) (\( N \subseteq M \)) bzw. \( M \) eine \textit{Obermenge} von \( N \) (\( M \supseteq N \)), wenn für alle \( x \in N \) gilt, dass auch \( x \in M \).\\
	Wir sagen \( N \) ist eine \textit{echte Teilmenge} von \( M \) (\( N \subset M \)) bzw. \( M \) eine \textit{echte Obermenge} von \( N \) (\( M \supset N \)), wenn es zusätzlich ein \( y \in M \) gibt mit \( y \notin N \).\\
	\( M \) und \( N \) sind \textit{gleich} (\( M = N \)), wenn sowohl \( M \subseteq N \) als auch \( N \subseteq M \) gilt.
\end{definition}
Wir kommen nun zum Mächtigkeitsbegriff der Mengenlehre, der für die Anzahl der Elemente einer Menge beschreibt.
\begin{definition}
	Sei \( M \) eine Menge. \( M \) heißt \textit{endlich}, wenn \( M \) nur endlich viele Elemente besitzt, dann beschreibt \( \left| M \right| \) die Anzahl der Elemente von \( M \). Andernfalls heißt \( M \) \textit{unendlich} und wir schreiben \( \left| M \right| = \infty \). Man nennt \( \left| M \right| \) die \textit{Mächtigkeit} von \( M \).
\end{definition}
Um eine konkrete Menge zu zu benennen gibt es im Wesentlichen vier verschiedene Möglichkeiten:
\begin{enumerate}
	\item \textit{Aufzählen.} Die Elemente der Menge werden aufgelistet und in Mengenklammern (\( \{, \} \)) eingeschlossen. Reihenfolge und Wiederholungen spielen keine Rolle.
		\[
			\{ 3, 4.5, \pi, \diamondsuit \} = \{ \pi, 4.5, \diamondsuit, \diamondsuit, 3 \} \subseteq \{ \diamondsuit, \pi, 4.5, 3, \clubsuit \}. 
		\]
	\item \textit{Beschreiben.} Mengen können durch Worte beschrieben werden.
		\[
			\text{Menge der natürlichen Zahlen} = \{ 0, 1, 2, 3, \ldots \} \eqqcolon \mathbb{N}.
		\]
		Aber Achtung: Natürliche Sprache neigt zu Uneindeutigkeit!
	\item \textit{Aussondern.} Sei \( M \) eine Menge, dann ist
		\[
			\{ x \in M \mid \varphi(x) \}
		\]
		die Menge aller Elemente aus \( M \), die die Eigenschaft \( \varphi \) erfüllen. Zum Beispiel:
		\[
			\mathbb{P} \coloneqq \{ n \in \mathbb{N} \mid n \text{ hat genau zwei Teiler} \}
		\]
		als Menge aller Primzahlen.
	\item \textit{Abbilden.} Sei \( M \) eine Menge und \( f \) ein Ausdruck, der für jedes \( x \in M \) definiert ist. Dann ist
		\[
			\{ f(x) : x \in M \}
		\]
		die Menge aller Ausdrücke \( f(x) \), wobei jedes \( x \in M \) in \( f \) eingesetzt wird. Zum Beispiel:
		\[
			\{ n^2 : n \in \mathbb{N} \}
		\]
		als Menge aller Quadtratzahlen.
\end{enumerate}
Wir können Abbilden und Aussondern auch kombinieren, zum Beispiel mit:
\[
	\{ n^2 : n \in \mathbb{N} \mid n \text{ ungerade} \}
\]
als Menge aller Quadratzahlen von ungeraden natürlichen Zahlen. Man würde hier jedoch abkürzend schreiben:
\[
	\{ n^2 : n \in \mathbb{N} \text{ ungerade} \}
\]
oder auch
\[
	\{ n^2 : n \in 2\mathbb{N}+1 \}.
\]

Eine wichtige Menge haben wir bisher außen vor gelassen: die \textit{leere Menge}. Wir schreiben \( \emptyset \coloneqq \{ \} \). Gelegentlich verwenden wir außerdem folgende Notation, wenn wir nur eine endliche geordnete Menge benötigen: \( [\ell] \coloneqq \{ 0, 1, \ldots, \ell-1 \} \). Ein-elementige Mengen (z.B. \( [1] = \{ 0 \} \)) nennt man auch \textit{Singleton}.


\subsection{Operationen auf Mengen}\label{sec:pre_setops}
Im folgendem betrachten wir Mengen immer als Teilmenge eines \textit{Universums} (oder auch \textit{Grundemenge}) \( \mathcal{U} \). In der Analysis ist das typischerweise die Menge der reellen Zahlen \( \mathbb{R} \) (oder die Menge der komplexen Zahlen \( \mathbb{C} \)), die betrachteten Teilmengen sind oftmals Intervalle in denen zum Beispiel Funktionen auf Stetigkeit hin untersucht werden. Vorweg: Wir betrachten später im Allgemeinen das Universum \( \Sigma^\ast \) und Sprachen als Teilmenge von eben diesem. Genaueres folgt im nächsten Kapitel.\\
Um die Operatoren auf den Mengen zu veranschaulichen gibt es die sogenannten \textit{Venn-Diagramme}, bei denen Kreise oder Ellipsen die Mengen visualisieren. In Abbildung~\ref{fig:venn_subset} finden wir zum Beispiel für die Inklusion (\( \subseteq \)) ein entsprechendes Diagramm.
\begin{figure}
	\centering
	\input{figs/venn_subset}
	\caption{Venn-Diagramm für \( A \subseteq B \).}
	\label{fig:venn_subset}
\end{figure}
Wir definieren nun einige Operationen auf Mengen ähnlich wie Addition und Multiplikation usw. auf Zahlen. Zusätzlich zur formalen Definition befindet sich in Abbildung~\ref{fig:venn} auch ein passendes Venn-Diagramm. Die jeweils eingefärbte Fläche kennzeichnet die resultierende Menge. \( \mathcal{U} \) sei ein beliebiges aber festes Universum.
\begin{definition}\label{def:setops}
	Seien \( A, B \) Mengen. 
	\begin{enumerate}[label=(\alph*)]
		\item Die \textit{Vereinigung} von \( A \) und \( B \) ist definiert als 
			\[ 
				A \cup B \coloneqq \{ a \in \mathcal{U} \mid a \in A \text{ oder } a \in B \}.
			\]
			Für endliche und unendliche Vereinigungen (z.B. gegeben durch eine Indexmenge \( I = \{ 0, 1, \ldots \} \)) schreiben wir abkürzend
			\[
				\bigcup_{i \in I} A_i = A_0 \cup A_1 \cup \ldots
			\]
		\item Der \textit{Schnitt} von \( A \) und \( B \) ist definiert als
			\[
				A \cap B \coloneqq \{ a \in A \mid a \in B \}.
			\]
			Für endlichen und unendlichen Schnitt (z.B. gegeben durch eine Indexmenge \( I = \{ 0, 1, \ldots \} \)) schreiben wir abkürzend
			\[
				\bigcap_{i \in I} A_i = A_0 \cap A_1 \cap \ldots
			\]
		\item Das \textit{Komplement} von \( A \) is definiert als
			\[
				\comp{A} \coloneqq \{ a \in \mathcal{U} \mid a \notin A \}.
			\]
		\item Die \textit{Differenz} (auch: \textit{relatives Komplement}) von \( A \) und \( B \) ist definiert als
			\[
				A \setminus B \coloneqq A \cap \comp{B}.
			\]
		\item Das \textit{kartesische Produkt} zwischen \( A \) und \( B \) ist definiert als
			\[
				A \times B \coloneqq \{(a, b) : a \in A, b \in B \}.
			\]
			Für ein endliches Produkt einer Menge \( A \) auf sich selbst schreiben wir abkürzend
			\[
				A^k \coloneqq A \times A^{k-1}, \quad\quad A^1 \coloneqq A, \quad\quad A^0 \coloneqq \{ \bullet \},
			\]
			wobei \( \bullet \) ein beliebiges Platzhaltersymbol ist, d.h. \( A^0 \) ist für jedes \( A \) ein Singleton.\\
			Die Elemente eines kartesischen Produkts \( (x_1, \ldots, x_k) \) heißen \( k\)-\textit{Tupel}. Für \( k = 2, 3, 4, \ldots \) kann man auch \textit{Paar, Tripel, Quadrupel, \ldots} sagen.
		\item Die \textit{Potenzmenge} von \( A \) ist definiert als
			\[
				2^A \coloneqq \{ M \subseteq \mathcal{U} \mid M \subseteq A \}.
			\]
	\end{enumerate}
\end{definition}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/venn_union}
		\caption{\( A \cup B \)}
		\label{fig:venn_union}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/venn_intersection}
		\caption{\( A \cap B \)}
		\label{fig:venn_intersection}
	\end{subfigure}\\
	\ \\
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/venn_complement}
		\caption{\( \comp{A} \)}
		\label{fig:venn_complement}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/venn_setminus}
		\caption{\( A \setminus B \)}
		\label{fig:venn_setminus}
	\end{subfigure}
	\caption{Venn-Diagramme für Mengen-Operationen.}
	\label{fig:venn}
\end{figure}


\subsection{Relationen}\label{sec:pre_relations}
Relationen drücken Beziehungen oder Zusammenhänge zwischen Elementen aus. Im Allgemeinen können dies Beziehungen zwischen beliebig vielen Elementen sein und wir werden verschieden stellige Relationen auch im Laufe der Vorlesung benutzen. In diesem Abschnitt legen wir aber ein besonderes Augenmerk auf 2-stellige Relationen.
\begin{definition}
	Es seien \( M_1, \ldots, M_k \) nicht-leere Mengen. Eine Teilmenge \( R \subseteq M_1 \times \ldots \times M_k \) heißt \textit{Relation} zwischen \( M_1, \ldots, M_k \) (oder auf \( M \), falls \( M = M_1 = \ldots = M_k \)).
\end{definition}
Für 2-stellige Relationen verwenden wir oft Symbole wie \( \sim, \prec \) und schreiben dann statt \( (a, b) \in\, \sim \) intuitiver \( a \sim b \).
\begin{definition}
	Sei \( \sim \,\subseteq M \times M \) eine 2-stellige Relation. \( \sim \) heißt
	\begin{itemize}
		\item \textit{reflexiv}, falls \( x \sim x \) für alle \( x \in M \),
		\item \textit{symmetrisch}, falls für alle \( x, y \in M \) mit \( x \sim y \) auch \( y \sim x \) gilt,
		\item \textit{antisymmetrisch}, falls für alle \( x, y \in M \) mit \( x \sim y \) und \( y \sim x \) gilt, dass \( x = y \),
		\item \textit{transitiv}, falls für alle \( x, y, z \in M \) mit \( x \sim y \) und \( y \sim z \) gilt, dass \( x \sim z \).
	\end{itemize}
\end{definition}
Wir klassifizieren außerdem 2-stellige Relationen, falls sie bestimmte Eigenschaften haben.
\begin{definition}\label{def:relations}
	Sei \( \sim \,\subseteq M \times M \) eine 2-stellige Relation. \( \sim \) heißt
	\begin{itemize}
		\item \textit{Äquivalenzrelation}, falls sie reflexiv, symmetrisch und transitiv ist,
		\item \textit{(partielle) Ordnung}, falls sie reflexiv, antisymmetrisch und transitiv ist,
		\item \textit{Totalordnung}, falls sie partielle Ordnung ist und für alle \( x, y \in M \) entweder \( x \sim y \) oder \( y \sim x \) gilt. 
	\end{itemize}	 
\end{definition}
\begin{example*}
	\
	\begin{enumerate}
		\item Die Relation \( \leq \) ist auf \( \mathbb{N} \) eine Totalordnung.
		\item Die Relation \( \{(a, b) \in \mathbb{R}^2 \mid a^2 = b^2 \} \) ist eine Äquivalenzrelation auf \( \mathbb{R} \).
	\end{enumerate}
\end{example*}

\begin{definition}\label{def:equivalenceclass}
	Sei \( \sim \) eine Äquivalenzrelation auf einer Menge \( M \). Für \( x \in M \) heißt
	\[
		[x] \coloneqq [x]_\sim \coloneqq \{ y \in M \mid x \sim y \}
	\]
	die \textit{Äquivalenzklasse} von \( x \). Die Menge aller Äquivalenzklassen von \( \sim \) wird notiert mit \( M /_\sim \coloneqq \{ [x]_\sim : x \in M \} \).
\end{definition}



\subsection{Gesetze für Mengen}\label{sec:pre_setlaws}
In diesem Abschnitt sammeln wir ein paar Gesetzmäßigkeiten, die für Mengen gelten. Manche davon sind offensichtlich, wir werden aber auch zu ein paar Aussagen die Beweise geben, manche bleiben als Übung.
\begin{remark*}
	Für die Inklusion gilt offensichtlich für jede Menge \( M \)
	\[
		\emptyset \subseteq M \subseteq M \subseteq \mathcal{U}.
	\]
	Insbesondere ist die Relation \( \subseteq \) reflexiv. Sie ist außerdem transitiv und per Definition der Gleichheit von Mengen antisymmetrisch, also eine partielle Ordnung.\par
	Schnitt und Vereinigung sind per Definition offenbar \textit{assoziativ} (d.h. \( A \cup (B \cup C) = (A \cup B) \cup C \) und \( A \cap (B \cap C) = (A \cap B) \cap C \)) und \textit{kommutativ} (d.h. \( A \cup B = B \cup A \) und \( A \cap B = B \cap A \)). Außerdem sind diese beiden Operationen zueinander \textit{distributiv}, was wir im folgenden einmal zeigen wollen. 
\end{remark*}
Das Beweisschema für solche Aufgaben ist stets das selbe und sollte deshalb auch ruhig übernommen werden für Übungsaufgaben. Tricks sind selten notwendig, es ist meist
\begin{center}
	\textit{Definition anwenden -- triviale Umformung -- Definition anwenden -- Profit}.
\end{center}
\begin{remark*}
	Für Mengen \( A, B, C \) gilt:
	\begin{enumerate}
		\item \( A \cup (B \cap C) = (A \cup B) \cap (A \cup C) \),
		\item \( A \cap (B \cup C) = (A \cap B) \cup (A \cap C) \).
	\end{enumerate}
	\begin{proof}
		Wir zeigen nur Aussage (i), die zweite Hälfte geht analog. Wir müssen zwei Richtungen beweisen.\\
		``\( \subseteq \)``: Sei \( a \in A \cup (B \cap C) \). D.h. \( a \in A \) oder \( a \in B \cap C \).
		\begin{itemize}
			\item \( a \in A \). Dann ist \( a \) auch in \( A \cup B \) und \( A \cup C \) (da \( \cup \) die Mengen nicht verkleinert). Also ist \( a \) auch im Schnitt dieser beiden Mengen.
			\item \( a \in B \cap C \). Dann ist \( a \in B \) und \( a \in C \). Also (da wie oben \( \cup \) die Menge nicht verkleinert) ist \( a \in A \cup B \) und \( a \in A \cup C \). Somit ist \( a \) auch wieder im Schnitt beider Mengen.
		\end{itemize}
		``\( \supseteq \)``: Sei \( a \in (A \cup B) \cap (A \cup C) \). Dann ist \( a \in A \cup B \) und \( a \in A \cup C \) (\( \ast \)). Wir unterscheiden zwei Fälle:
		\begin{itemize}
			\item \( a \in A \). Unabhängig von \( B, C \) ist dann \( a \in A \cup (B \cap C) \).
			\item \( a \notin A \). Dann muss \( a \in B \) und \( a \in C \) gelten, sonst würde (\( \ast \)) nicht gelten. Somit ist \( a \in B \cap C \) und damit auch wieder \( a \in A \cup (B \cap C) \). 
		\end{itemize}
		Wir haben also \( A \cup (B \cap C) \subseteq (A \cup B) \cap (A \cup C) \) und \( A \cup (B \cap C) \supseteq (A \cup B) \cap (A \cup C) \) gezeigt. Somit muss Gleichheit zwischen diesen beiden Mengen vorliegen.
	\end{proof}
\end{remark*}
Weiterhin nützlich sind noch folgende Bemerkungen.
\begin{remark*}[DeMorgan'sche Gesetze]
	Für Mengen \( A, B \) gilt:
	\begin{itemize}
		\item \( \comp{(A \cup B)} = \comp{A} \cap \comp{B} \),
		\item \( \comp{(A \cap B)} = \comp{A} \cup \comp{B} \).
	\end{itemize}
\end{remark*}
\begin{remark*}[Absorptionsgesetz]
	Für Mengen \( A, B \) gilt:
	\begin{itemize}
		\item \( A \cup (A \cap B) = A \),
		\item \( A \cap (A \cup B) = A \).
	\end{itemize}
\end{remark*}
Die Beweise hierfür bleiben als Übung.


\subsection{Funktionen}\label{sec:pre_mappings}
\begin{definition}
	Seien \( M, N \) Mengen. Eine \textit{Funktion} (oder \textit{Abbildung)} \( f \) von \( M \) nach \( N \) ist eine Vorschrift (z.B. eine Formel), die jedem \( x \in M \) genau ein \( f(x) \in N \) zuordnet. Wir schreiben dazu
	\[
		f\colon M \to N, \quad x \mapsto f(x).
	\]
	\( M \) heißt der \textit{Definitionsbereich} (auch Domäne) von \( f \), \( N \) heißt der \textit{Wertebereich} von \( f \). \( f(x) \) ist das \textit{Bild} von \( x \) unter \( f \) und \( x \) ist ein \textit{Urbild} von \( f(x) \) unter \( f \). Die Menge aller Funktionen von \( M \) nach \( N \) wird mit \( N^M \) bezeichnet. Falls \( M = A^n, N = A \) für ein nicht-leeres \( A \) ist, sagen wir auch, dass \( f \) eine \textit{\( n \)-stellige} Funktion über \( A \) ist. Desweitere nennen wir eine \( 0 \)-stellige Funktion auch \textit{Konstante}.
\end{definition}
\begin{definition}
	Sei \( f\colon M \to N \) eine Funktion.
	\begin{itemize}
		\item Für jede Teilmenge \( X \subseteq M \) ist \( f(X) \coloneqq \{ f(x) : x \in X \} \) das \textit{Bild von \( X \) unter \( f \)}. Falls \( X = M \) sprechen wir auch nur vom \textit{Bild} von \( f \).
		\item Für jede Teilmenge \( Y \subseteq N \) ist \( f^{-1}(Y) \coloneqq \{ x \in M \mid f(x) \in Y \} \) das \textit{Urbild von \( Y \) unter \( f \)}.
	\end{itemize}
\end{definition}
\begin{definition}
	Zu einer Menge \( M \) ist \( \id_M\colon M \to M, x \mapsto x \) die \textit{Identität}.
\end{definition}
\begin{definition}
	Eine Funktion \( f\colon M \to N \) heißt
	\begin{itemize}
		\item \textit{surjektiv}, falls für alle \( y \in N \) ein \( x \in M \) mit \( f(x) = y \) existiert,
		\item \textit{injektiv}, falls für alle \( x, x^\prime \in M \) mit \( x \neq x^\prime \) gilt, dass \( f(x) \neq f(x^\prime) \),
		\item \textit{bijektiv}, falls \( f \) surjektiv und injektiv ist.
	\end{itemize}
\end{definition}
\begin{example*}
	Die Addition zweier natürlicher Zahlen kann als Funktion aufgefasst werden:
	\[
		+\colon \mathbb{N}^2 \to \mathbb{N}, \quad (m, n) \mapsto m+n.
	\]
	\( + \) ist surjektiv (jedes \( y \in \mathbb{N} \) wird z.B. durch \( (y, 0) \in \mathbb{N}^2 \) getroffen), aber nicht injektiv (\( 1 \in \mathbb{N} \) wird sowohl von \( (1, 0) \) als auch \( (0, 1) \) getroffen).
\end{example*}
Für Funktionen \( f\colon [k] \to M \) für beliebige \( k \in \mathbb{N} \) in beliebige \( M \) können wir auch abkürzend die Tupelschreibweise \( (y_0, \ldots, y_{k-1}) \) verwenden. Dann ist \( y_i = f(i) \) für alle \( i \in [k] \).\par
Wir fügen nun noch ein paar nützliche Definitionen an um dieses Skript in sich abgeschlossen zu halten.
\begin{definition}
	Sei \( f\colon M \to N \) eine Funktion und \( A \subseteq M \). Dann ist \( f\vert_A\colon A \to N \) die \textit{Einschränkung von \( f \) auf \( A \)} mit \( f\vert_A(x) = f(x) \) für alle \( x \in A \).
\end{definition}
\begin{definition}
	Wir schreiben für zwei Funktionen \( f, g\colon M \to N \), dass sie \textit{identisch} sind (\( f \equiv g \)), wenn für alle \( x \in M \) gilt, dass \( f(x) = g(x) \).
\end{definition}
\begin{definition}
	Die \textit{Komposition} zweier Funktionen \( f\colon M \to M^\prime, g\colon M^\prime \to N \) ist bezeichnet mit \( g \circ f\colon M \to N, x \mapsto g(f(x)) \).
\end{definition}
\begin{definition}
	Seien \( f\colon M \to N, g\colon N \to M \) Funktionen. Dann heißt \( g \) eine \textit{linksseitige (rechtseitige) Umkehrfunktion} von \( f \), wenn \( g \circ f \equiv id_M \) (\( f \circ g \equiv id_N \)). Wenn \( g \) sowohl links- als auch rechtsseitige Umkehrfunktion ist sprechen wir schlicht von einer \textit{Umkehrfunktion} von \( f \) und bezeichnen diese mit \( f^{-1} \).
\end{definition}
\begin{remark*}
	Die Schreibweise \( f^{-1} \) benutzen wir sowohl für das Urbild als auch für Umkehrabbildungen, was jedoch zwei verschiedene Begriffe sind!
\end{remark*}


\subsection{Strukturen}\label{sec:pre_structures}
Wir haben Mengen, Relationen und Abbildungen jeweils eigenständig kennengelernt. Tatsächlich benutzen wir sie allerdings nur zusammen. Beispielsweise sind die natürlichen Zahlen für sich allein nicht sehr interessant, wenn man auf ihnen nicht addieren könnte. Umgekehrt sind auch Funktionsvorschriften wertlos, wenn sie nicht auf Elemente einer Menge angewendet werden kann. Wir fassen alles nun unter dem Begriff einer \textit{Struktur} zusammen.
\begin{definition}
	Sei \( \tau = \{ R_1, \ldots, R_m, f_1, \ldots, f_n \} \) eine Menge von Relationssymbolen und Funktionssymbolen, eine sogenannte \textit{Signatur}. Eine \textit{\( \tau \)-Struktur} ist ein Paar \( \mathfrak{A} = (A, \mathfrak{a}) \), wobei \( A \) eine nicht-leere Menge ist, die \textit{Universum} (oder \textit{Träger}) heißt und \( \mathfrak{a} \) ist eine Funktion, die jedem \( k \)-stelligem Relationssymbol \( R \in \tau \) eine \( k \)-stellige Relation und jedem \( k \)-stelligem Funktionssymbol \( f \in \tau \) eine \( k \)-stellige Funktion zuordnet.
\end{definition}
Statt \( \mathfrak{a}(R), \mathfrak{a}(f) \) schreibt man auch häufig \( R^\mathfrak{A}, f^\mathfrak{A} \). In der mathematischen Logik ist es wichtig eine Unterscheidung zwischen Relations- und Funktionssymbolen und ihren Interpretationen durch konkrete Relationen \( R^\mathfrak{A} \) bzw. Funktionen \( f^\mathfrak{A} \) zu machen. Deshalb sollten Schreibweisen wie \( \mathfrak{N} = (\mathbb{N}, +) \) nur als Abkürzungen verstanden werden für \( \mathfrak{N} = (\mathbb{N}, +^\mathfrak{N}) \) mit \( +^\mathfrak{N}\colon \mathbb{N}^2 \to \mathbb{N}, (m, n) \mapsto +^\mathfrak{N}(m, n) = m + n \), wobei wir mit \( m + n \) den gewohnten \( n \)-ten Nachfolger von \( m \) bezeichnen. Der Autor hofft, dass es klar ist worauf er hinaus wollte.\par
Wir können Strukturen wieder klassifizieren nach sogenannten \textit{Axiomen}, die sie erfüllen.
\begin{definition}
	Sei \( \mathfrak{A} = (A, \bullet) \) eine Struktur mit \( \bullet\colon A \times A \to A \). Wir sagen \( \mathfrak{A} \) ist ein \textit{Monoid}, wenn folgende Axiome gelten:
	\begin{enumerate}
		\item \textit{Assoziativität}, für alle \( a, b, c \in A \) gilt \( (x \bullet y) \bullet z = x \bullet (y \bullet z) \).
		\item \textit{neutrales Element}, es gibt ein \( e \in A \), sodass für alle \( a \in A \) gilt \( a \bullet e = e \bullet a = a \).
	\end{enumerate}
	Gilt zusätzlich
	\begin{enumerate}\setcounter{enumi}{3}
		\item \textit{Kommutativität}, für alle \( a, b \in A \) gilt \( a \bullet b = b \bullet a \).
	\end{enumerate}
	so heißt das Monoid \textit{abelsch}.
\end{definition}
\begin{example}
	Sei \( A \) eine beliebige nicht-leere Menge und \( \mathbb{B} = \{ 0, 1 \} \).
	\begin{enumerate}
		\item \( (\mathbb{N}, +) \) ist abelsches Monoid mit neutralem Element \( 0 \),
		\item \( (\mathbb{R}, \cdot) \) ist abelsches Monoid mit neutralem Element \( 1 \),
		\item \( (A^A, \circ) \) ist Monoid mit neutralem Element \( \id_A \),
		\item \( (\mathbb{B}, \wedge) \) ist abelsches Monoid mit neutralem Element \( 1 \).
	\end{enumerate}
\end{example}
Es ist bei Monoiden auch üblich, dass das neutrale Element mit in die Struktur geschrieben wird also, z.B. \( (\mathbb{R}, \cdot, 1) \).
\begin{definition}
	Sei \( \mathfrak{A} = (A, \bullet) \) ein Monoid. Eine Teilmenge \( E \subseteq A \) heißt \textit{Erzeugendensystem} von \( \mathfrak{A} \) falls jedes \( a \in A \) als \( a = e_0 \bullet \ldots \bullet e_{n-1} \) mit \( e_i \in E \) für alle \( i \in [n] \) dargestellt werden kann. \( E \) heißt \textit{frei}, falls diese Darstellung eindeutig ist. In diesem Fall nennen wir \( \mathfrak{A} \) auch das von \( E \) \textit{frei erzeugte Monoid} (oder unter Missbrauch der Notation auch \textit{freies Monoid}).
\end{definition}
\begin{remark*}
	Das neutrale Element wird stets durch eine \textit{leere} Anwendung dargestellt. Beispielsweise ist für \( (\mathbb{N}, +) \) die Menge \( \{1\} \) ein (freies) Erzeugendensystem, denn jede natürliche Zahl \( n \) lässt sich als Summe von \( n \) \( 1 \)en darstellen \( n = \sum_{i=1}^n 1 \), für \( n = 0 \) erhalten wir eben genau die leere Summe ohne Summanden.
\end{remark*}
\begin{definition}
	Sei \( (A, \bullet, e) \) Monoid und \( a \in A \).
	\begin{itemize}
		\item Gibt es ein \( b \in A \) mit \( a \bullet b = e \) so heißt \( a \) \textit{rechtsinvertierbar}.
		\item Gibt es ein \( b \in A \) mit \( b \bullet a = e \) so heißt \( a \) \textit{linksinvertierbar}.
		\item Ist \( a \) rechts- und linksinvertierbar, dann heißt \( a \) eine \textit{Einheit}.
		\item Gibt es ein \( b \in A \) mit \( b \bullet a = a \bullet b = e \) so heißt \( a \) \textit{invertierbar} und \( b \) \textit{invers} zu \( a \). 
	\end{itemize}
	Die \textit{Menge aller Einheiten} von \( A \) bezeichnen wir mit \( A^\times \).
\end{definition}
Wir bezeichnen die Inversen zu Einheiten \( a \) in der Regel mit \( a^{-1} \) oder \( -a \). 
\begin{definition}
	Sei \( \mathfrak{A} = (A, \bullet, e) \) ein Monoid. Falls in \( \mathfrak{A} \) zusätzlich
	\begin{enumerate}\setcounter{enumi}{2}
		\item \textit{Invertierbarkeit}, für alle \( a \in A \) existiert \( b \in A \) mit \( a \bullet b = b \bullet a = e \), 
	\end{enumerate}
	dann ist \( \mathfrak{A} \) eine \textit{Gruppe} (bzw. \textit{abelsche Gruppe}).
\end{definition}
\begin{lemma}
	Ist \( (A, \bullet, e) \) Monoid, so ist \( (A^\times, \bullet, e) \) Gruppe (Einheitengruppe).
	\begin{proof}
		Per Definition ist jedes Element in \( A^\times \) eine Einheit. Außerdem ist \( e \) stets eine Einheit, da es invers zu sich selbst ist. Es bleibt zu zeigen, dass \( A^\times \) auch abgeschlossen ist unter der Operation \( \bullet \). Sei dazu \( a, b \in A^\times \) und \( a^\prime, b^\prime \) die jeweiligen Inversen. Wir zeigen das \( a \bullet b \) Einheit ist mit Inversem \( b^\prime \bullet a^\prime \):
		\[
			(a \bullet b) \bullet (b^{-1} \bullet a^{-1}) = a \bullet (b \bullet b^{-1}) \bullet a^{-1} = a \bullet e \bullet a^{-1} = a \bullet a^{-1} = e. \qedhere
		\]
	\end{proof}
\end{lemma}
\begin{example}
	Sei \( \mathbb{B} = \{ 0, 1 \} \). Das Symbol \( \nleftrightarrow \) steht für das Boolsche exklusive Oder (\( \xor \)).
	\begin{enumerate}
		\item \( (\mathbb{B}, \nleftrightarrow) \) ist abelsche Gruppe,
		\item \( (\mathbb{R} \setminus \{ 0 \}, \cdot) \) ist abelsche Gruppe,
		\item \( (\mathbb{Z}, -) \) ist Gruppe.
	\end{enumerate}
\end{example}
\begin{definition}
	Sei \( \mathfrak{A} = (A, \bullet) \) eine Gruppe und \( B \subseteq A \) nicht-leer. \( \mathfrak{B} = (B, \bullet) \) ist eine \textit{Untergruppe} von \( \mathfrak{A} \), wenn für alle \( a, b \in B \) auch \( a \bullet b^{-1} \in B \) gilt.
\end{definition}
% Ringe, Körper


\subsection{Graphen}\label{sec:pre_graphs}


\subsection{Beweismethoden}\label{sec:pre_proofs}



\newpage
\section{Alphabete, Wörter, Sprachen}\label{sec:awl}
Das erste Kapitel hat uns mit den nötigen mathematischen Grundlagen versorgt, die wir als Modellierungswerkzeuge in der theoretischen Informatik verwenden wollen. Wir definieren dazu später abstrakte Rechenmodelle, sogenannte Automaten, um das Verhalten von konkreten Rechenmodellen (z.B. Computern) formal zu erfassen. Zunächst sehen wir uns an, wie wir ganz allgemein diese konkreten Rechenmodelle funktionieren und wie sich diese Funktionsweisen möglichst knapp und allgemein (d.h. abstrakt, ``vereinfacht auf das wesentliche``) darstellen lassen. Nach diesem Kapitel haben wir das Hauptthema der Vorlesung, die \textit{abstrakte Automatentheorie}, vorbereitet.

\subsection{Grundlegende Definitionen}\label{sec:awl_def}
Wir fassen Aktionen eines Computers (oder eines Getränkeautomaten, \ldots) in unserer Abstrakion als \textit{Symbole} (Buchstaben) auf, im Rahmen dieser Vorlesung sind das immer nur endlich viele, d.h. das \textit{Alphabet} ist endlich. Aktionsfolgen (z.B. vom Einwurf einer Münze bis zur Ausgabe des Getränkes) entsprechen somit einem \textit{Wort}. Die Menge aller gültigen Aktionsfolgen (solche, die für das betrachtete System ``sinnvoll`` sind) bezeichnen wir dann als \textit{Sprache des Automaten}.
\begin{definition}
	Ein \textit{Alphabet} ist eine nicht-leere endliche Menge, deren Elemente als \textit{Symbole} bezeichnet sind.
\end{definition}
Alphabete werden durch griechische Großbuchstaben \( \Sigma, \Gamma \) oder Variationen wie \( \Sigma_1, \Gamma^\prime \) bezeichnet. Symbole werden durch kleine lateinische Buchstaben \( a, b, c, \ldots \) oder arabische Ziffern \( 0, 1, \ldots \) bezeichnet.
\begin{example}
	\
	\begin{enumerate}
		\item Das \textit{Boole'sche Alphabet} \( \{ 0, 1 \} \).
		\item Das \textit{Morsealphabet} \( \{ \cdot, -, \,\,\, \} \).
		\item Das \textit{ASCII-Alphabet} für zum Beispiel Textdateien.
	\end{enumerate}
\end{example}

\begin{definition}
	Ein \textit{Wort} über einem Alphabet \( \Sigma \) ist eine Abbildung
	\[
		w\colon [n] \to \Sigma.
	\]
	Für \( n = 0 \) ist \( w\colon \emptyset \to \Sigma \) das \textit{leere Wort}, was wir als \( \varepsilon \) bezeichnen.\\
	Die \textit{Länge} des Wortes \( w \) ist bezeichnet mit \( \left| w \right| = n \).\\
	\( \left| w \right|_a \coloneqq \left| \{ i \in [n] \mid w(i) = a \} \right| \) ist die \textit{Häufigkeit des Symbols} \( a \) im Wort \( w \).
\end{definition}
Wie in Abschnitt~\ref{sec:pre_mappings} lässt sich \( w \) auch als Tupel \( (a_0, \ldots, a_{n-1}) \) schreiben. Wir gehen hier sogar noch einen Schritt weiter und benutzen \( a_0 a_1 \ldots a_{n-1} \) als Abkürzung für die langen Schreibweisen. Für Wörter verwenden wir in der Regel \( u, v, w \) und Varianten als Bezeichner. In der Literatur sind auch kleine griechische Buchstaben \( \alpha, \beta, \ldots \) gebräuchlich.
\begin{definition}
	Sei \( \Sigma \) ein Alphabet. Dann ist \( \Sigma^n \coloneqq \Sigma^{[n]} \) die \textit{Menge aller Wörter mit Länge} \( n \) über \( \Sigma \).
	Die \textit{Menge aller Wörter} ist definiert als
	\[
		\Sigma^\ast \coloneqq \bigcup_{n \in \mathbb{N}} \Sigma^n
	\]
	und \( \Sigma^+ \coloneqq \Sigma^\ast \setminus \{ \varepsilon \} \).
\end{definition}
Wie in Abschnitt~\ref{sec:pre_setops} angekündigt wird für ein fixiertes \( \Sigma \) die Menge \( \Sigma^\ast \) unser Universum sein.
\begin{definition}
	Eine \textit{(formale) Sprache} über einem Alphabet \( \Sigma \) ist eine Teilmenge von \( \Sigma^\ast \).
\end{definition}
Sprachen bezeichnen wir in der Regel mit \( L, K, \ldots \) und Varianten.
\begin{example}
	\
	\begin{itemize}
		\item Die leere Sprache \( \emptyset \).
		\item Die Sprache, die nur das leere Wort enthält \( \{ \varepsilon \} \).
		\item Die Sprache aller Binärdarstellungen von Primzahlen \( \{ \bin(n) : n \in \mathbb{P} \} \).
		\item Die Menge aller grammatikalisch korrekten deutschen Sätze.
	\end{itemize}
\end{example}


\subsection{Operationen und Relationen auf Wörtern und Sprachen}\label{sec:awl_wordops}
Durch diese vorgegangen Definitionen haben wir das Fundament für die theoretische Informatik bereits definiert. Da dies nur mithilfe von Funktionen und Mengen  passiert ist lassen sich Beweismethoden und Ergebnisse aus der Mathematik einfach übertragen. Wir definieren nun noch ein paar Operationen auf Wörtern und erweitern diese Definitionen auf Sprachen.
\begin{definition}
	Seien \( u, v \in \Sigma^\ast \) mit \( m = \left| u \right|, n = \left| v \right| \). Die \textit{Konkatenation} (Verkettung) ist definiert als
	\begin{align*}
		(u \cdot v)&\colon [m{+}n] \to \Sigma \text{ mit}\\
		(u \cdot v)(i) &= \left\lbrace \begin{array}{ll}u(i), & i < m\\ v(i-m), & i \geq m. \end{array} \right.
	\end{align*}
	Außerdem ist \( u^0 \coloneqq \varepsilon \) und \( u^n \coloneqq u \cdot u^{n-1} \).
\end{definition}
Aus Bequemlichkeitsgründen wird der Punkt auch weggelassen. Für Sprachen erhalten wir noch die Definitionen.
\begin{definition}
	Seien \( L, K \subseteq \Sigma^\ast \) Sprachen.
	\begin{enumerate}
		\item \textit{Konkatenation.} \( L \cdot K \coloneqq \{ uv : u \in L, v \in K \} \) und \( L^0 \coloneqq \{ \varepsilon \}, L^n \coloneqq L \cdot L^{n-1} \).
		\item \textit{Inklusion.} Wie in Definition~\ref{def:subsets}.
		\item \textit{Vereinigung, Schnitt, Komplement, Differenz.} Wie in Definiton~\ref{def:setops}.
		\item \textit{Kleene'scher Abschluss.} (auch \textit{Iteration}, \textit{Kleene-Stern})
			\[
				L^\ast \coloneqq \bigcup_{n \in \mathbb{N}} L^n.
			\]
	\end{enumerate}
\end{definition}
\begin{definition}
	Seien \( u, v \) Wörter. Dann ist \( u \)
	\begin{itemize}
		\item \textit{Präfix} von \( v \) (geschrieben: \( u \sqsubseteq v \)), falls es ein Wort \( w \) gibt mit \( v = uw \),
		\item \textit{Infix} von \( v \), falls es Wörter \( w, w^\prime \) gibt mit \( v = wuw^\prime \),
		\item \textit{Suffix} von \( v \), falls es ein Wort \( w \) gibt mit \( v = wu \).
	\end{itemize}
\end{definition}
\begin{example}
	Sei \( v = aaba \).
	\begin{itemize}
		\item Die Präfixe von \( v \) sind \( \varepsilon, a, aa, aab, aaba \).
		\item Die Suffixe von \( v \) sind \( \varepsilon, a, ba, aba, aaba \).
		\item Die Infixe von \( v \) sind alle Präfixe und Suffixe, sowie \( ab, b \).
	\end{itemize}
\end{example}


\subsection{Gesetze für Wörter und Sprachen}\label{sec:awl_wordlaws}
In diesem Abschnitt wollen wir einige Gesetzmäßigkeiten, die bei der Anwendung von Operationen auf Wörtern und Sprachen gelten, herausarbeiten. Einige Eigenschaften übertragen sich sofort aus denen für Mengen aus Abschnitt~\ref{sec:pre_setlaws}. Bei anderen ist etwas mehr zu zeigen und bei wieder anderen gibt es vielleicht auch zunächst unintuitive Unterschiede.
\begin{remark*}
	Für das leere Wort \( \varepsilon \) gilt:
	\begin{enumerate}
		\item Es ist für jedes Wort sowohl Präfix, Infix als auch Suffix.
		\item Es ist das \textit{neutrale Element} der Konkatenation, d.h. für alle \( w \in \Sigma^\ast \) gilt \( \varepsilon w = w = w \varepsilon \).
		\item Daran anknüpfend gilt für jede Sprache \( L \), dass \(  \{ \varepsilon \} L = L = L  \{ \varepsilon \} \).
		\item Für jede Menge \( A \) (inklusive dem Fall \( A = \emptyset \)) ist \( A^0 = \{ \varepsilon \} \).
		\item \( \varepsilon \) ist eindeutig, d.h. es gibt kein zweites Wort mit diesen Eigenschaften.
		\item Weil es häufig durcheinander gebracht wird: \( \{ \varepsilon \} \neq \emptyset \).
	\end{enumerate}
\end{remark*}

\begin{remark*}
	Für Vereinigung, Schnitt, Differenz, \ldots von Sprachen gelten die selben Regeln (Assoziativ-, Kommutativ-, Distributivgesetze, deMorgan, Absorption, \ldots) wie für Mengen.
\end{remark*}

\begin{remark*}
	Für jede Sprache \( L \) gilt, dass \( \emptyset L = L \emptyset = \emptyset \).
	\begin{proof}
		Wir zeigen, dass \( L \emptyset \) leer ist. Der andere Fall geht analog. Angenommen es existiert ein \( w \in L \emptyset \). Dann lässt sich \( w \) zerlegen in \( w = uv \) mit \( u \in L, v \in \emptyset \). Da ein solches \( v \) nicht existieren kann (leere Menge), kann auch die gesamte Zerlegung und somit auch \( w \) nicht existieren. Also ist \( L \emptyset \) leer.
	\end{proof}
\end{remark*}



\newpage
\section{Endliche Automaten und Reguläre Sprachen}\label{sec:regular}
Wir haben formale Sprachen eingeführt als Modellierung für Prozessabläufe auf z.B. Computern. Diese Ansicht werden wir zunächst aber beiseite legen und erst in Kapitel~\ref{sec:process} wieder aufgreifen. In der Zwischenzeit untersuchen wir Sprachen auf verschiedene Eigenschaften und klassifizieren unter anderem nach der sogenannten \textit{Chomsky-Hierarchie}. Wir prüfen, wie sich Sprachen von formalen Systemen (Automaten, abstrakte Rechenmodelle) erkennen lassen. Diese Systeme wirken manchmal etwas künstlich, sind aber sehr sinnvoll, da sie sich mit den uns zu Verfügungen Werkzeugen aus der Mathematik gut handhaben lassen. Die daraus entstehenden Resultate haben außerdem auch einen nicht zu vernachlässigenden ästhetischen Wert für die theoretische Informatik. Zugegeben, der Praxisbezug offenbart sich bei einigen Sätzen nicht sofort und ist vielleicht auch gar nicht überalll vorhanden. Dennoch sollte der Wert dieser Ergebnisse auch nicht unterschätzt werden, denn wir liefern hier auch die Grundlagen zur Untersuchung, was sich prinzipiell mit Computern überhaupt berechnen lässt und die Erkenntnis, dass es Probleme in der Informatik gibt, die von einem Computer mehr Funktionalität beansprucht als andere Probleme (und vielleicht sogar mehr als ein Computer prinzipiell haben kann), sollte Motivation genug sein, sich mit theoretischer Informatik auseinanderzusetzen.

\subsection{Deterministische Endliche Automaten}\label{sec:regular_dfa}
Wir beginnen mit der Art Automaten, die ``die einfachste`` Klasse formaler Sprachen erkennen kann.
\begin{definition}
	Ein \textit{deterministischer endlicher Automat (DFA)} (von engl.: deterministic finite automaton) ist ein 5-Tupel
	\[
		(Q, \Sigma, \delta, q_0, F),
	\]
	mit
	\begin{itemize}
		\item \( Q \) eine nicht-leere, endliche Menge von \textit{Zuständen},
		\item \( \Sigma \) ein nicht-leeres, endliches \textit{Eingabealphabet},
		\item \( \delta\colon Q \times \Sigma \to Q \) die \textit{Transitionsfunktion},
		\item \( q_0 \in Q \) der \textit{Startzustand},
		\item \(F \subseteq Q \) die Menge der \textit{akzeptierenden Zustände} (oder \textit{Endzustände}).
	\end{itemize}
\end{definition}
DFAs lassen sich auch problemlos als Strukturen wie in Abschnitt~\ref{sec:pre_structures} auffassen. Aus Gründen der Lesbarkeit verzichten wir jedoch darauf. Wir bezeichnen DFAs stets mit \( \mathcal{A}, \mathcal{B}, \ldots \), Zustände mit \( p, q, r, s \) und Variationen.\\
Es lassen sich auch durchaus noch einfacherere Rechenmodelle definieren (z.B. durch Restriktionen gegenüber der Größe der Zustandsmenge), dies ist jedoch vorerst nicht sinnvoll. Auf die bereits angesprochene Chomsky-Hierarchie werden wir noch genauer eingehen, aber auch dort sind die Sprachen, die durch DFAs erkannt werden können, als die einfachste Klasse bezeichnet.\\
Möchte man einen DFA konkret angeben, so ist die Darstellung als Transitionsgraph sinnvoller, als die Angabe des 5-Tupels.
\begin{definition}
	Sei \( \mathcal{A} = (Q, \Sigma, \delta, q_0, F) \) ein DFA. Der \textit{Transitionsgraph} von \( \mathcal{A} \) ein beschrifteter Graph \( G_\mathcal{A} = ((Q, E), \lambda, q_0, F) \) mit
	\[
		E = \{ (p, q) \mid \text{es ex. } a \in \Sigma \text{ mit } \delta(p, a) = q \}
	\]
	und
	\[
		\lambda\colon E \to 2^\Sigma, \quad (p, q) \mapsto \{ a \mid \delta(p, a) = q \}.
	\]
\end{definition}
Aus Gründen der Bequemlichkeit, lassen wir die Mengenklammern bei der Beschriftung der Transitionen weg. Der Startzustand \( q_0 \) bekommt einfach eine eingehende Kante ohne Beschriftung und ohne Startknoten. Die Endzustände werden zusätzlich eingekreist. Ein Beispieltransitionsgraph ist in Abbildung~\ref{fig:dfa_ex1}.
\begin{figure}
	\centering
	\input{figs/dfa_ex1}
	\caption{DFA für die Sprache aus Beispiel~\ref{exp:ex1}.}
	\label{fig:dfa_ex1}
\end{figure}
Wir werden die Begriffe Automat und Transitionsgraph gelegentlich synonym verwenden, da sich sowohl im Graphen als auch in der ursprünglichen Automatenstruktur alle Informationen befinden. Wir greifen dann auf den Begriff zurück, der für das aktuelle Thema die bequemere Anschauung hat.\\
Wir schauen uns nun das Verhalten eines DFA auf einem Wort an.
\begin{definition}
	Sei \( \mathcal{A} = (Q, \Sigma, \delta, q_0, F) \) ein DFA.
	Ein \textit{Lauf} von \( \mathcal{A} \) auf einem Wort \( w = a_0 \ldots a_{n-1} \) für ein \( n \in \mathbb{N} \) ist eine endliche Folge
	\[
		(r_0, a_0, r_1, a_1, \ldots, a_{n-1}, r_n),
	\]
	wobei \( r_0, \ldots, r_n \in Q \) und \( a_0, \ldots, a_{n-1} \in \Sigma \), sodass
	\begin{enumerate}
		\item \( r_0 = q_0 \),
		\item \( \delta(r_i, a_i) = r_{i+1} \) für \( i \in [n] \).
	\end{enumerate}
	Wir sagen ein Lauf ist \textit{akzeptierend}, wenn zusätzlich \( r_n \in F \) gilt.
\end{definition}
Wir bezeichnen Läufe in der Regel mit \( \varrho \) bzw. Variationen. Einen Lauf \( (r_0, a_0, r_1, a_1, \ldots, a_{n-1}, r_n) \) kürzen wir gelegentlich durch \( (r_0, r_1, \ldots, r_n) \) ab, wenn die Symbole nicht relevant für unsere Betrachtungen sind.
\begin{remark*}
	Zu jedem Wort \( w \in \Sigma^\ast \) existiert genau ein Lauf von \( \mathcal{A} \) auf \( w \).
\end{remark*}
\begin{definition}
\
	\begin{enumerate}
		\item Ein DFA \( \mathcal{A} = (Q, \Sigma, \delta, q_0, F) \) \textit{akzeptiert} ein Wort \( w \in \Sigma^\ast \), wenn der Lauf von \( \mathcal{A} \) akzeptierend ist. Andernfalls \textit{verwirft} \( \mathcal{A} \) das Wort \( w \).
		\item Die von einem DFA \( \mathcal{A} = (Q, \Sigma, \delta, q_0, F) \) \textit{erkannte Sprache} ist
			\[
				L(\mathcal{A}) \coloneqq \{ w \in \Sigma^\ast \mid \mathcal{A} \text{ akzeptiert } w \}.
			\]
		\item Eine Sprache \( L \) heißt \textit{DFA-erkennbar}, wenn es einen DFA \( \mathcal{A} \) gibt, sodass \( L = L(\mathcal{A}) \).
	\end{enumerate}
\end{definition}
\begin{example}\label{exp:ex1}
	Betrachte erneut den Automaten \( \mathcal{A} \) in Abbildung~\ref{fig:dfa_ex1}.
	\begin{itemize}
		\item Sei \( w_1 = abaaba \). Der Lauf von \( \mathcal{A} \) auf \( w_1 \) ist
			\[
				\varrho_1 = (q_0, q_1, q_2, q_1, q_1, q_2, q_1).
			\]
			D.h. \( \mathcal{A} \) akzeptiert \( w_1 \).
		\item Sei \( w_2 = baa \). Der Lauf von \( \mathcal{A} \) auf \( w_2 \) ist
			\[
				\varrho_2 = (q_0, q_3, q_3, q_3).
			\]
			D.h. \( \mathcal{A} \) verwirft \( w_2 \).
		\item \( \mathcal{A} \) erkennt die Sprache 
			\[
				L = \{ w \in \{a, b\}^\ast \mid w \text{ beginnt und endet mit } a \}.
			\]
	\end{itemize}
	Die letzte Aussage sagt etwas über das Verhalten des Automaten auf allen, d.h. unendlich vielen, Wörtern aus. Man kann also nicht für jedes Wort einzeln zeigen, dass sich der Automat korrekt verhält. Stattdessen beweisen wir die Aussage per Induktion.
	\begin{proof}
		Wir zeigen die folgende Aussage:
		\begin{center}
			\( \mathcal{A} \) akzeptiert das Wort \( w \) {g.d.w.} \( w \) beginnt und endet mit \( a \).				\end{center}
		Beweis per vollständige Induktion über Wortlänge \( n \in \mathbb{N} \). Ist \( r = (r_0, \ldots, r_n) \) der Lauf auf dem Wort \( w = a_0 \ldots a_{n-1} \), so ist
		\[
			r_n = \left\lbrace 
					\begin{array}{ll}
						q_0, & w = \varepsilon\\
						q_1, & a_0 = a_{n-1} = a\\
						q_2, & a_0 = a \text{ und } a_{n-1} = b\\
						q_3, & a_0 = b.
					\end{array}
				\right.
		\]
		Wir wollen also zeigen, dass ein Lauf von \( \mathcal{A} \) auf einem Wort dann und nur dann in \( q_1 \), dem einzigen akzeptierendem Zustand, endet, wenn \( w \) mit \( a \) beginnt und endet.\\
		Induktionsverankerung: \( n = 0 \). Dann ist \( w = \varepsilon \) und der Lauf von \( \mathcal{A} \) auf \( w \) ist \( r = (q_0) \), also auch \( r_n = r_0 = q_0 \).\checkmark\\
		Induktionshypothese (IH): Für \( w = a_0 \ldots a_{n-1} \) sei \( r = (r_0, \ldots, r_n) \) der Lauf auf \( \mathcal{A} \) mit \( r_n \) wie in der Behauptung.\\
		Induktionsschritt: Betrachte nun das Wort \( w = a_0 \ldots a_n \).
		\begin{itemize}
			\item \( a_0 \ldots a_{n-1} = \varepsilon \). Dann ist nach IH \( r_n = q_0 \) und somit
				\[
					r_{n+1} = \delta(r_n, a_n) = \left\lbrace
							\begin{array}{ll}
								q_1, & a_n = a\\
								q_3, & a_n = b.
							\end{array}
						\right.
				\]
			\item \( a_0 = a_{n-1} = a \). Dann ist nach IH \( r_n = q_1 \) und somit
				\[
					r_{n+1} = \delta(r_n, a_n) = \left\lbrace
							\begin{array}{ll}
								q_1, & a_n = a\\
								q_2, & a_n = b.
							\end{array}
						\right.
				\]
			\item \( a_0 = a \) und \( a_{n-1} = b \). Dann ist nach IH \( r_n = q_2 \) und somit
				\[
					r_{n+1} = \delta(r_n, a_n) = \left\lbrace
							\begin{array}{ll}
								q_1, & a_n = a\\
								q_2, & a_n = b.
							\end{array}
						\right.
				\]
			\item \( a_0 = b \). Dann ist nach IH \( r_n = q_3 \) und somit
				\[
					r_{n+1} = \delta(r_n, a_n) = q_3
				\]
				unabhängig von \( a_n \).
		\end{itemize}
		Insgesamt gilt also: 
		\begin{align*}
			% if you think this is ugly, I agree.
			w \text{ beginnt und endet mit } a. &\text{ g.d.w. } \text{Ist } r = (r_0, \ldots, r_n) \text{ der Lauf von }\\ &\quad\quad\quad\,\, \mathcal{A} \text{ auf } w \text{ so gilt } r_n = q_1 \in F.\\
			&\text{ g.d.w. } \mathcal{A} \text{ akzeptiert } w.\\
			&\text{ g.d.w. } L(\mathcal{A}) = L.
		\end{align*}
	\end{proof}
\end{example}
So ausführlich wie hier werden wir später nicht mehr beweisen, dass ein gegebener Automat ``das richtige tut``, sollte dies nicht klar sein werden wir die Funktionsweise nur grob erläutern. Ausführliche Beweise sind im Wesentlichen dann gefordert, wenn man zum Beispiel zeigen möchte, dass eine Sprache nicht DFA-erkennbar ist.


\subsection{Abschlusseigenschaften DFA-erkennbarer Sprachen}\label{sec:regular_closure}
Bei einer Einteilung aller möglichen Sprachen in Klassen will man in einer möglichst sinnvollen Weise vorgehen. Damit meint man u.a., dass die einzelnen Klassen in sich abgeschlossen sind bzgl. verschiedener Operationen oder auch andere Eigenschaften haben, die in einer wissenschaftlichen Weise ``schön`` sind. Die folgenden Abschnitte sind dazu da um zu zeigen, dass DFA-erkennbare Sprachen dies in vielerlei Hinsicht erfüllen. In diesem Abschnitt beginnen wir damit zu zeigen, dass DFA-erkennbare Sprachen unter den üblichen Mengenoperationen (Komplementbildung, Vereinigung und Schnitt) abgeschlossen sind. Im späteren Verlauf werden wir noch einige andere Operationen betrachten.\par
Wir zeigen als erstes, dass die DFA-erkennbaren Sprachen unter Komplementbildung abgeschlossen sind.
\begin{theorem}\label{thm:regular_complement}
	Sei \( L \subseteq \Sigma^\ast \) eine DFA-erkennbare Sprache. Dann ist auch \( \comp{L} \) DFA-erkennbar.
	\begin{proof}
		Sei \( L \subseteq \Sigma^\ast \) eine beliebige DFA-erkennbare Sprache. D.h. es existiert ein DFA \( \mathcal{A} = (Q, \Sigma, \delta, q_0, F) \), der \( L \) erkennt. Wir konstruieren aus \( \mathcal{A} \) den DFA
		\[
			\comp{\mathcal{A}} = (Q, \Sigma, \delta, q_0, Q \setminus F),
		\]
		welcher die Sprache \( \comp{L} \) erkennt. Die Konstruktion vertauscht also lediglich akzeptierende und nicht-akzeptierende Zustände. Wir müssen nun noch zeigen, dass diese Konstruktion korrekt ist, also formal, dass \( \comp{L(\mathcal{A})} = L(\comp{\mathcal{A}}) \) gilt. Dazu zeigen wir folgende Aussage, aus der offensichtlich die Behauptung folgt.
		\begin{center}
			Für alle \( w \in \Sigma^\ast \) gilt: \( \mathcal{A} \) akzeptiert \( w \) {g.d.w.} \( \comp{\mathcal{A}} \) verwirft \( w \).
		\end{center}
		Sei also \( w \in \Sigma^\ast \). Da \( \mathcal{A} \) und \( \comp{\mathcal{A}} \) den selben Startzustand \( q_0 \) und die selbe Transitionsfunktion \( \delta \) haben, haben beide Automaten den selben (eindeutigen) Lauf \( (r_0, \ldots, r_n) \) auf \( w \). \( \mathcal{A} \) akzeptiert \( w \) falls \( r_n \in F \). Dann gilt \( r_n \notin Q \setminus F \) und somit \( \comp{\mathcal{A}} \) verwirft \( w \). Die andere Richtung geht analog. Also gilt die Behauptung.
	\end{proof}
\end{theorem}
Diese Abschlusseigenschaft war einfach zu beweisen, da wir nur eine kleine Änderung am ursprünglichen DFA machen mussten und dann wieder einfach über den eindeutigen Lauf argumentieren konnten. Die Idee hinter der Konstruktion ist auch sehr intuitiv, da wir genau die Wörter akzeptieren wollen, die vom ursprünglichen Automaten verworfen wurden, also deren Läufe in nicht-akzeptierenden Zuständen enden. Der Ansatz die Zustände einfach zu vertauschen drängt sich also geradezu auf.\par
Für den Abschluss unter Vereinigung ist etwas mehr Arbeit zu machen. Wir haben also nun zwei DFA-erkennbare Sprachen (und damit die zugehörigen Automaten) und wollen nun prüfen ob ein Wort in wenigstens einer der beiden Sprachen liegt. Wir müssen nun also einen Automaten konstruieren, der zwei gegebene Automaten simuliert. Diese Simulation muss synchron bzw. parallel stattfinden, eine sequentielle (d.h. Hintereinander-)Ausführung beider Automaten ist nicht möglich (da DFAs bereits gelesene Symbole ``vergessen``, ein explizites ``Abspeichern`` ist nur möglich für konstant viele Symbole -- das reicht nicht für beliebig große Eingabelängen). Wir präsentieren für die parallele Ausführung nun die Produktkonstruktion im Rahmen des nächsten Satzes.
\begin{theorem}\label{thm:regular_intersection}
	Seien \( L_1, L_2 \subseteq \Sigma^\ast \) DFA-erkennbare Sprachen. Dann ist auch \( L_1 \cap L_2 \) DFA-erkennbar.
	\begin{proof}
		Seien \( \mathcal{A}_1 = (Q_1, \Sigma, \delta_1, q_0^1, F_1) \) und \( \mathcal{A}_1 = (Q_2, \Sigma, \delta_2, q_0^2, F_2) \) die DFAs für \( L_1, L_2 \). Wir betrachten den \textit{Produktautomaten}
		\[
			\mathcal{A} \coloneqq (Q_1 \times Q_2, \Sigma, \delta, (q_0^1, q_0^2), F)
		\]
		mit \( \delta((p, q), a) = (\delta_1(p, a), \delta_2(q, a)) \) für alle \( p \in Q_1, q \in Q_2, a \in \Sigma \) und \( F = F_1 \times F_2 \).\\
		Wir zeigen nun, dass \( L(\mathcal{A}) = L(\mathcal{A}_1) \cap L(\mathcal{A}_2) \) ist.
		Sei dazu \( w = a_0 \ldots a_{n-1} \) gegeben. Wir zeigen, dass \( \mathcal{A} \) akzeptiert \( w \) {g.d.w.} \( \mathcal{A}_1 \) und \( \mathcal{A}_2 \) das Wort \( w \) akzeptiert.\\
		``Wenn \( \mathcal{A}_1 \) und \( \mathcal{A}_2 \) akzeptiert, dann akzeptiert \( \mathcal{A} \)``:
		Es gibt also einen Lauf \( (r_0, \ldots, r_n) \) mit \( r_n \in F_1 \) von \( \mathcal{A}_1 \) und einen Lauf \( (p_0, \ldots, p_n) \) mit \( p_n \in F_2 \) von \( \mathcal{A}_2 \) jeweils auf \( w \). Dann ist der Lauf auf auf \( \mathcal{A} \)
		\[
			((r_0, p_0), \ldots, (r_n, p_n)),
		\]
		da in jedem Schritt 
		\begin{align*}
			\delta((r_i, p_i), a_i) &= (\delta_1(r_i, a_i), \delta_2(p_i, a_i)) \\
			&= (r_{i+1}, p_{i+1}).
		\end{align*}
		Wegen \( r_n \in F_1 \) und \( p_n \in F_2 \) ist auch \( (r_n, p_n) \in F_1 \times P_2 = F \). Also akzeptiert \( \mathcal{A} \).\\
		``Wenn \( \mathcal{A} \) akzeptiert, dann akzeptieren \( \mathcal{A}_1 \) und \( \mathcal{A}_2 \)``:
		Wir zeigen die Kontraposition dieser Aussage. \( \mathcal{A}_1 \) oder \( \mathcal{A}_2 \) verwirft. Die Läufe von \( \mathcal{A}_1, \mathcal{A}_2 \) sind also \( (r_0, \ldots, r_n) \) und \( (p_0, \ldots, p_n) \) mit \( r_n \notin F_1 \) oder \( p_n \notin F_2 \). Wie oben ist \( ((r_0, p_0), \ldots, (r_n, p_n)) \) der Lauf von \( \mathcal{A} \) und es gilt \( (r_n, p_n) \notin F_1 \times F_2 = F \). Somit verwirft auch \( \mathcal{A} \) das Wort.
	\end{proof}
\end{theorem}
Eine Beispielkonstruktion für einen Produktautomaten für den Schnitt zweier DFA-erkennbarer Sprachen befindet sich in Abbildung~\ref{fig:dfa_product}.
\begin{figure}
	\centering
	\input{figs/dfa_product}
	\caption{Produktkonstruktion: Der Automat \( \mathcal{A} \) erkennt die Sprache \( \{ w \in \{a, b \}^\ast \mid \left| w \right|_a + \left| w \right|_b \text{ teilbar durch } 3 \} \), \( \mathcal{B} \) erkennt die Sprache \( \{ ua : u \in \{a, b\}^\ast \} \). Der Produktautomat erkennt den Schnitt.}
	\label{fig:dfa_product}
\end{figure}
Aus den Sätzen~\ref{thm:regular_complement} und~\ref{thm:regular_intersection} erhält man nun einfach alle übrigen Abschlusseigenschaften für die üblichen Mengenoperationen, aber auch mit der Produktkonstruktion lassen sich diese Eigenschaften zeigen.
\begin{corollary}\label{cor:regular_intersection}
	Seien \( L_1, L_2 \) DFA-erkennbare Sprachen. Dann sind auch die Sprachen \( L_1 \cup L_2, L_1 \setminus L_2 \) DFA-erkannbar.
	\begin{proof}
		Wegen Satz~\ref{thm:regular_complement} sind auch \( \comp{L_1}, \comp{L_2} \) DFA-erkennbar und damit nach Satz~\ref{thm:regular_intersection} \( \comp{L_1} \cap \comp{L_2} \). Mit erneuter Anwendung von Satz~\ref{thm:regular_complement} ist auch \( \comp{\comp{L_1} \cap \comp{L_1}} \) DFA-erkennbar, was nach den DeMorgan'schen Gesetzen \( L_1 \cup L_2 \) entspricht. Alternativ kann man in der Produktkonstruktion auch \( F = (F_1 \times Q_2) \cup (Q_1 \times F_2) \) setzen und erhält einen DFA für \( L_1 \cup L_2 \).\\
		Für \( L_1 \setminus L_2 \) setzt man \( F = F_1 \times (Q_2 \setminus F_2) \) und erhält einen DFA für die Differenz.
	\end{proof}
\end{corollary}
Die Produktkonstruktion ist eine sehr grundlegende Konstruktion, die in ähnlicher Form immer wieder Anwendungen findet. Wir können auch eigene Sprachoperatoren erfinden und DFA-erkennbare Sprachen daraufhin untersuchen ob sie abgeschlossen sind bzgl. dieser Operationen.
\begin{example}
	Seien \( L, K \subseteq \Sigma^\ast \). Wir definieren die Operation \textit{Perfect Shuffle} wie folgt:
	\[
		L \shuffle K \coloneqq \{ a_0 b_0 \ldots a_{n-1} b_{n-1} : a_0 \ldots a_{n-1} \in L, b_0 \ldots b_{n-1} \in K \}.
	\]
\end{example}
\begin{theorem}
	Seien \( L_1, L_2 \subseteq \Sigma^\ast \) DFA-erkennbare Sprachen. Dann ist auch \( L_1 \shuffle L_2 \) DFA-erkennbar.
	\begin{proof}
		Seien \( \mathcal{A}_1 = (Q_1, \Sigma, \delta_1, q_0^1, F_1), \mathcal{A}_2 = (Q_2, \Sigma, \delta_2, q_0^2, F_2) \) die DFAs für \( L_1, L_2 \). Wir definieren wieder einen Produktautomaten
		\[
			\mathcal{A} = (Q_1 \times Q_2 \times [2], \Sigma, \delta, (q_0^1, q_0^2, 0), F)
		\]
		mit
		\[
			\delta((p, q, i), a) = \left\lbrace
				\begin{array}{ll}
					(\delta_1(p, a), q, 1), & i = 0\\
					(p, \delta_2(q, a), 0), & i = 1
				\end{array}
			\right.
		\]
		und \( F = F_1 \times F_2 \times \{0\} \). Die Idee ist diesmal nur eine \textit{quasi-parallele} Simulation von \( \mathcal{A}_1 \) und \( \mathcal{A}_2 \). Die dritte Komponente des Zustands gibt an in welchem Automat \( \mathcal{A} \) den nächsten Schritt simulieren soll. Wir akzeptieren, wenn beide Simulationen in einem Endzustand sind und der zuletzt durchgeführte Simulationsschritt auf \( \mathcal{A}_2 \) war. Wir zeigen nun noch, dass die Konstruktion funktioniert.\\
		``\( L(\mathcal{A}) \subseteq L_1 \shuffle L_2 \)``: Sei \( w = c_0 \ldots c_{n-1} \in L(\mathcal{A}) \). D.h. es existiert ein Lauf 
		\[
			\varrho = ((p_0, q_0, i_0), \ldots, (p_n, q_n, i_n))
		\]
		von \( \mathcal{A} \) auf \( w \) mit \( p_n \in F_1, q_n \in F_2 \) und \( i_j = j \mod 2 \) und \( i_0 = i_n = 0 \) (insbesondere ist \( \left| w \right| \) gerade), wobei abwechselnd in jedem Schritt \( j \) die \( 1 + i_j \)te Komponenten gleich bleibt (s. Definition von \( \delta \)). Aus den geraden Positionen in \( \varrho \) (die mit der dritten Komponente \( 0 \)) ergeben dann einen Lauf von \( \mathcal{A}_1 \) auf dem Wort \( c_0 c_2 \ldots c_{n-2} \). Umgekehrt sind die ungeraden Positionen induziert durch den Lauf von \( \mathcal{A}_2 \) auf \( c_1 c_3 \ldots c_{n-1} \). Wegen \( p_n \in F_1 \) und \( i_n = 0 \) ist \( p_{n-1} = p_n \in F_1 \) und somit akzeptiert \( \mathcal{A}_1 \) das Wort \( c_0 c_2 \ldots c_{n-2} \). Analog für \( \mathcal{A}_2 \). Also ist \( w \in L(\mathcal{A}_1) \shuffle L(\mathcal{A}_2) \).\\
		``\( L(\mathcal{A}) \supseteq L_1 \shuffle L_2 \)``: Sei \( w = a_0 b_0 \ldots a_{n-1} b_{n-1} \in L_1 \shuffle L_2 \). Dann exitieren Läufe \( \varrho_1 = (p_0, \ldots, p_n), \varrho_2 = (q_0, \ldots, q_n) \) mit \( p_n \in F_1, q_n \in F_2 \) von \( \mathcal{A}_1, \mathcal{A}_2 \). Nach Definition von \( \delta \) ist der Lauf auf \( \mathcal{A} \) dann
		\[
			((p_0, q_0, 0), (p_1, q_0, 1), \ldots, (p_n, q_{n-1}, 1), (p_n, q_n, 0))
		\]
		und \( \mathcal{A} \) akzeptiert \( w \).
	\end{proof}
\end{theorem}


\subsection{Nichtdeterministische Endliche Automaten}\label{sec:regular_nfa}
Im letzten Abschnitt haben wir gesehen, dass unter einigen Operationen abgeschlossen sind. Die Mengenoperationen wirken dabei ohnehin von Vorteil für weitere Betrachtungen unter mathematischen Aspekten. Perfect Shuffle könnte man dagegen als eine Routine sehen, ob zum Beispiel ein Prozessor zwei Prozessen wirklich abwechselnd Berechnungszeit gibt. Wir würden gerne weitere Abschlusseigenschaften kennenlernen, besonders die Konkatenation unter der Kleene-Stern wären nun interessant. Eine simultane Ausführung zweier Automaten bringt hier jedoch nichts mehr, da beide Operationen eher von sequentieller Natur sind (Hintereinanderausführung, Wiederholung). DFAs sind für diese Aufgaben zunächst nicht sonderlich sinnvoll. Denkbar wäre ein Modell, dass nach Erreichen eines Endzustandes den nächsten Automaten (im Falle der Konkatenation) auf dem Rest des Wortes startet. Ein DFA weiß aber nicht ad hoc wie das Wort zerlegt ist, es kann also sein, dass nach Erreichen eines Endzustandes der erste Automat noch weiterlaufen soll und erst bei erneutem Besuch eines akzeptierenden Zustandes das zweite Wort losgeht. Zu diesem Zweck führen wir das Prinzip des Nichtdeterminismus ein. Ein Automat kann dann ``raten`` in welchen Zustand er wechseln soll (bzw. ob er ``den nächsten Automaten startet``). Dieses Konzept wirkt etwas unnatürlich, da es nicht von einem Computer simuliert werden kann (Zufall \( \neq \) Nichtdeterminismus!), in unserem Modell rät der Automat stets ``richtig``.
\begin{definition}
	Ein \textit{nicht-deterministischer endlicher Automat (NFA)} (von engl.: non-deterministic finite automaton) ist ein 5-Tupel
	\[
		(Q, \Sigma, \Delta, q_0, F),
	\]
	mit
	\begin{itemize}
		\item \( Q \) eine nicht-leere, endliche Menge von \textit{Zuständen},
		\item \( \Sigma \) ein nicht-leeres, endliches \textit{Eingabealphabet},
		\item \( \Delta \subseteq Q \times \Sigma \times Q \) die \textit{Transitionsrelation},
		\item \( q_0 \in Q \) der \textit{Startzustand},
		\item \(F \subseteq Q \) die Menge der \textit{akzeptierenden Zustände} (oder \textit{Endzustände}).
	\end{itemize}
\end{definition}
Wir bezeichnen NFAs genau wie DFAs mit \( \mathcal{A}, \mathcal{B} \) usw. Die Transitionsgraphen sehen ebenfalls genauso aus, nur, dass nun Zustände mehrere Kanten haben können, die gleich beschriftet sind. Außerdem ist es erlaubt, dass Transitionen komplett fehlen.
\begin{remark*}
	Eine äquivalente und ebenfalls verbreitete Definition benutzt statt einer Transitionsrelation wieder eine Transitionsfunktion \( \delta\colon Q \times \Sigma \to 2^Q \). 
\end{remark*}
\begin{remark*}
	Auch wenn es widersprüchlich klingt, aber jeder DFA kann auch als ein NFA gesehen werden. Genauer gesagt ist ein DFA ein NFA bei dem die Transitionsrelation der Graph einer totalen Funktion \( Q \times \Sigma \to Q \) ist.
\end{remark*}
Bisher ist noch nicht klar, wie das Akzeptanzverhalten von NFAs sein soll.
\begin{definition}
	Sei \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) ein NFA.
	Ein \textit{Lauf} von \( \mathcal{A} \) auf einem Wort \( w = a_0 \ldots a_{n-1} \) für ein \( n \in \mathbb{N} \) ist eine endliche Folge
	\[
		(r_0, a_0, r_1, a_1, \ldots, a_{n-1}, r_n),
	\]
	wobei \( r_0, \ldots, r_n \in Q \) und \( a_0, \ldots, a_{n-1} \in \Sigma \), sodass
	\begin{enumerate}
		\item \( r_0 = q_0 \),
		\item Für alle \( i \in [n] \) gilt, dass \( (r_i, a_i, r_{i+1}) \in \Delta \).
	\end{enumerate}
	Wir sagen ein Lauf ist \textit{akzeptierend}, wenn zusätzlich \( r_n \in F \) gilt.
\end{definition}
\begin{remark*}
	Für NFAs müssen Läufe nicht mehr eindeutig sein. Es kann zu einem Wort mehrere Läufe geben oder auch gar keine Läufe, wenn entsprechende Transitionen fehlen.
\end{remark*}
\begin{remark*}
	Produktkonstruktionen um zum Beispiel den Schnitt zweier NFA-erkennbaren Sprachen zu erkennen funktioniert für NFAs genauso wie für DFAs. Schwieriger ist es aber schon das Komplement zu erkennen -- ein einfaches vertauschen von akzeptierenden und nicht-akzeptierenden Zuständen genügt nicht, da weiterhin Wörter aus der Sprache rausgelassen werden für die kein Lauf existiert.
\end{remark*}
\begin{definition}
\
	\begin{enumerate}
		\item Ein NFA \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) \textit{akzeptiert} ein Wort \( w \in \Sigma^\ast \), wenn es mindestens einen akzeptierenden Lauf von \( \mathcal{A} \) gibt. Andernfalls \textit{verwirft} \( \mathcal{A} \) das Wort \( w \).
		\item Die von einem NFA \( \mathcal{A} = (Q, \Sigma, \delta, q_0, F) \) \textit{erkannte Sprache} ist
			\[
				L(\mathcal{A}) \coloneqq \{ w \in \Sigma^\ast \mid \mathcal{A} \text{ akzeptiert } w \}.
			\]
		\item Eine Sprache \( L \) heißt \textit{NFA-erkennbar}, wenn es einen NFA \( \mathcal{A} \) gibt, sodass \( L = L(\mathcal{A}) \).
	\end{enumerate}
\end{definition}
\begin{example}\label{exp:ex2}
	Wir betrachten die Sprache
	\[
		L = \{ w \in \{a, b\}^\ast \mid \text{das vorletzte Symbol in } w \text{ ist } a \}.
	\]
	In Abbildung~\ref{fig:nfa_ex1} befindet sich ein DFA und ein NFA für die Sprache. Wir sehen, dass der NFA weniger Zustände hat. Dies ist nicht überraschen, denn der DFA muss sich stets das zuletzt gelesene Symbol im Zustand merken, während der NFA dies nicht muss und einfach ``rät`` welches Symbol das vorletzte ist.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/nfa_ex1dfa}
		\caption{DFA}
		\label{fig:nfa_ex1dfa}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/nfa_ex1nfa}
		\caption{NFA}
		\label{fig:nfa_ex1nfa}
	\end{subfigure}
	\caption{Ein DFA und ein NFA für die Sprache aus Beispiel~\ref{exp:ex2}.}
	\label{fig:nfa_ex1}
\end{figure}
Für NFAs ist das \textit{Wortproblem}, das Problem ob ein Automat ein gegebenes Wort akzeptiert, nicht ganz so offensichtlich zu lösen wie für DFAs. Bei DFAs müssen wir lediglich die eindeutige Transitionsfolge anwenden und prüfen ob der letzte Zustand ein akzeptierender ist. Für NFAs können wir natürlich alle möglichen Läufe ausprobieren und prüfen, ob ein akzeptierender dabei ist. Doch es gieht effizienter mit der Erreichbarkeitsrelation.
\begin{definition}\label{def:reachability}
	Sei \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) ein NFA und \( w = a_0 \ldots a_{n-1} \). Ein Zustand \( q \in Q \) heißt \textit{erreichbar} von \( p \) über \( w \) (geschrieben \( \mathcal{A}: p \reaches{w} q \)), wenn es einen Lauf \( (r_0, \ldots, r_n) \) gibt mit 
	\begin{itemize}
		\item \( r_0 = p, r_n = q \) und
		\item \( (r_i, a_i, r_{i+1}) \in \Delta \) für alle \( i \in [n] \).
	\end{itemize}
\end{definition}
Diese Notation können wir auch selbstverständlich auf DFAs anwenden. Wir lassen gelegentlich, dass \( \mathcal{A} \) weg, wenn der Automat aus dem Kontext klar ist.
\begin{definition}
	Sei \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) ein NFA und \( w \in \Sigma^\ast \). Die \textit{Menge der in \( \mathcal{A} \) über \( w \) erreichbaren Zustände} ist definiert als
	\[
		E(\mathcal{A}, w) \coloneqq \{ q \in Q \mid \mathcal{A}: q_0 \reaches{w} q \}.
	\]
\end{definition}
Wir haben zwei Lemmata, die uns die Anwendung dieser Definition nahelegen.
\begin{lemma}\label{lem:reach1}
	Sei \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F)\) ein NFA und \( w \in \Sigma^\ast \). \( w \in L(\mathcal{A}) \) {g.d.w.} \( E(\mathcal{A}, w) \cap F \neq \emptyset \).
	\begin{proof}
		``wenn, dann``: Sei \( w \in L(\mathcal{A}) \). Dann exitiert ein Lauf \( (r_0, \ldots, r_n) \) von \( \mathcal{A} \) auf \( w \) mit \( r_n \eqqcolon q \in F \), also \( q_0 \reaches{w} q \). Somit ist \( q \in E(\mathcal{A}, w) \). Also ist \( \emptyset \neq \{q\} \subseteq E(\mathcal{A}, w) \cap F \).\\
		``genau dann``: Es exitiert ein \( q \in E(\mathcal{A}, w) \cap F \), also \( q \in F \) und \( q \in E(\mathcal{A}, w) \). Also gibt es einen Lauf von \( \mathcal{A} \) auf \( w \), der in \( q_0 \) startet und in \( q \) endet. Dieser ist akzeptierend, also ist \( w \in L(\mathcal{A}) \).
	\end{proof}
\end{lemma}
\begin{lemma}\label{lem:reach2}
	Sei \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) ein NFA.
	\begin{enumerate}
		\item \( E(\mathcal{A}, \varepsilon) = \{q_0\} \),
		\item Für alle \( u \in \Sigma^\ast \) und \( a \in \Sigma \) gilt
			\[
				E(\mathcal{A}, ua) = \bigcup_{p \in E(\mathcal{A}, u)} \{ q \in Q \mid (p, a, q) \in \Delta \}.
			\]
	\end{enumerate}
	\begin{proof}
		Induktionsverankerung: \( q \in E(\mathcal{A}, \varepsilon) \) {g.d.w.} \( q_0 \reaches{\varepsilon} q \) {g.d.w.} \( q = q_0 \). \checkmark\\
		Induktionsschritt: \( q \in E(\mathcal{A}, ua) \), also \( q_0 \reaches{ua} q \). Damit gibt es ein \( p \in Q \), sodass \( q_0 \reaches{u} p \reaches{a} q \). Wir folgern, dass \( p \in E(\mathcal{A}, u) \) und \( (p, a, q) \in \Delta \) und somit \( q \in \bigcup_{p \in E(\mathcal{A}, u)} \{ r \in Q \mid (p, a, r) \in \Delta \} \). Rückrichtung analog.
	\end{proof}
\end{lemma}
Mit Hilfe der Erreichbarkeitsrelation und Lemmata~\ref{lem:reach1} und~\ref{lem:reach2}, erhalten wir für das Wortproblem für NFAs einen Algorithmus.
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\underline{NFA-Akzeptanz}{$(\mathcal{A}, w)$}\\
	\Input{NFA $\mathcal{A}$, Wort $w$}
	\Output{Ja {g.d.w.} $\mathcal{A}$ akzeptiert $w = a_0 \ldots a_{n-1}$.}	
	$u \coloneqq \varepsilon$\\
	$E(\mathcal{A}, u) \coloneqq \{ q_0 \}$\\	
	\For{$i = 0, \ldots, n-1$}{
		Bestimme $E(\mathcal{A}, ua_i)$ aus $E(\mathcal{A}, u)$ (Lemma~\ref{lem:reach2})\\
		$u \coloneqq ua_i$
	}
	Prüfe, ob $E(\mathcal{A}, w) \cap F \neq \emptyset$.
	\caption{Wortproblem für NFAs}
	\label{alg:nfaacc}
\end{algorithm}
\begin{example}
	Wir betrachten erneut den NFA in Abbildung~\ref{fig:nfa_ex1nfa} und die Erreichbarkeitsmengen für die Präfixe von \( w = abbabaa \).
	\begin{align*}
		E(\mathcal{A}, \varepsilon) &= \{ q_0 \},\\
		E(\mathcal{A}, a) &= \{ q_0, q_1 \},\\
		E(\mathcal{A}, ab) &= \{ q_0, q_2 \},\\
		E(\mathcal{A}, abb) &= \{ q_0 \},\\
		E(\mathcal{A}, abba) &= \{ q_0, q_1 \},\\
		E(\mathcal{A}, abbab) &= \{ q_0, q_2 \},\\
		E(\mathcal{A}, abbaba) &= \{ q_0, q_1 \},\\
		E(\mathcal{A}, abbabaa) &= \{ q_0, q_1, q_2 \}.
	\end{align*}
	Wegen \( E(\mathcal{A}, w) \cap F \neq \emptyset \) wird \( w \) akzeptiert.
\end{example}


\subsection{Äquivalenz von NFAs und DFAs}\label{regular_equivalence}
Auf den ersten Blick wirkt es vermutlich so, dass NFAs ``mehr`` können als DFAs, da NFAs durch das stets richtige Raten der Transition einen Blick in die Zukunft werfen können. Dieser Abschnitt widmet sich jedoch der Äquivalenz von NFAs und DFAs, d.h. auch wenn wie in Beispiel~\ref{exp:ex2} NFAs mit weniger Zuständen die gleichen Sprachen erkennen können wie DFAs, so können auch DFAs jede NFA-erkennbare Sprache erkennen. Dies ist mit einer einfachen Überlegung auch gar nicht so unintuitiv: Die Erreichbarkeitsmenge für einen NFA und ein Wort ist stets endlich, da auch ein NFA lediglich endlich viele Zustände hat. In einer Simulation eines NFA in einem DFA könnte man also also einen Zustand durch die Menge der erreichbaren Zustände wählen und würde endlich bleiben. Details dazu folgen in diesem Abschnitt.
\begin{definition}\label{def:fa_equivalence}
	Seien \( \mathcal{A}, \mathcal{B} \) zwei endliche Automaten (deterministisch oder nicht-deterministisch). \( \mathcal{A} \) und \( \mathcal{B} \) heißen \textit{äquivalent}, wenn \( L(\mathcal{A}) = L(\mathcal{B}) \).
\end{definition}
Eine Bemerkung aus dem vorigen Abschnitt greifen wir nochmal im folgendem Lemma auf.
\begin{lemma}\label{lem:dfa2nfa}
	Zu jedem DFA exitiert ein äquivalenter NFA.
	\begin{proof}
		Sei \( \mathcal{A} = (Q, \Sigma, \delta, q_0, F) \) ein DFA. Wir definieren einen NFA
		\[
			\mathcal{A}^\prime = (Q, \Sigma, \Delta, q_0, F),
		\]
		mit \( \Delta = \{ (p, a, q) \mid \delta(p, a) = q \} \). Wir zeigen, dass \( \mathcal{A}^\prime \) und \( \mathcal{A} \) äquivalent sind. Dazu sei \( w = a_0 \ldots a_{n-1} \in L(\mathcal{A}) \). Also ist der eindeutige Lauf \( \varrho = (r_0, \ldots, r_n) \) akzeptierend auf \( \mathcal{A} \). Der Lauf ist nach Konstruktion auch ein Lauf auf \( \mathcal{A}^\prime \), also akzeptiert auch \( \mathcal{A}^\prime \). Rückrichtung analog.
	\end{proof}
\end{lemma}
Diese Richtung war relativ einfach mit den eingangs genannten Bemerkungen. Man könnte den Beweis auch aufwändig wieder über Induktion führen, dies ist aber nicht sinnvoll, da das Ergbenis recht klar ist.
\begin{lemma}\label{lem:nfa2dfa}
	Zu jedem NFA existiert ein äquivalenter DFA.
	\begin{proof}
		Sei \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) ein NFA. Wir konstruieren einen DFA, der äquivalent ist, durch die sogenannte \textit{Potenzmengenkonstruktion}:
		\[
			\mathcal{A}^\prime = (2^Q, \Sigma, \delta, \{q_0\}, \{ P \subseteq Q \mid P \cap F \neq \emptyset \})
		\]
		mit \( \delta(P, a) = \{ q \mid \text{es ex. } p \in P \text{ mit } (p, a, q) \in \Delta \} \).
		Wir zeigen, dass beide Automaten äquivalent sind durch die Behauptung, dass für alle \( w \in \Sigma^\ast \) und \( P \subseteq Q \) mit \( \mathcal{A}^\prime: \{q_0\} \reaches{w} P \) gilt, dass \( P = E(\mathcal{A}, w) \), d.h. der Zustand, den der Potenzmengenautomatauf dem Wort \( w \) erreicht, entspricht der Erreichbarkeitsmenge auf dem selben Wort für den NFA. Dies zeigen wir per Induktion über alle Wortlängen \( n \).\\
		Induktionsverankerung: \( n = 0 \). Dann ist \( w = \varepsilon \) und es gilt \( P = \{q_0\} = E(\mathcal{A}, \varepsilon) \) nach Lemma~\ref{lem:reach2}.\\
		Induktionsschritt: \( n \rightarrow {n+1} \). Dann ist \( w = ua \) für ein \( u \in \Sigma^n \) und \( a \in \Sigma \). Sei \( P^\prime \subseteq Q \), sodass \( \mathcal{A}^\prime: \{q_0\} \reaches{u} P^\prime \). Nach Induktionshypothese ist \( P^\prime = E(\mathcal{A}, u) \). Somit gilt
		\begin{align*}
			P &= \delta(P^\prime, a)\\
			&= \{q \in Q \mid \text{es ex. } p \in P^\prime \text{ mit } (p, a, q) \in \Delta\}\\
			&= \bigcup_{p \in P^\prime = E(\mathcal{A}, u)} \{q \in Q \mid (p, a, q) \in \Delta\}\\
			&= E(\mathcal{A}, w)
		\end{align*}
		nach Definition von \( \delta \) und Lemma~\ref{lem:reach2}. Insgesamt gilt also, dass \( w \in L(\mathcal{A}) \) {g.d.w.} \( E(\mathcal{A}, w) \cap F \neq \emptyset \) (Lemma~\ref{lem:reach1}). Nach der oben gezeigten Behauptung gilt dies {g.d.w.} \( P \cap F \neq \emptyset \) und nach Definition des Konstruktion ist \( P \) genau dann akzeptierend im Potenzmengenautomaten. Somit akzeptiert \( \mathcal{A}^\prime \) das Wort \( w \).
	\end{proof}
\end{lemma}
In der Potenzmengenkonstruktion haben wir als Zustandsmenge stets die Potenzmenge der Zustandsmenge des NFA genutzt. Offenbar ist es aber so, dass dann einige Zustände unerreichbar sind, diese können auch weggelassen werden.
\begin{definition}\label{def:reduced_automaton}
	Sei \( \mathcal{A} = (Q, \Sigma, \frac{\delta}{\Delta}, q_0, F) \) ein DFA bzw. NFA.
	\begin{enumerate}
		\item Ein Zustand \( q \in Q \) ist \textit{erreichbar}, wenn es ein \( w \in \Sigma^\ast \) gibt, sodass \( \mathcal{A}: q_0 \reaches{w} q \).
		\item Der \textit{reduzierte Automat} (auf die erreicharen Zustände) ist:
			\[
				\mathcal{A}^\prime = (Q^\prime, \Sigma, \frac{\delta^\prime}{\Delta^\prime}, q_0, F^\prime)
			\]
			wobei
			\begin{itemize}
				\item \( Q^\prime \coloneqq \{ q \in Q \mid q \text{ erreichbar} \} \),
				\item \( \delta^\prime \coloneqq \left. \delta \right|_{Q^\prime \times \Sigma} \) bzw. \( \Delta^\prime \coloneqq \Delta \cap (Q^\prime \times \Sigma \times Q^\prime) \),
				\item \( F^\prime \coloneqq F \cap Q^\prime\).
			\end{itemize}
	\end{enumerate}
\end{definition}
Wir verwenden die Potenzmengenkonstruktion aus Lemma~\ref{lem:nfa2dfa} zur Determinisierung eines NFA. Wir können dabei schrittweise vom Anfangszustand die Zustände und Transitionen konstruieren um so direkt auf einen reduzierten Automaten gemäß Definition~\ref{def:reduced_automaton} zu kommen. Dies ist in Algorithmus~\ref{alg:nfa2dfa} beschrieben.
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\underline{Potenzmengenkonstruktion}{$(\mathcal{A})$}\\
	\Input{NFA $\mathcal{A} = (Q, \Sigma, \Delta, q_0, F)$}
	\Output{äquivalenter, reduzierter DFA $\mathcal{A}^\prime = (Q^\prime, \Sigma, \delta, q_0^\prime, F^\prime)$}	
	$Q^\prime \coloneqq \{\{q_0\}\}$\\
	$q_0^\prime \coloneqq \{q_0\}$\\
	$F^\prime \coloneqq \emptyset$\\
	Queue $P \coloneqq Q^\prime$\\
	\While{$P \neq \emptyset$}{
		$S \coloneqq P$.dequeue()\\
		\For{$a \in \Sigma$}{
			$R \coloneqq \{\bigcup_{p \in S} \{ r \mid (p, a, r) \in \Delta \}$\\
			$P$.enqueue($R$)\\
			$Q^\prime \coloneqq Q^\prime \cup R$\\
			$\delta(S, a) = R$
		}
	}
	\For{$P \in Q^\prime$}{
		\uIf{$P \cap F \neq \emptyset$}{
			$F^\prime \coloneqq F^\prime \cup \{ P \}$
		}
	}
	
	\caption{NFA-Determinisierung}
	\label{alg:nfa2dfa}
\end{algorithm}
\begin{example}
	Wir betrachten den NFA in Abbildung~\ref{fig:powersetconstruction_nfa}. In Abbildung~\ref{fig:powersetconstruction_dfa} ist ein äquivalenter, reduzierter DFA, erhalten durch Potenzmengenkonstruktion.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/powersetconstruction_nfa}
		\caption{NFA}
		\label{fig:powersetconstruction_nfa}
	\end{subfigure}\\
	\ \\
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/powersetconstruction_dfa}
		\caption{Potenzmengenautomat}
		\label{fig:powersetconstruction_dfa}
	\end{subfigure}
	\caption{Beispiel zur Potenzmengenkonstruktion.}
	\label{fig:powersetconstruction}
\end{figure}
\begin{theorem}
	Eine Sprache ist genau dann DFA-erkennbar, wenn sie NFA-erkennbar ist.
	\begin{proof}
		Die Lemmata~\ref{lem:dfa2nfa} und~\ref{lem:nfa2dfa} zusammen geben den Beweis.
	\end{proof}
\end{theorem}
In diesem Sinne ist es sinnvoll von nun an nur noch von FA-erkennbaren Sprachen zu sprechen statt zwischen DFA- und NFA-erkennbaren Sprachen zu unterscheiden.


\subsection[NFAs mit $\varepsilon$-Transitionen]{NFAs mit $\bm{\varepsilon}$-Transitionen}
Mit Blick auf spätere Abschnitte erweitern wir das Modell der NFAs um die Möglickeit den Zustand zu wechseln ohne ein Symbol dabei zu lesen.
\begin{definition}
	Ein \textit{nicht-deterministischer endlicher Automat mit} \textit{\(\varepsilon\)-Transitionen (\(\varepsilon\)-NFA)} ist ein 5-Tupel
	\[
		(Q, \Sigma, \Delta, q_0, F),
	\]
	mit
	\begin{itemize}
		\item \( Q \) eine nicht-leere, endliche Menge von \textit{Zuständen},
		\item \( \Sigma \) ein nicht-leeres, endliches \textit{Eingabealphabet},
		\item \( \Delta \subseteq Q \times (\Sigma \cup \{\varepsilon\}) \times Q \) die \textit{Transitionsrelation},
		\item \( q_0 \in Q \) der \textit{Startzustand},
		\item \(F \subseteq Q \) die Menge der \textit{akzeptierenden Zustände} (oder \textit{Endzustände}).
	\end{itemize}
\end{definition}
Wir bezeichnen \(\varepsilon\)-NFAs wieder mit \( \mathcal{A}, \mathcal{B} \) usw. Transitionsgraphen lassen sich ebenfalls analog zeichnen wie für NFAs und DFAs, wobei \(\varepsilon\)-Transitionen durch Kanten gekennzeichnet werden, die mit \( \varepsilon \) beschriftet sind. Der Voll\-stän\-dig\-keit halber definieren wir auch wieder Läufe auf \(\varepsilon\)-NFAs und das Akzeptanzverhalten.
\begin{definition}
	Sei \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) ein \(\varepsilon\)-NFA.
	Ein \textit{Lauf} von \( \mathcal{A} \) auf einem Wort \( w = a_0 \ldots a_{n-1} \) für ein \( n \in \mathbb{N} \) ist eine endliche Folge
	\[
		(r_0, \sigma_0, r_1, \sigma_1, \ldots, sigma_{m-1}, r_m),
	\]
	wobei \( r_0, \ldots, r_m \in Q \) und \( \sigma_0, \ldots, \sigma_{m-1} \in \Sigma \cup \{\varepsilon\} \), sodass
	\begin{enumerate}
		\item \( r_0 = q_0 \),
		\item Für alle \( i \in [m] \) gilt, dass \( (r_i, \sigma_i, r_{i+1}) \in \Delta \).
	\end{enumerate}
	Wir sagen ein Lauf ist \textit{akzeptierend}, wenn zusätzlich \( r_n \in F \) gilt.
\end{definition}
\begin{definition}
\
	\begin{enumerate}
		\item Ein \(\varepsilon\)-NFA \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) \textit{akzeptiert} ein Wort \( w \in \Sigma^\ast \), wenn es mindestens einen akzeptierenden Lauf von \( \mathcal{A} \) gibt. Andernfalls \textit{verwirft} \( \mathcal{A} \) das Wort \( w \).
		\item Die von einem \(\varepsilon\)-NFA \( \mathcal{A} = (Q, \Sigma, \delta, q_0, F) \) \textit{erkannte Sprache} ist
			\[
				L(\mathcal{A}) \coloneqq \{ w \in \Sigma^\ast \mid \mathcal{A} \text{ akzeptiert } w \}.
			\]
		\item Eine Sprache \( L \) heißt \textit{\(\varepsilon\)-NFA-erkennbar}, wenn es einen \(\varepsilon\)-NFA \( \mathcal{A} \) gibt, sodass \( L = L(\mathcal{A}) \).
	\end{enumerate}
\end{definition}
\begin{example}\label{exp:ex3}
	Wir betrachten die Sprache \( L = \{ a^n b^m : n, m \in \mathbb{N} \} \cup \{ a^n c^m : n, m \in \mathbb{N} \} \). Der \(\varepsilon\)-NFA in Abbildung~\ref{fig:epsnfa_ex1} erkennt die Sprache \( L \) mit drei Zuständen.
\end{example}
\begin{figure}
	\centering
	\input{figs/epsnfa_ex1}
	\caption{\(\varepsilon\)-NFA, der die Sprache aus Beispiel~\ref{exp:ex3} erkennt.}
	\label{fig:epsnfa_ex1}
\end{figure}
Für \(\varepsilon\)-NFAs zeigen wir wie im vorigen Abschnitt, dass auch sie die Klasse der FA-erkennbaren Sprachen nicht vergrößern. Der Äquivalenzbegriff aus Definition~\ref{def:fa_equivalence} überträgt sich natürlich auf \(\varepsilon\)-NFAs.
\begin{lemma}
	Zu jedem NFA existiert ein äquivalenter \(\varepsilon\)-NFA.
	\begin{proof}
		Dies ist sofort klar, da jeder NFA als ein \(\varepsilon\)-NFA betrachtet werden kann, der keine \(\varepsilon\)-Transitionen besitzt.
	\end{proof}
\end{lemma}
Bevor wir den anderen Teil der Äquivalenz zeigen fehlt uns eine Definition.
\begin{definition}\label{def:eps_closure}
	Sei \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) ein \(\varepsilon\)-NFA. Der \textit{\(\varepsilon\)-Abschluss} (auch \(\varepsilon\)-Hülle) eines Zustands \( p \in Q \) ist
	\[
		\comp{\varepsilon}(p) \coloneqq \{ q \in Q \mid \text{es ex. } p_1, \ldots, p_k \text{ mit } (p, \varepsilon, p_1), (p_i, \varepsilon, p_{i+1}), (p_k, \varepsilon, q) \in \Delta \}.
	\]
	Für Mengen \( P \subseteq Q \) ist der \textit{\(\varepsilon\)-Abschluss} dann erweitert durch
	\[
		\comp{\varepsilon}(P) \coloneqq \bigcup_{p \in P} \comp{\varepsilon}(p).
	\]
\end{definition}
Auf diese Weise lässt sich die Erreichbarkeitsrelation, die wir schon aus Definition~\ref{def:reachability} kennen erweitern.
\begin{definition}\label{def:reachability2}
	Sei \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) ein NFA und \( w = a_0 \ldots a_{n-1} \). Ein Zustand \( q \in Q \) heißt \textit{erreichbar} von \( p \) über \( w \) (geschrieben \( \mathcal{A}: p \reaches{w} q \)), wenn es ein \( m \geq n \), Indizes \( 0 \leq i_0 < \ldots < i_{n-1} \leq m \) und Zustände \( r_0, \ldots, r_m \in Q \) gibt, sodass
	\begin{itemize}
		\item \( r_0 = p, r_m = q \),
		\item \( (r_{i_j}, a_j, r_{i_j+1}) \in \Delta \) für alle \( j \in [n] \) und
		\item \( (r_i, \varepsilon, r_{i+1}) \in \Delta \) für alle \( i \in [m] \setminus \{i_1, \ldots, i_{n-1}\} \).
	\end{itemize}
\end{definition}
Man beachte, dass \( p \reaches{\varepsilon} q \) nicht \( p = q \) bedeutet wie bei NFAs.
\begin{lemma}\label{lem:epsnfa2nfa}
	Zu jedem \(\varepsilon\)-NFA existiert ein äquivalenter NFA.
	\begin{proof}
		Wir betrachten einen \(\varepsilon\)-NFA \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) und überführen ihn in einen äquivalenten NFA \( \mathcal{A}^\prime = (Q, \Sigma, \Delta^\prime, q_0, F^\prime) \). Dabei ist
		\[
			\Delta^\prime \coloneqq \{(p, a, q) \in Q \times \Sigma \times Q \mid \mathcal{A}: p \reaches{a} q\}
		\]
		und \( F^\prime \coloneqq \{q \in Q \mid q \in \comp{\varepsilon}(F)\} \).
		Die Korrektheit ist einfach einzusehen. Sei \( w = a_0 \ldots a_{n-1} \in L(\mathcal{A}) \). Dann gibt es ein \( q \in F \) mit \( q_0 \reaches{w} q \). Also gibt es Zustände \( r_0 = q_0, \ldots, r_m = q \) wie in Definition~\ref{def:reachability2}. Nach Konstruktion von \( \Delta^\prime \) gibt es somit Zustände \( r_{i_0} = q_0, \ldots, r_{i_n} = q \), sodass \( r_{i_j} \reaches{a_j} r_{i_{j+1}} \) für alle \( j \in [n] \). Damit ist \( w \in L(\mathcal{A}^\prime) \). Rückrichtung analog.
	\end{proof}
\end{lemma}
\begin{example}
	Wir betrachten den \(\varepsilon\)-NFA in Abbildung~\ref{fig:epsnfa_ex2_eps}. In Abbildung~\ref{fig:epsnfa_ex2_noeps} sehen wir den NFA den wir nach Lemma~\ref{lem:epsnfa2nfa} erhalten, wobei die neu eingefügten Transitionen rot markiert sind. Beachte auch, dass sich die Menge der akzeptierenden Zustände geändert hat, da sonst das leere Wort nicht mehr akzeptiert werden würde.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/epsnfa_ex2_eps}
		\caption{mit \(\varepsilon\)-Transitionen}
		\label{fig:epsnfa_ex2_eps}
	\end{subfigure}~
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\input{figs/epsnfa_ex2_noeps}
		\caption{ohne \(\varepsilon\)-Transitionen}
		\label{fig:epsnfa_ex2_noeps}
	\end{subfigure}
	\caption{Eliminieren von \(\varepsilon\)-Transitionen}
	\label{fig:epsnfa_ex2}
\end{figure}
\begin{remark*}
	Beachte, dass ein Zusammenziehen der Zustände, die durch \(\varepsilon\)-Transitionen verbunden sind nicht funktioniert, da das im Allgemeinen die erkannte Sprache verändert. Ein solches Vorgehen würde zum Beispiel dazu führen, dass der entstehende Automat zu Beispiel~\ref{fig:epsnfa_ex1} nur noch einen Zustand besitzt mit einem durch \( a, b, c \) beschrifteten Loop. Die erkannte Sprache wäre dann \( \{a, b, c\}^\ast \).
\end{remark*}
\begin{corollary}
	Zu jedem \(\varepsilon\)-NFA existiert ein äquivalenter DFA.
	\begin{proof}
		Lemma~\ref{lem:epsnfa2nfa} liefert zunächst einen NFA, der mit Lemma~\ref{lem:nfa2dfa} in einen DFA umgewandelt werden kann.
	\end{proof}
\end{corollary}
Wir werden also auch weiterhin von FA-erkennbaren Sprachen sprechen, solange sie von einem DFA, NFA oder \(\varepsilon\)-NFA erkannt werden, da alle drei Modelle gleich mächtig sind. In mancher Fachliteratur wird wegen ihrer Äqui\-va\-lenz auch zwischen NFAs und \(\varepsilon\)-NFAs gar nicht unterschieden -- wir werden im Folgenden die Unterscheidung dennoch weiterhin machen.


\subsection{Reguläre Ausdrücke}\label{sec:regular_regexp}
Bisher haben wir Sprachen betrachtet, die sich von endlichen Automaten erkennen lassen. Diese Art von Sprachen liefern uns eine eigene Klasse von Sprachen, die unter verschiedenen Operationen abgeschlossen ist. Auch wenn bisher noch nicht alles gezeigt wurde (Konkatenation, Kleene'sche Hülle) lässt sich mit einiger Berechtigung sagen, dass die Klasse der FA-erkennbaren Sprachen gute Eigenschaften hat. Wir untersuchen nun welche Sprache wir mit sogenannten \textit{regulären Ausdrücken} beschreiben können.
\begin{definition}\label{def:regexsyntax}
	Sei \( \Sigma \) ein endliches Alphabet. Ein \textit{regulärer Ausdruck} ist induktiv definiert mit:
	\begin{itemize}
		\item \( \bm{\emptyset} \) ist ein regulärer Ausdruck.
		\item Für jedes \( a \in \Sigma \) ist \( \bm{a} \) ein regulärer Ausdruck.
		\item Falls \( r, r^\prime \) reguläre Ausdrücke sind, dann ist auch \( (r \bm{+} r^\prime) \) ein regulärer Ausdruck.
		\item Falls \( r, r^\prime \) reguläre Ausdrücke sind, dann ist auch \( (r \bm{\cdot} r^\prime) \) ein regulärer Ausdruck.
		\item Falls \( r \) reguläre Ausdrücke sind, dann ist auch \( (r) \overset{\bm{\ast}}{} \) ein regulärer Ausdruck.
	\end{itemize}
	Die \textit{Menge aller regulären Ausdrücke} über \( \Sigma \) bezeichnen mit \( \mathsf{RE}_\Sigma \).
\end{definition}
Das ist bisher lediglich die Syntax der regulären Ausdrücke gewesen. Nun definieren wir eine Sematik für diese Ausdrücke, d.h. wir ordnen jedem regulären Ausdruck eine Sprache zu.
\begin{definition}
	Sei \( \Sigma \) ein endliches Alphabet. Die \textit{Interpretation eines regulären Ausdrucks} ist die Abbildung
	\[
		\llbracket \cdot \rrbracket \colon \mathsf{RE}_\Sigma \to 2^{\Sigma^\ast}
	\]
	mit
	\begin{itemize}
		\item \( \llbracket \bm{\emptyset} \rrbracket = \emptyset \),
		\item \( \llbracket \bm{a} \rrbracket = \{ a \} \) für jedes \( \bm{a} \in \mathsf{RE}_\Sigma \),
		\item \( \llbracket (r \bm{+} r^\prime) \rrbracket = \llbracket r \rrbracket \cup \llbracket r^\prime \rrbracket \),
		\item \( \llbracket (r \bm{\cdot} r^\prime) \rrbracket = \llbracket r \rrbracket \cdot \llbracket r^\prime \rrbracket \),
		\item \( \llbracket (r) \overset{\bm{\ast}}{} \rrbracket = \llbracket r \rrbracket^\ast \).
	\end{itemize}
	Eine Sprache \( L \subseteq \Sigma^\ast \) heißt \textit{regulär}, wenn ein regulärer Ausdruck \( r \in \mathsf{RE}_\Sigma \) existiert mit \( \llbracket r \rrbracket = L \).
\end{definition}
Wir erlauben in der Regel auch als Abkürzung die Ausdrücke \( \bm{\varepsilon} \) für \( (\bm{\emptyset}) \overset{\bm{\ast}}{} \) und \( (r) \overset{\bm{+}}{} \) für \( r \bm{\cdot} (r) \overset{\bm{\ast}}{} \). Außerdem lassen wir den Punkt (\( \bm{\cdot} \)) weg, es sei denn er dient der Lesbarkeit. Die Klammern können wir auch weglassen unter der Konvention, dass \( \overset{\bm{\ast}}{} \) stärker bindet als \( \bm{\cdot} \), was wiederum stärker bindet als \( \bm{+} \). Statt \( \llbracket r \rrbracket \) ist auch die Schreibweise \( L(r) \) gebräuchlich.\par
Reguläre Ausdrücke kennt man auch für Computerprogramme wie \texttt{grep}, \texttt{awk} oder \texttt{sed}. Diese sind syntaktisch etwas anders aufgebaut, jedoch lässt sich jeder POSIX-Ausdruck (das sind die regulären Ausdrücke für oben genannte Programme) auch als regulärer Ausdruck wie in Definition~\ref{def:regexsyntax} umschreiben.
\begin{example}
	Die POSIX-Syntax für reguläre Ausdrücke geht wie folgt.
	\begin{itemize}
		\item \( r \)\verb$|$\( r^\prime \) für \( r \bm{+} r^\prime \),
		\item Für \( (\bm{a + b +} \ldots \bm{+ z}) \) geht auch \verb$[a-z]$\\
			(analog: \verb$[A-Z]$,  \verb$[0-9]$, \verb$[a-z0-9,\.:;\?!]$ usw.),
		\item \verb$.$ für ein beliebiges Zeichen,
		\item \( r \)\verb$?$ für \( r \bm{+ \varepsilon} \),
		\item \( r \)\verb$+$ und \( r \)\verb$*$ für \( r\overset{\bm{+}}{}, r\overset{\bm{\ast}}{} \),
		\item \( r \)\verb${m,n}$ für \( r^m \bm{+} r^{m+1} \bm{+} \ldots \bm{+} r^n \).
	\end{itemize}
	Wir können zum Beispiel die Sprache aller E-Mail-Adressen als Ausdruck
	\[
		\verb$[a-zA-Z0-9\.-]+@[a-zA-Z0-9\.-]+\.(de|com|net)$
	\]
	darstellen.
\end{example}
Für unsere Analyse von regulären Sprachen benutzen wir nur reguläre Ausdrücke der Form wie in Definition~\ref{def:regexsyntax} vorgestellt. Dadurch lassen sich viele Beweise über den minimalistischen induktiven Aufbau der Ausdrücke führen.
\begin{example}
	Wir betrachten das Alphabet \( \Sigma = \{a, b\} \).
	\begin{itemize}
		\item \( L(\bm{((a+b)(a+b))} \overset{\bm{\ast}}{}) = \{ w \in \Sigma^\ast \mid \left| w \right| \text{ gerade} \} \).
		\item \( L(\bm{(a+b)(a+b)(a+b)} \overset{\bm{\ast}}{}) = \{ w \in \Sigma^\ast \mid \left| w \right| \geq 2 \} \).
		\item \( L(\bm{a} \overset{\bm{\ast}}{} \bm{+ b} \overset{\bm{\ast}}{}) = \{ w \in \Sigma^\ast \mid \left| w \right|_a = 0 \text{ oder } \left| w \right|_b = 0 \} \).
	\end{itemize}
\end{example}
Aus Gründen der Einfachheit werden wir von nun an bei der Notation von regulären Ausdrücken auf eine Unterscheidung zu Alphabetsymbolen, Kleene-Stern usw. verzichten, d.h. wir verwenden \( \emptyset, \varepsilon, a, +, \cdot, ^\ast \) statt \( \bm{\emptyset}, \bm{\varepsilon}, \bm{a}, \bm{+}, \bm{\cdot}, \overset{\bm{\ast}}{} \). Außerdem verwenden wir Shortcuts, z.B. \( \sum_{a\in\Sigma} a = \Sigma^\ast \). Formal ist aber wichtig weiterhin eine Unterscheidung zwischen Sprachen und regulären Ausdrücken zu haben.
\begin{definition}
	Zwei reguläre Ausdrücke \( r, e \in \mathsf{RE}_\Sigma \) heißen \textit{äquivalent} (geschrieben \( r \equiv e \)), wenn \( \llbracket r \rrbracket = \llbracket e \rrbracket \).\\
	Ein regulärer Ausdruck \( r \in \mathsf{RE}_\Sigma \) und ein endlicher Automat \( \mathcal{A} \) (DFA, NFA) heißen \textit{äquivalent}, wenn  \( \llbracket r \rrbracket = L(\mathcal{A}) \).
\end{definition}
\begin{example}
	Die regulären Ausdrücke \( (a + b)^\ast \) und \( (a^\ast b^\ast)^\ast \) sind äquivalent.
\end{example}
Wir kommen nun zu einem der wichtigsten Ergebnisse der Automatentheorie und dieser Vorlesung.
\begin{theorem}[Äquivalenzsatz von Kleene]\label{thm:kleene_equivalence}
	Eine Sprache ist genau dann regulär, wenn sie FA-erkennbar ist.
\end{theorem}
Aus Gründen der Übersichtlichkeit werden wir den Beweis des Satzes aufteilen auf zwei Lemmata.
\begin{lemma}\label{lem:regex2nfa}
	Jede reguläre Sprache ist FA-erkennbar.
	\begin{proof}
		Wir stellen im Rahmen dieses Beweises eine Variante der sogenannten Thompson-Konstruktion vor, die einen regulären Ausdruck in einen äquivalenten \( \varepsilon \)-NFA umwandelt. Die Konstruktion nutzt den induktiven Aufbau von regulären Ausdrücken aus.\\
		Basisfälle (s. Abbildung~\ref{fig:thompson_basic}): 
		Wir geben drei \(\varepsilon\)-NFAs für die drei Basisfälle für reguläre Ausdrücke: 
		\begin{align*}
			\mathcal{A}_\emptyset &= (\{q_0\}, \Sigma, \emptyset, q_0, \emptyset),\\
			\mathcal{A}_\varepsilon &= (\{q_0\}, \Sigma, \emptyset, q_0, \{q_0\}),\\
			\mathcal{A}_a &= (\{q_0, q_1\}, \Sigma, \{(q_0, a, q_1)\}, q_0, \{q_1\}).
		\end{align*}
		\begin{figure}
			\centering
			\begin{subfigure}[b]{.25\textwidth}
				\centering
				\input{figs/thompson_empty}
				\caption{\( r = \emptyset \)}
				\label{fig:thompson_empty}
			\end{subfigure}~
			\begin{subfigure}[b]{.25\textwidth}
				\centering
				\input{figs/thompson_eps}
				\caption{\( r = \varepsilon \)}
				\label{fig:thompson_eps}
			\end{subfigure}~
			\begin{subfigure}[b]{.4\textwidth}
				\centering
				\input{figs/thompson_symbol}
				\caption{\( r = a \)}
				\label{fig:thompson_symbol}
			\end{subfigure}
			\caption{Basisfälle der Thompson-Konstruktion}
			\label{fig:thompson_basic}
		\end{figure}\\
		Rekursive Fälle (s. Abbildung~\ref{fig:thompson_recursive}):
		Seien für \( e, e^\prime \in \mathsf{RE}_\Sigma \) die \(\varepsilon\)-NFAs \( \mathcal{A}_e = (Q, \Sigma, \Delta, q_0, F) \) und \( \mathcal{A}_{e^\prime} = (Q^\prime, \Sigma, \Delta^\prime, q_0^\prime, F^\prime) \) gegeben mit \( q_{-1} \notin Q \cup Q^\prime \) und \( Q \cap Q^\prime = \emptyset \). Dann erhalten wir \(\varepsilon\)-NFAs für die Ausdrücke \( e+e^\prime, e \cdot e^\prime\) und \( e^\ast \):
		\begin{align*}
			\mathcal{A}_{e+e^\prime} &= (Q \cup Q^\prime \cup \{q_{-1}\}, \Sigma, \Delta \cup \Delta^\prime \cup \{(q_{-1}, \varepsilon, q_0), (q_{-1}, \varepsilon, q_0^\prime)\}, q_{-1}, F \cup F^\prime),\\
			\mathcal{A}_{e \cdot e^\prime} &= (Q \cup Q^\prime, \Sigma, \Delta \cup \Delta^\prime \cup \{(q, \varepsilon, q_0^\prime) \mid q \in F \}, q_0, F^\prime),\\
			\mathcal{A}_{e^\ast} &= (Q \cup \{q_{-1}\}, \Sigma, \Delta \cup \{(q_{-1}, \varepsilon, q_0)\} \cup \{(q, \varepsilon, q_{-1}) \mid q \in F \}, q_{-1}, \{q_{-1}\}).
		\end{align*}
		
		\begin{figure}
			\centering
			\begin{subfigure}[b]{.9\textwidth}
				\centering
				\input{figs/thompson_plus}
				\caption{\( r = e + e^\prime \)}
				\label{fig:thompson_plus}
			\end{subfigure}\\
			\begin{subfigure}[b]{.9\textwidth}
				\centering
				\input{figs/thompson_cat}
				\caption{\( r = e \cdot e^\prime \)}
				\label{fig:thompson_cat}
			\end{subfigure}\\
			\begin{subfigure}[b]{.9\textwidth}
				\centering
				\input{figs/thompson_star}
				\caption{\( r = e^\ast \)}
				\label{fig:thompson_star}
			\end{subfigure}
			\caption{Rekursive Fälle der Thompson-Konstruktion}
			\label{fig:thompson_recursive}
		\end{figure}
		Wir zeigen nun noch die Korrektheit der Konstruktion. Dies geschieht per Induktion über den Aufbau des regulären Ausdrucks.\\
		Induktionsverankerung: \( e \in \{\emptyset, \varepsilon, a\} \). Der Automat \( \mathcal{A}_e \) wie in Abbildung~\ref{fig:thompson_basic} erkennt offensichtlich \( \llbracket e \rrbracket \).\checkmark\\
		Induktionshypothese: Für die regulären Ausdrücke \( e, e^\prime \in \mathsf{RE}_\Sigma \) erkennen die Automaten \( \mathcal{A}_e = (Q, \Sigma, \Delta, q_0, F) \) und \( \mathcal{A}_{e^\prime} = (Q^\prime, \Sigma, \Delta^\prime, q_0^\prime, F^\prime) \) die Sprachen \( \llbracket e \rrbracket \) bzw. \( \llbracket e^\prime \rrbracket \).\\
		Induktionsschritt: Wir zeigen nur den Fall für \( e + e^\prime \), die anderen beiden gehen analog.
		Sei \( w \in \llbracket e + e^\prime \rrbracket = \llbracket e \rrbracket \cup \llbracket e^\prime \rrbracket \). OBdA sei \( w \in \llbracket e \rrbracket \). Demnach existiert ein Lauf \( (r_0, \ldots, r_n) \) mit \( r_n \in F \). Dann ist \( (q_{-1}, r_0, \ldots, r_n) \) akzeptierender Lauf auf \( \mathcal{A}_{e+e^\prime} \) und damit \( w \in L(\mathcal{A}_{e+e^\prime}) \).\\
		Sei umgekehrt \( w \in L(\mathcal{A}_{e+e^\prime}) \). Das heißt es gibt einen Lauf \( (q_{-1}, r_0, \ldots, r_n) \) mit \( r_n \in F \cup F^\prime \). Da \( q_{-1} \) nur zwei \(\varepsilon\)-Transitionen hat muss \( (r_0, \ldots, r_n) \) ebenfalls ein akzeptierender Lauf sein mit Startzustand \( r_0 \). Nach Konstruktion ist \( r_0 \) Startzustand von oBdA \( \mathcal{A}_e \) und da \( Q \cap Q^\prime = \emptyset \) muss \( r_n \in F \) sein. Damit ist \( w \in \llbracket e \rrbracket \subseteq \llbracket e+e^\prime \rrbracket \).
	\end{proof}
\end{lemma}
\begin{remark*}
	Wir sehen, dass die Thompson-Konstruktion einige \(\varepsilon\)-Tran\-si\-tio\-nen verwendet und auch gelegentlich den Zustandsraum unnötig vergrößert. Abseits von einigen Optimierungen dazu gibt es auch Verfahren, die ganz ohne \(\varepsilon\)-Transitionen auskommt (s. Glushkov-Konstruktion). In dieser Vorlesung wird diese aber nicht betrachten.
\end{remark*}
\begin{remark*}
	Der ursprüngliche Beweis von Lemma~\ref{lem:regex2nfa} von Stephen {C.} Kleene ging übrigens von einem regulären Ausdruck zu einem DFA, das Konzept ''Nichtdeterminismus`` wurde erst später eingeführt. Man kann sich vorstellen, dass der Beweis um einiges schwieriger war als diese recht anschauliche Konstruktion, u.a. waren auch algebraische Konzepte und Halbgruppentheorie involviert.
\end{remark*}
Ein Nebenprodukt der Thompson-Konstruktion aus dem Beweis ist folgendes Korollar.
\begin{corollary}
	Seien \( L, K \subseteq \Sigma^\ast \) FA-erkennbare Sprachen. Dann sind auch \( L \cdot K \) und \( L^\ast \) FA-erkennbar.
\end{corollary}
\begin{example}
	Wir betrachten den regulären Ausdruck \( r = (a+b)^\ast c \) und bauen nun einen \(\varepsilon\)-NFA für \( r \). Die Automaten für die atomaren Teilausdrücke \( a, b, c \) sind klar. In Abbildungen~\ref{fig:thompson_example_1} bis~\ref{fig:thompson_example_3} gehen wir Schritt für Schritt die Konstruktion durch. Man sieht wie der Automat aus dem vorigen Schritt in den Automaten aus dem aktuellen Schritt eingebettet ist. In Abbildung~\ref{fig:thompson_example_4} finden wir außerdem einen zustandsminimalen NFA, der sich recht schnell nur durch Ablesen des Ausdrucks hinschreiben lässt. Dies zeigt, dass auch bei einem solchen Minimalbeispiel bereits viele unnötige Zustände und Transitionen eingefügt werden in der Thompson-Konstruktion. Der Vorteil ist aber natürlich, dass ein Algorithmus immer funktioniert.
\end{example}
\begin{figure}
	\centering
	\begin{subfigure}{.99\textwidth}
		\centering
		\input{figs/thompson_example_1}
		\caption{Automat für \( (a+b) \).}
		\label{fig:thompson_example_1}
	\end{subfigure}\\
	\begin{subfigure}{.99\textwidth}
		\centering
		\input{figs/thompson_example_2}
		\caption{Automat für \( (a+b)^\ast \).}
		\label{fig:thompson_example_2}
	\end{subfigure}\\
	\begin{subfigure}{.99\textwidth}
		\centering
		\input{figs/thompson_example_3}
		\caption{Automat für \( (a+b)^\ast c \).}
		\label{fig:thompson_example_3}
	\end{subfigure}\\
	\begin{subfigure}{.99\textwidth}
		\centering
		\input{figs/thompson_example_4}
		\caption{Minimaler NFA für \( (a+b)^\ast c \).}
		\label{fig:thompson_example_4}
	\end{subfigure}
	\caption{Thompson-Konstruktion für \( r = (a+b)^\ast c \).}
	\label{fig:thompson_example}
\end{figure}

Wir fahren nun fort mit dem zweiten Teil von Satz~\ref{thm:kleene_equivalence}. Wir schauen uns hier ein Verfahren an, was aus jedem endlichen Automaten einen regulären Ausdruck macht. Auch dazu gibt es verschiedene Varianten, u.a. auch grafische, die grundlegende Idee hinter diesen Verfahren ist jedoch meist die selbe.
\begin{lemma}\label{lem:nfa2regex}
	Jede FA-erkennbare Sprache ist regulär.
	\begin{proof}
		Wir gehen von einem NFA \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) aus und erzeugen einen äquivalenten regulären Ausdruck \( r_\mathcal{A} \in \mathsf{RE}_\Sigma \). Die Idee des Verfahrens ist recht einfach. Wir suchen einen regulären Ausdruck der alle Wörter beschreibt mit denen man vom Startzustand \( q_0 \) zu einem der akzeptierenden Zustände \( q \in F \) kommt (\(\ast\)). Wir benutzen nun folgende Notation: Für \( P \subseteq Q \) und \( p, q \in Q \) ist \( r_P(p, q) \) der reguläre Ausdruck, der alle Wörter \( w \in \Sigma^\ast \) beschreibt mit den man von \( p \) nach \( q \) kommt und dazwischen nur Zustände aus \( P \) benutzt (für die Korrektheit später schreiben wir dafür \( p \reachess{w}{P} q \)). Anders gesagt ist \( r_P(p, q) \) der reguläre Ausdruck zum Automaten \( \mathcal{A} = (P, \Sigma, \Delta \cap ((P \cup \{p\}) \times \Sigma \times (P \cup \{q\})), p, \{q\}) \). Bzgl. (\(\ast\)) suchen wir also den regulären Ausdruck
		\[
			r_\mathcal{A} = \sum_{q \in F} r_Q(q_0, q).
		\]
		Das Verfahren lässt sich rekursiv beschreiben nun mit folgender Überlegung: Ein regulärer Ausdruck \( r_\emptyset(p, q) \) ist \( \sum_{(p, a, q) \in \Delta} a \), also alle Symbole \( a \in \Sigma \) mit denen eine direkte Transition von \( p \) nach \( q \) möglich ist (oder noch anschaulicher: Alle Symbole die im Transitionsgraphen auf der Kante von \( p \) nach \( q \) stehen). Sollte keine Transition existieren, dann ist der reguläre Ausdruck entsprechend \( \emptyset \) für \( p \neq q \) und \( \varepsilon \) für \( p = q \). Einmal formal aufgeschrieben:
		\[
			r_\emptyset(p, q) = \left\lbrace
				\begin{array}{*3{>{\displaystyle}l}p{5cm}}
					\ &\sum_{(p, a, q) \in \Delta} a, & p \neq q\\
					\varepsilon + &\sum_{(p, a, q) \in \Delta} a, & p = q.
				\end{array}
			\right.
		\]
		Der Rekursionsschritt funktioniert nun folgendermaßen: Für ein \( P \neq \emptyset \) wählen wir einen Zustand  \( s \in P \) und entfernen diesen mit folgender Umformung
		\[
			r_P(p, q) = r_{P^\prime}(p, q) + r_{P^\prime}(p, s) r_{P^\prime}(s, s)^\ast r_{P^\prime}(s, q),
		\]
wobei \( P^\prime \coloneqq P \setminus \{ s \} \). Auch dies einmal in einfache Worte gefasst: Um von \( p \) nach \( q \) zu kommen über Zustände ausschließlich aus \( P \) können wir entweder (links vom +) von \( p \) nach \( q \) gehen ohne einen Zustand \( s \) zu besuchen oder (rechts vom +) von \( p \) nach \( s \) gehen, Pfade von \( s \) nach \( s \) benutzen und schließlich von \( s \) nach \( q \) gehen, wobei auch hier nur Zustände in \( P \setminus \{ s \} \) benutzt werden (s. Abbildung~\ref{fig:state_elimination}).
		\begin{figure}
			\centering
			\input{figs/state_elimination}
			\caption{Rekursionsschritt um einen Zustand rauszuwerfen. Die blau getönte Fläche ist \( P^\prime \coloneqq P \setminus \{ s \} \). Die größere Fläche ist \( P \). Wichtig: Die Kanten in diesem Graphen sind keine Transitionen, sondern sind mit einem regulären Ausdruck beschriftet, der die Sprache zwischen den Knoten beschreibt mit Zuständen aus dem Index!}
			\label{fig:state_elimination}
		\end{figure}
		%TODO Soundness Completeness
	\end{proof}
\end{lemma}
\begin{remark*}
	Das Verfahren könnte einem auch aus Algorithmik-Vor\-le\-sung\-en bekannt vorkommen. Es handelt sich dabei um eine (zugegeben umständlich aufgeschriebene) Variante des Floyd-Warshall-Algorithmus mit dem sich zum Beispiel auch kürzeste Wege in gerichteten Graphen finden lassen.
\end{remark*}
\begin{example}\label{exp:fw_example}
	Wir betrachten den NFA in Abbildung~\ref{fig:fw_example}. Wir suchen also einen regulären Ausdruck, der die Worte beschreibt mit denen man von \( q_0 \) nach \( q_0 \) (\( q_0 \) ist einziger akzeptierender Zustand) kommt mit allen Zuständen, also den Ausdruck \( r_Q(q_0, q_0) \). Wir wählen zunächst \( s = q_1 \) (eliminieren \( q_1 \)):
	\[
		r_\mathcal{A} = r_Q(q_0, q_0) = r_{\{ q_0 \}}(q_0, q_0) + r_{\{ q_0 \}}(q_0, q_1) r_{\{ q_0 \}}(q_1, q_1)^\ast r_{\{ q_0 \}}(q_1, q_0)
	\]
	Noch zu berechnen sind \( r_{\{ q_0 \}}(q_0, q_0), r_{\{ q_0 \}}(q_0, q_1), r_{\{ q_0 \}}(q_1, q_1), r_{\{ q_0 \}}(q_1, q_0) \). Wir berechnen \( r_{\{ q_0 \}}(q_0, q_0) \) und eliminieren jetzt auch \( q_0 \) und können dann direkt den Basisfall einsetzen. Dabei vereinfachen wir die regulären Ausdrücke noch:
	\begin{align*}
		r_{\{ q_0 \}}(q_0, q_0) &= r_\emptyset(q_0, q_0) + r_\emptyset(q_0, q_0) r_\emptyset(q_0, q_0)^\ast r_\emptyset(q_0, q_0)\\
		&= \varepsilon + \varepsilon \varepsilon^\ast \varepsilon\\
		&\equiv \varepsilon.
	\end{align*}
	Wir berechnen nun \( r_{\{ q_0 \}}(q_0, q_1) \) und eliminieren wieder \( q_0 \):
	\begin{align*}
		r_{\{ q_0 \}}(q_0, q_1) &= r_\emptyset(q_0, q_1) + r_\emptyset(q_0, q_0) r_\emptyset(q_0, q_0)^\ast r_\emptyset(q_0, q_1)\\
		&= c + \varepsilon \varepsilon^\ast c\\
		&\equiv c.
	\end{align*}
	Jetzt berechnen wir: \( r_{\{ q_0 \}}(q_1, q_1) \) und eliminieren wieder \( q_0 \):
	\begin{align*}
		r_{\{ q_0 \}}(q_1, q_1) &= r_\emptyset(q_1, q_1) + r_\emptyset(q_1, q_0) r_\emptyset(q_0, q_0)^\ast r_\emptyset(q_0, q_1)\\
		&= (a + b + \varepsilon) + a \varepsilon^\ast c\\
		&\equiv a + b + \varepsilon + ac.
	\end{align*}
	Zuletzt berechnen wir \( r_{\{ q_0 \}}(q_1, q_0) \) und eliminieren wieder den einzigen verbleibenden Zustand \( q_0 \):
	\begin{align*}
		r_{\{ q_0 \}}(q_1, q_0) &= r_\emptyset(q_1, q_0) + r_\emptyset(q_1, q_0) r_\emptyset(q_0, q_0)^\ast r_\emptyset(q_0, q_0)\\
		&= a + a \varepsilon^\ast \varepsilon\\
		&\equiv a.
	\end{align*}
	Diese Ausdrücke können wir nun rückwärts wieder einsetzen:
	\begin{align*}
		r_\mathcal{A} = r_Q(q_0, q_0) &= \varepsilon + c (a + b + \varepsilon + ac)^\ast  a\\
		&\equiv \varepsilon + c(a+b+ac)^\ast a.
	\end{align*}
	Da der Automat recht einfach ist, lässt sich hier natürlich auch ein einfacher Ausdruck einfach ablesen:
	\[
		r_\mathcal{A}^\prime = (c (a+b)^\ast a)^\ast \equiv r_\mathcal{A}.
	\]
\end{example}
\begin{figure}
	\centering
	\input{figs/fw_example}
	\caption{NFA \( \mathcal{A} \) für Beispiel~\ref{exp:fw_example}.}
	\label{fig:fw_example}
\end{figure}
Die beiden Lemmata~\ref{lem:regex2nfa} und~\ref{lem:nfa2regex} ergeben zusammen den Äquivalenzsatz von Kleene. Mit den Sätzen~\ref{thm:regular_complement} und~\ref{cor:regular_intersection} können wir auch schließen, dass sich die regulären Ausdrücke auch um Operatoren für Komplement und Schnitt erweitern ließen, ohne dadurch zusätzliche Sprachen zu beschreiben. Dies ist nicht offensichtlich: Dazu versuche man einmal Komplement und Schnitt nur mit Hilfe von Vereinigung (\(+\)), Konkatenation (\(\cdot\)) und Iteration (\(^\ast\)) zu simulieren.\par
Wir werden nun anstelle von FA-erkennbaren Sprachen auch nur noch von regulären Sprachen sprechen.


\subsection{Weitere Abschlusseigenschaften}\label{sec:regular_closure2}
Wir haben bis hier hin reguläre Sprachen eine umfangreiche Charakterisierung gegeben, nämlich als Sprachen, die sich durch reguläre Ausdrücke oder Sprachen von endliche Automaten beschreiben lassen -- auf die Äquivalenz dieser Charakterisierungen haben wir lange hingearbeitet und mit dem Ä\-qui\-va\-lenz\-satz von Kleene einen schweren Brocken der theoretischen Informatik bewiesen. Dieser Abschnitt wird nun wieder deutlich einfacher. Wir haben bereits einige Abschlusseigenschaften regulärer Sprachen bewiesen: Schnitt, Vereinigung, Komplement, Konkatenation, Iteration usw., aber auch unter ''erfundenen`` Operatoren wie \textit{Perfect Shuffle}. Diese Abschlüsse allein zusammen mit dem Äquivalenzsatz von Kleene ergeben bereits genug Gründe um reguläre Sprachen als eine in wissenschaftlicher Hinsicht \textit{sinnvolle} Sprachklasse zu sehen. Wir werden in diesem Abschnitt nun ein paar weitere sinnvolle Abschlusseigenschaften beweisen.\par
%TODO Freies Monoid
\begin{definition}\label{def:stringhomomorphism}
	Seien \( \Sigma, \Gamma \) Alphabete. Die  Abbildung \( h\colon \Sigma^\ast \to \Gamma^\ast \) heißt \textit{(Wort-)Homomorphismus}, wenn für alle \( a_0 \ldots a_{n-1} \in \Sigma^\ast \) gilt, dass
	\[
		h(a_0 \ldots a_{n-1}) = h(a_0) \ldots h(a_{n-1}).
	\]
\end{definition}
Insbesondere gilt bei einem Homomorphismus \( h \), dass \( h(uv) = h(u)h(v) \) und somit auch \( h(\varepsilon) = \varepsilon \).
\begin{definition}
	Sei \( L \subseteq \Sigma^\ast \) eine Sprache und \( h\colon \Sigma^\ast \to \Gamma^\ast \) ein Homomorphismus. Dann ist
	\[
		h(L) \coloneqq \{h(w) : w \in L\}
	\]
	die \textit{Sprache unter dem Homomorphismus} \( h \) von \( L \) (über \( \Gamma \)).
\end{definition}
\begin{theorem}
	Sei \( L \subseteq \Sigma^\ast \) eine reguläre Sprache und \( h\colon \Sigma^\ast \to \Gamma^\ast \) ein Homomorphismus. Dann ist auch \( h(L) \subseteq \Gamma^\ast \) regulär.
	\begin{proof}
		Sei \( L \subseteq \Sigma^\ast \) regulär und \( h \) Homomorphismus auf \( \Sigma \). Da \( L \) regulär existiert ein regulärer Ausdruck \( e \in \mathsf{RE}_\Sigma \) mit \( \llbracket e \rrbracket = L \). Wir können auf dem regulären Ausdruck den Homomorphismus anwenden, wobei wir Klammern, Punkte und Sterne ignorieren, dazu definieren wir die Erweiterung \( \hat{h} \) von \( h \) mit \( \hat{h} \vert_\Sigma \equiv h \) und \( \hat{h}(\sigma) = \sigma \) für \( \sigma \in \{(,),\bm{+}, \bm{\cdot}, \overset{\bm{\ast}}{}\} \). Aus der Definition~\ref{def:stringhomomorphism} geht hervor, dass es genügt den Homomorphismus auf den einzelnen Symbolen \( a \in \Sigma \) anzuwenden. Im Detail heißt das: 
		\begin{itemize}
			\item \( \hat{h}(\bm{\emptyset}) = \bm{\emptyset} \),
			\item \( \hat{h}(\bm{a}) = \bm{b}_0 \ldots \bm{b}_{n-1} \), wenn \( h(a) = b_0 \ldots b_{n-1} \),
			\item \( \hat{h}((r \bm{+} r^\prime)) = ((\hat{h}(r)) + (\hat{h}(r^\prime))) \),
			\item \( \hat{h}((r \bm{\cdot} r^\prime)) = ((\hat{h}(r)) \bm{\cdot} (\hat{h}(r^\prime))) \),
			\item \( \hat{h}((r)\overset{\bm{\ast}}{}) = (\hat{h}(r))\overset{\bm{\ast}}{} \).
		\end{itemize} 
		Es bleibt zu zeigen, dass \( \llbracket \hat{h}(e) \rrbracket = h(L) \).
	\end{proof}
\end{theorem}
Wir betrachten nun eine weitere Abbildung:
\begin{definition}
	Die \textit{Reverse}-Operation ist definiert als
	\[
		\cdot^\mathcal{R} \colon \Sigma^\ast \to \Sigma^\ast, \quad\quad a_0 a_1 \ldots a_{n-1} \mapsto a_{n-1} \ldots a_1 a_0.
	\]
	Wir können sie auf Sprachen \( L \subseteq \Sigma^\ast \) natürlich erweitern durch
	\[
		L^\mathcal{R} \coloneqq \{ w^\mathcal{R} : w \in L \}.
	\]
\end{definition}
Beachte, dass \( w^\mathcal{R} \) wohldefiniert ist für jedes \( w \in \Sigma
^\ast \) da jedes \( w \) eine eindeutige Darstellung besitzt im freien Monoid \( (\Sigma^\ast, \cdot) \).
\begin{remark*}
	Die Reverse-Operation ist kein Homomorphismus für \( \left| \Sigma \right| > 1 \), denn es gilt für z.B: \( abb = (bba)^\mathcal{R} \). Wäre \( \cdot^\mathcal{R} \) Homomorphismus müsste aber gelten \( (bba)^\mathcal{R} = b^\mathcal{R} b^\mathcal{R} a^\mathcal{R} = bba \neq abb \). Eine zeichenweise Anwendung von \( \cdot^\mathcal{R} \) ist also nicht möglich.
\end{remark*}
\begin{theorem}
	Sei \( L \subseteq \Sigma^\ast \) regulär. Dann ist auch \( L^\mathcal{R} \) regulär.
	\begin{proof}
		folgt. %TODO
	\end{proof}
\end{theorem}


\subsection{Nicht-reguläre Sprachen}\label{sec:regular_nonregular}
Bisher haben wir nur sehr einfache Sprachen betrachtet, die alle regulär waren. D.h. wir konnten zu jeder bisher betrachteten Sprache einen regulären Ausdruck angeben oder einen endlichen Automaten. Man könnte auf die Idee kommen, dass alle formalen Sprachen regulär sind. Dies ist aber nicht der Fall. Dass es mehr Sprachen als nur reguläre Sprachen geben muss folgt bereits aus kombinatorischen Argumenten. Dies heben wir uns aber für ein späteres Kapitel noch auf. Schauen wir uns zunächst ein Beispiel für eine nicht-reguläre Sprache an:
\[
	L_\mathbb{P} \coloneqq \{ a^p : p \in \mathbb{P} \}.
\]
\( L_\mathbb{P} \) ist die Sprache aller (mit \( a \)) unär codierten Primzahlen, d.h. \( L_\mathbb{P} = \{ aa, aaa, a^5, a^7, a^{11}, \ldots \} \). Dass \( L_\mathbb{P} \) nicht regulär ist, ist auch nicht so schwer einzusehen: Intuitiv, wie würde ein regulärer Ausdruck aussehen? Natürlich könnten wir eine unendliche Vereinigung bilden (dies wäre jedoch nicht regulär, da reguläre Ausdrücke endlich sind):
\[
	r_\mathbb{P} = \sum_{p \in \mathbb{P}} a^p = aa + aaa + a^5 + a^7 + a^{11} + \ldots
\]
Es erscheint klar, dass es jedoch keine Möglichkeit gibt diesen Ausdruck zu vereinfachen. In einem Automaten wird es noch deutlicher. Wenn wir mit endlich vielen Zuständen auskämen um genau alle unendlich vielen Primzahlcodierungen zu erkennen würden wir Kreise im Transitionsgraphen und somit eine gewisse Regelmäßigkeit in der Wiederholung von Primzahlen bekommen. Dies würde bedeuten, dass die Generierung neuer Primzahlen viel einfacher wäre als bisher angenommen, da wir ab einer bestimmten Primzahl einfach immer eine Konstante addieren könnten um die nächste zu bekommen. Das ist natürlich nicht der Fall. Eine schematische Erklärung dieses Arguments ist in Abbildung~\ref{fig:dfa_primes}. Egal an welcher und an wie vielen Stellen wir eine Zustandwiederholung zulassen (durch gestrichtelte Kante angedeutet), wir würden stets Codierungen akzeptieren, die nicht zu Primzahlen gehören können. In diesem Abschnitt werden wir uns ansehen, wie wir dieses Argument formalisieren und auf weitere nicht-reguläre Sprachen anwenden können.
\begin{figure}
	\centering
	\input{figs/dfa_primes}
	\caption{Wenn \( L_\mathbb{P} \) regulär wäre gäbe es unendlich viele Primzahlen, die sich als \( p + ck \) darstellen lassen, wobei \( p \in \mathbb{P}, c \in \mathbb{N}, k = 0, 1, 2, \ldots \).}
	\label{fig:dfa_primes}
\end{figure}
Vorab: Mit diesem Abschnitt der Vorlesung haben viele Studenten Probleme, da das später vorgestellte Lemma etwas sperrig ist. Tatsächlich basiert dieses Lemma jedoch auf dem wahrscheinlich einfachsten Prinzip der Mathematik. Wir führen die Idee des Lemmas anhand dieses Prinzips einmal vor. Man sollte dieses Beispiel zunächst verstehen, dann sollte der Schritt zum Lemma geradezu trivial wirken.
\begin{example}\label{exp:pumping1}
	Die Sprache \( L = \{a^n b^n : n \in \mathbb{N} \} \) ist nicht regulär.
	\begin{proof}
		Angenommen \( L \) wäre regulär. Dann existiert ein endlicher Automat \( \mathcal{A} \) mit \( L(\mathcal{A}) = L \). Wir nehmen an, dass \( \mathcal{A} \) \( n \) Zustände besitzt. Betrachten wir nun das Wort \( w = a^n b^n \) Offenbar ist \( w \in L \), also gibt es einen akzeptierenden Lauf \( \varrho = (r_0, \ldots, r_{2n}) \). Das Wort hat die Länge \( \left| w \right| = 2n \geq n \). Das heißt aber, dass spätestens nach Lesen des letzten \( a \)s sich im Lauf von \( \mathcal{A} \) ein Zustand wiederholt haben muss. Dies folgt aus dem sogenannten \textit{pigeonhole principle} (oder deutsch: \textit{Schubfachprinzip}): Der Lauf auf den ersten \( n \) Zeichen ist \( (r_0, \ldots, r_n) \), d.h. er besteht aus \( n+1 \) Zuständen. Da \( \mathcal{A} \) nur \( n \) Zustände hat, muss mindestens einer doppelt im Lauf vorkommen. Wir nehmen an, dass \( r_i = r_j \) mit oBdA \( i < j \). Es ist auch klar, dass bis zu dieser Wiederholung im \(j\)-ten Schritt nur \( a \)s gelesen wurden, da die ersten \( n \) Symbole von \( w \) alle \( a \) sind (d.h. \( j \leq n \)). Betrachten wir nun folgenden Lauf
		\[
			\varrho^\prime = (r_0, \ldots, r_i, r_{j+1}, \ldots, r_{2n})
		\]
		auf dem Wort \( w^\prime = a^{n-k}b^n \) mit \( k \coloneqq j-i > 0 \). Der Lauf entsteht also durch das Weglassen der \( a \)s die sonst zwischen der Zustandswiederholung gelesen worden wären. Dieser Lauf ist gültig auf dem Automaten, denn \( r_i = r_j \), d.h. die selben Transitionen die im \(j\)-ten Schritt möglich waren sind auch bereits im \(i\)-ten Schritt möglich. Außerdem gilt, dass \( \varrho^\prime \) ein akzeptierender Lauf ist, da er auf \( r_{2n} \in F \) endet. Also akzeptiert \( \mathcal{A} \) das Wort \( w^\prime \), welches aber echt weniger \( a \)s als \( b \)s besitzt (denn \( k > 0 \), weil \( i < j \)), also ist \( w^\prime \notin L \). Der Automat \( \mathcal{A} \) erkennt also nicht \( L \). Da \( \mathcal{A} \) und die Anzahl seiner Zustände \( n \) belieibig gewählt wurde, existiert also kein endlicher Automat für \( L \). Somit ist \( L \) nicht regulär.
	\end{proof}
\end{example}
\begin{remark*}
	Statt die \( a \)s zwischen der Zustandswiederholung wegzulassen hätten wir natürlich auch beliebig viele weitere Wiederholungen hinzufügen können. Wir würden also den Lauf
	\[
		\varrho^{\prime\prime} = (r_0, \ldots, r_i, \ldots, r_j, r_{i+1} \ldots r_j, \ldots, r_{j+1}, \ldots, r_{2n})
	\]
	betrachten zu dem Wort \( w^{\prime\prime} = a^{n+hk}b^n \) für ein beliebiges \( h > 0 \).
\end{remark*}
Dieses Beispiel ist sehr ausführlich jetzt besprochen worden. Sobald man diese Überlegungen verstanden hat, hat man diesen gesamten Abschnitt eigentlich auch schon verstanden und sollte auch beim folgendem Lemma nicht mehr allzu große Schwierigkeiten haben. Tatsächlich bildet das Beispiel den Beweis des Lemmas auf einen einzelnen Automaten bzw. eine einzelne Sprache ab.
\begin{lemma}[Pumping-Lemma]
	Sei \( L \subseteq \Sigma^\ast \) eine reguläre Sprache. Dann existiert ein \( n \in \mathbb{N}_+ \), sodass für alle \( w \in L \) mit \( \left| w \right| \geq n \) eine Zerlegung \( w = xyz \) existiert mit den Eigenschaften:
	\begin{enumerate}
		\item \( \left| xy \right| \leq n \),
		\item \( y \neq \varepsilon \),
		\item \( xy^iz \in L \) für alle \( i \in \mathbb{N} \). 
	\end{enumerate}
	\begin{proof}
		Sei \( L \subseteq \Sigma^\ast \) eine beliebige reguläre Sprache und \( \mathcal{A} = (Q, \Sigma, \Delta, q_0, F) \) ein NFA mit \( L(\mathcal{A}) = L \). Wir setzen \( n \coloneqq \left| Q \right| \). Betrachte nun ein belibiges Wort \( w = a_0 \ldots a_{m-1} \in L \) mit Länge \( \left| w \right| \eqqcolon m \geq n \). Falls kein solches Wort existiert sind wir bereits fertig. Ansonsten betrachten wir den Lauf \( \varrho = (r_0, \ldots, r_m) \) von \( \mathcal{A} \) auf \( w \). Da \( w \in L = L(\mathcal{A}) \) ist \( r_m \in F \). Wegen \( m \leq n \) muss eine Zustandswiederholung in \( \varrho \) vorkommen, spätestens nach Lesen des \(n\)-ten Zeichens. Seien also die Zustände \( r_k \) und \( r_j \) mit \( 0 \leq k < j \leq n \) gleich. Wir wählen die Zerlegung \( x \coloneqq a_0 \ldots a_{k-1}, y \coloneqq a_k \ldots a_{j-1}, z \coloneqq a_j \ldots a_m \) und zeigen, dass diese die gewünschten Eigenschaften hat:
		\begin{enumerate}
			\item \( \left| xy \right| = \left| a_0 \ldots a_{j-1} \right| = j \leq n \). \checkmark
			\item \( y = a_k \ldots a_{j-1} \neq \varepsilon \), da nach Annahme \( j-k > 0 \). \checkmark
			\item Die Aussage gilt offensichtlich für \( i = 1 \). Für \( i = 0 \) betrachten wir nun den Lauf für das Wort \( xz = xy^0z \): \( (r_0, \ldots, r_k, r_{j+1}, \ldots, r_m) \). Dieser Lauf ist tatsächlich korrekt, denn \( r_k = r_j \eqqcolon r \) und somit gilt \( (r_k, a_j, r_{j+1}) = (r_j, a_j, r_{j+1}) \in \Delta \) (alle anderen Transitionen sind nach Annahme trivialerweise ebenfalls in \( \Delta \)). Außerdem gilt weiterhin \( r_m \in F \). Also akzeptiert \( \mathcal{A} \) das Wort \( xz \). Ansonsten haben wir, dass \( r \reaches{y} r \) und somit auch \( r \reaches{y^i} r \) für \( i \geq 2 \).\checkmark \qedhere
		\end{enumerate}
	\end{proof}
\end{lemma}
\begin{figure}
	\centering
	\input{figs/pumping}
	\caption{Zustandswiederholung in \( p \). Es gibt also einen Lauf von \( q_0 \) nach \( p \) und einen Lauf von \( p \) nach \( q \in F \). Den Zwischenlauf von \( p \) nach \( p \) kann man also weglassen (oder auch beliebig oft wiederholen).}
	\label{fig:pumping}
\end{figure}
\begin{remark*}
	Wir stellen wieder fest, dass der Beweis des Pumping-Lemmas eine Verallgemeinerung des Vorgehens aus Beispiel~\ref{exp:pumping1} ist. Wir betrachten auch wieder einen Automaten mit \( n \) Zuständen und akzeptierte Wörter, die zu lang sind, als dass sie dann ohne Wiederholung erkannt werden könnten. Das Weglassen oder Wiederholen von Infixen (\textit{pumpen}) entspricht dem Weglassen oder Wiederholen im Lauf.
	Eine Visualisierung des Prinzip ist in Abbildung~\ref{fig:pumping}.
\end{remark*}
\begin{remark*}
	Wir können mit dem Pumping-Lemma nicht zeigen, dass eine Sprache regulär ist. Es ist lediglich eine notwendige Bedingung für Regularität, aber keine hinreichende Bedingung. D.h. es gibt Sprachen, die nicht regulär sind, aber dennoch dem Pumping-Lemma genügen, z.B.
	\[
		K = \{a^m b^n c^n : m \in \mathbb{N}_+, n \in \mathbb{N} \} \cup \{ b^m c^n : m, n \in \mathbb{N} \}.
	\]
\end{remark*}
\begin{example}
	Wir betrachten die Sprache
	\[
		L = \{ ww^\mathcal{R} : w \in \Sigma^\ast \}
	\]
	aller Palindrome gerader Länge über \( \Sigma = \{a, b\} \). \( L \) ist nicht regulär.
	\begin{proof}
		Angenommen \( L \) wäre regulär. Dann gilt das Pumping-Lemma. Sei also \( n \in \mathbb{N}_+ \) beliebig und wir betrachten das Wort \( w = a^n bb a^n \in L \). Es hat die Länge \( \left| w \right| = 2n+2 \geq n \). Wir betrachten nun Zerlegungen, die die Eigenschaften (i) und (ii) erfüllen und zeigen, dass (iii) dann nicht gelten kann. Wegen (i) ist \( y = a^k \) für ein \( k \leq n \) (\( k = \left| y \right| \leq \left| xy \right| \leq n \), da die ersten \( n \) Zeichen alle \( a \) sind, kommt also auch kein \( b \) in \( y \) vor). Wegen (ii) gilt außerdem, dass \( k > 0 \). Betrachte nun das Wort \( w^\prime \coloneqq xyyz = a^{n+k}bba^n \). Es liegt nicht in \( L \), da \( w^\prime \) kein Palindrom ist. Damit gilt die Eigenschaft (iii) für \( i = 2 \) nicht (gilt sogar für gar kein \( i \neq 1 \)). Dies ist ein Widerspruch zum Pumping-Lemma, also kann \( L \) nicht regulär sein. 
	\end{proof}
\end{example}


\subsection{Minimierung von DFAs}\label{sec:regular_minimization}
folgt. %TODO


\subsection{Myhill-Nerode-Äquivalenz}\label{sec:regular_myhill-nerode}
Bisher haben wir reguläre Sprachen durch Formalismen wie reguläre Ausdrücke und endliche Automaten charakterisiert. Nun wollen wir eine Eigenschaft finden, die reguläre Sprachen auf einer stark mathematisch-strukturellen Ebene charakterisiert. Dazu erinnern wir uns an unser freies Monoid \( (\Sigma^\ast, \cdot, \varepsilon) \), welches bereits in Abschnitt~\ref{sec:regular_closure2} uns einmal angesehen haben. Im vorigen Abschnitt haben wir außerdem den Begriff der Zustandsäquivalenz definiert und festgestelt, dass der minimale DFA zu einer regulären Sprache bis auf Isomorphie eindeutig ist. Wir wollen nun diese Äquivalenz auf Wörter übertragen: Die ganze Idee dabei ist die Wörter die im minimalen DFA den selben Zustand vom Startzustand erreichen als äquivalent zu betrachten. Wir definieren also eine Äquivalenzrelation (siehe Definition~\ref{def:relations}). Tatsächlich definiert die gleich eingeführte Relation sogar noch etwas mehr, deshalb schieben wir eine weitere Definition hier ein.
\begin{definition}
	Sei \( A \) eine Menge und \( \bullet \) eine zweistellige Funktion auf \( A \). Eine Relation \( \sim \subseteq A \times A \) heißt \textbf{rechtsseitige Kongruenz} (bezüglich \( \bullet \)), wenn
	\begin{enumerate}
		\item \( \sim \) eine Äquivalenzrelation ist und
		\item \( \bullet \) diese Relation respektiert (d.h. für alle \( a \sim b \) und alle \( c \in A \) gilt \( a \bullet c \sim b \bullet c \)).
	\end{enumerate}
\end{definition}
Wir definieren nun wie angekündigt unsere Äquivalenzrelation auf Wörtern.
\begin{definition}
	Sei \( \Sigma \) ein Alphabet und \( L \subseteq \Sigma^\ast \). Seien \( u, v \in \Sigma^\ast \). \( u \) und \( v \) heißen \textbf{Myhill-Nerode-äquivalent bezüglich \( L \)} (\( u \sim_L v \)) genau dann, wenn für alle \( w \in \Sigma^\ast \) gilt, dass \( uw \in L \) g.d.w. \( vw \in L \).
\end{definition}
Diese Relation ist tatsächlich sogar nicht nur eine Äquivalenz sondern auch eine rechtsseitige Kongruenz, deswegen sind auch die Begriffe \textit{Myhill-Nerode-Rechtskongruenz} und -- nicht ganz korrekterweise -- \textit{Myhill-Nerode-Kongruenz} gebräuchlich. Dies wollen wir im folgenden zeigen.
Eine einfacher Folgerung aus der Definition ist
\begin{lemma}\label{lem:mncongruence}
	Sei \( L \subseteq \Sigma^\ast \) eine Sprache. Für alle \( u, v, w \in \Sigma^\ast \) mit \( u \sim_L v \) gilt \( uw \sim_L vw \).
	\begin{proof}
		folgt. %TODO
	\end{proof}
\end{lemma}
Wir können auch wie in Definition~\ref{def:equivalenceclass} die Myhill-Nerode-Äquivalenzklassen bestimmen. Man benennt Äquivalenzklassen auch durch einzelne Repräsentanten, z.B. mit \( [u]_L \) für die Äquivalenzklasse in der alle Elemente enthalten sind, die zu \( u \) Myhill-Nerode-äquivalent sind.
Das Lemma~\ref{lem:mncongruence} kann dafür verwendet werden um Myhill-Nerode-Äquivalenzklassen zu finden. Wir gehen dabei wie folgt vor:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Algo?
Wir starten mit dem leeren Wort \( \varepsilon \) und der zugehörigen Äquivalenzklasse \( [\varepsilon]_L \). Danach wiederholen wir folgende Schritte bis keine neue Äquivalenzklassen mehr gefunden werden:\\
Für jedes Symbol \( a \in \Sigma \) und jede bisher gefundene Äquivalenzklasse \( [u]_L \) betrachte die Äquivalenzklasse \( [ua]_L \). Ist \( [ua]_L = [v]_L \) für eine bereits gefundene Äquivalenzklasse, passiert nichts. Ist dies nicht der Fall, fügen wir \( [ua]_L \) als neue Äquivalenzklasse hinzu.\par
Beachte, dass dieses Verfahren nur für reguläre Sprachen terminiert. Nicht-reguläre Sprachen haben unendlich viele Äquivalenzklassen (s. Satz~\ref{thm:nerode}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Die Myhill-Nerode-Äquivalenz ist inhaltlich eines der schönsten Sachen in FoSAP, weil sie 1. eine notwendige und hinreichende Bedingung liefert, dass eine Sprache regulär ist und weil sie 2. auch einen direkten Weg zu einem minimalen deterministischen endlichen Automaten liefert.
\begin{theorem}[Satz von Nerode]\label{thm:nerode}
	Eine Sprache ist genau dann regulär, wenn die Anzahl der Myhill-Nerode-Äquivalenzklassen endlich ist.
	\begin{proof}
		folgt. %TODO
	\end{proof}
\end{theorem}
Man schreibt auch \( index(\sim_L) < \infty \). Die Sprache \( L \) im Beispiel hat \( index(\sim_L) = 3 < \infty \) und ist somit regulär. Betrachte nun ein Standardbeispiel für eine nicht-reguläre Sprache 
\[
	K = \{ a^n b^n : n \in \mathbb{N} \}.
\]
\( K \) ist nicht regulär. Betrachte die Wörter \( u_i = a^i \) und \( u_j = a^j \) mit \( i \neq j \). Es gilt offensichtlich \( u_i \not\sim_K u_j \), denn das Wort \( b^i \) trennt die Wörter, weil \( u_i b^i = a^i b^i \in K \), aber \( u_j b^i = a^j b^i \notin K \). Da es unendlich viele \( i, j \in \mathbb{N} \) mit \( i \neq j \) gibt, gibt es auch unendlich viele Äquivalenzklassen. Also ist \( K \) nach dem Satz von Nerode nicht regulär.
\begin{theorem}
	Der minimale DFA zu einer reguläre Sprache \( L \) ist isomorph zum Myhill-Nerode-DFA:
	\[
		\mathcal{A}_L = (\Sigma^\ast/_L, \Sigma, \delta, [\varepsilon]_L, \{[u]_L : u \in L \}),
	\]
	mit \( \delta([u]_L, a) = [ua]_L \).
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Algorithmen für Reguläre Sprachen}\label{sec:regular_algorithms}
% Emptiness, Universality, Equivalence, Membership


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Anwendung: First-Longest-Match-Analyse}\label{sec:regular_first-longest-match}
Wir schauen uns hier einmal eine praktische Anwendung von regulären Ausdrücken und endlichen Automaten an. Bisher haben wir stets das \textit{einfache Matching-Problem} für reguläre Ausdrücke betrachtet. Dabei ging es darum zu prüfen, ob ein gegebenes Wort \( w \in \Sigma^\ast \) zur Sprache eines regulären Ausdrucks \( r \in \mathsf{RE}_\Sigma \) gehört. Dies ließ sich mit Hilfe der Thompson-Konstruktion einfach als das Wortproblem eines \(\varepsilon\)-NFA betrachten. In der Praxis ist das einfach Matching-Problem aber zu primitiv um echte Probleme lösen zu können. Vorallem im Compilerbau benötigen wir etwas mehr, wenn wir Programmcode in seine Bestandteile zerlegen wollen um später Syntax-Checks darauf durchführen zu können und schließlich eine Semantik für das Programm festzulegen. In der Fachsprache nennt man diesen Teil eines Compilers auch \textit{Scanner} oder \textit{Lexer} (siehe auch die Programme \texttt{lex}, \texttt{flex}).
In diesem Dokument verwenden wir die Variablen \( h, i, j, k, \ell \) stets als natürliche Zahlen aus einer Menge \( [m] \coloneqq \{ 1, \ldots, m \} \). Manchmal ist die Notation etwas sloppy, aber hoffentlich verständlich genug.


%\subsection*{Problemstellung: Extended Matching Problem}
Das \textit{erweiterte (extended) Matching-Problem} kommt direkt aus der Anwendung des Compilerbaus. Gegeben ist ein Wort \( w \in \Sigma^\ast \) und eine Reihe von regulären Ausdrücken \( r_1, \ldots, r_n \in \mathsf{RE}_\Sigma \). (\textit{Bemerkung:} In der Praxis ist die Annahme \( \varepsilon \notin L(r_i) \neq \emptyset \) für alle \( i \) sinnvoll). Gesucht ist nun eine Zerlegung des Wortes \( w = u_1 \ldots u_k \), wobei für jedes \( u_j \) ein \( r_{i_j} \) existieren muss, sodass \( u_j \in L(r_{i_j}) \). Man nennt die Zerlegung in \( (u_1, \ldots, u_k) \) auch \textit{Dekomposition} und die zugehörigen Indizes der regulären Ausdrücke \( (i_1, \ldots, i_k) \) \textit{Analyse} von \( w \) bezüglich \( r_1, \ldots, r_n \).

Wir sehen schnell, dass weder Dekomposition, noch Analyse eindeutig sein müssen (insbesondere dann, wenn wir \( \varepsilon \in L(r_i) \) zulassen).
\begin{example}
	\begin{enumerate}
		\item \( r_1 = a^+, w = aa \). Ergibt Dekompositionen \( (aa) \) und \( (a, a) \) mit zugehöriger (eindeutiger) Analyse \( (1) \) bzw. \( (1, 1) \).
		\item \( r_1 = a+b, r_2 = a+c, w = a \). Ergibt eindeutige Dekomposition \( (a) \) mit zwei verschiedenen Analysen \( (1) \) und \( (2) \).	
	\end{enumerate}
	Im zweiten Fall stellen wir uns vor, dass \( r_1 \) mögliche Schlüsselwörter einer Programmiersprache (\texttt{while}, \texttt{true}, \texttt{if}) beschreibt und \( r_2 \) mögliche Variablennanem (Identifier), dann sieht man warum eine eindeutige Analyse wichtig ist. In diesem Anwendungsfall ist es natürlich zu sagen, dass der Ausdruck \( r_1 \) ''wichtiger`` ist als \( r_2 \) (deshalb verbieten die mesiten Programmiersprachen auch Schlüsselwörter als Identifier). Ähnliche Beispiele lassen sich auch für die Dekomposition finden.
\end{example}

Wir wollen nun sowohl, die Dekomposition, als auch die Analyse eindeutig machen. Ein erster Ansatz wäre zunächst das leere Wort zu verbieten und sicherzustellen, dass \( L(r_i) \cap L(r_j) = \emptyset \) für alle \( i \neq j \), indem wir den gemeinsamen Schnitt von zwei Ausdrücken aus dem ''unwichtigeren`` Ausdruck entfernen. Dadurch würde zumindest die Analyse eindeutig werden. Diese Idee ist jedoch nicht sinnvoll, u.a. deshalb, da das Entfernen des Schnittes -- also das Erkennen von \( L(r_j^\prime) \coloneqq L(r_j) \setminus L(r_i) \) -- eine Produktkonstruktion erfordert und somit tendenziell teuer ist.


%\subsection*{Eindeutigkeit durch First-Longest-Match}
Der am meisten verbreitete Weg ist es folgende zwei Prinzipien zu implementieren:
\begin{description}
	\item[Longest Match] Mache jedes \( u_i \) der Zerlegung so lang wie möglich.
	\item[First Match] Wähle aus den matchenden regulären Ausdrücken den mit dem kleinesten Index.
\end{description}

\begin{definition}
	Eine Dekomposition \( (u_1, \ldots, u_k) \) von \( w \in \Sigma^\ast \) bezüglich der regulären Ausdrücke \( r_1, \ldots, r_n \in \mathsf{RE}_\Sigma \) heißt \textit{Longest-Match-Dekomposition} (LMD), wenn für jedes \( i \in [k] \), \( x \in \Sigma^+ \), \( y \in \Sigma^\ast \) mit \( w = u_1 \ldots u_i x y \) gilt, dass kein \( j \in [n] \) existiert, sodass \( u_i x \in L(r_j) \). 
\end{definition}
Umgangssprachlich bedeutet das, dass wir an einen Teil der Komposition \( u_i \) kein nicht-leeres Wort mehr anhängen können, was noch nicht verarbeitet wurde.
Es ist klar, dass eine LMD eindeutig ist, falls sie existiert. Sie muss jedoch nicht immer existieren:

\begin{example}
	\( r_1 = a^+, r_2 = ab, w = aab \) hat Dekomposition \( (a, ab) \) aber keine LMD.
\end{example}

\begin{definition}
	Sei \( (u_1, \ldots, u_k) \) eine LMD von \( w \in \Sigma^\ast \) bezüglich der regulären Ausdrücke \( r_1, \ldots, r_n \in \mathsf{RE}_\Sigma \). Die zugehörige \textit{First-Longest-Match-Analyse} (FLM-Analyse) \( (i_1, \ldots, i_k) \) ist gegeben durch
	\[
		i_j \coloneqq \min \{ \ell \mid u_j \in L(r_\ell) \}
	\]
	für jedes \( j \in [k] \).
\end{definition}
Auch hier ist klar, dass es höchstens eine FLM-Analyse gibt und sie existiert, wenn die LMD existiert.


%\subsection*{Eine mögliche Implementierung}
Es gibt viele Möglichkeiten eine FLM-Analyse durchzuführen. Die von mir hier vorgestellte Art ist nicht die von mir favorisierte, aber sie ist insofern intuitiv, als dass sie nur Konzepte aus der FoSAP-Vorlesung verwendet und ein wenig über Arrays.

Als Eingabe erhalten wir also \( w \in \Sigma^\ast \) und die regulären Ausdrücke \( r_1, \ldots, r_n \in \mathsf{RE}_\Sigma \). Gesucht ist eine FLM-Analyse inklusive zugehöriger LMD oder ein Error, falls diese nicht existieren.
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwComment{Comment}{\texttt{// }}{}
	\underline{FLM}{($w, (r_i)_{i=1}^n$)}\\
	\Output{FLM-Analyse $(i_j)_{j=1}^k$ und LMD $(u_j)_{j=1}^k$}	
	\Comment{Vorbereitung}
	Wandle alle \( r_i \) mit Thompson-Konstruktion in \(\varepsilon\)-NFAs \( \mathcal{A}_i = (Q_i, \Sigma, \delta_i, q_0^i, F_i) \) um.\\
	Baue neuen $\varepsilon$-NFA $\mathcal{A} = (\biguplus_i Q_i \uplus \{q_0, q_f\}, \Sigma \uplus [n], \delta, q_0, \{q_f\})$ mit $\delta(p, a) = \delta_i(p, a)$ falls $p \in Q_i$ und $\delta(q_0, \varepsilon) = \{q_0^i \mid i \in [n]\}$ und $\delta(q, i) = \{q_f\}$, falls $q \in F_i$.\\
	Eliminiere $\varepsilon$-Transitionen.\\
	Determinisiere mit Potenzmengenkonstruktion.\\
	Minimiere mit Markierungsalgorithmus.\\
	\tcc{Wir haben jetzt einen DFA mit folgender Eigenschaft: Vom Zustand $\hat{\delta}(q_0, u)$ ist genau dann eine Transition mit $i \in [n]$ in einen Endzustand möglich, wenn $u \in L(r_i)$.}
	\Comment{Löse nun das erweiterte Matching-Problem}	
	Setze $\ell = 1, w_\ell = w$.\\
	\While{$w_\ell \neq \varepsilon$}{
		\texttt{A} = leeres Array der Länge $\left| w_\ell \right|$.\\
		Simuliere $\mathcal{A}$ auf $w_\ell$, prüfe in jedem Schritt $j$, ob eine $i$-Transition in einen akzeptierenden Zustand möglich ist.
		Falls ja, setze \texttt{A[$j$]} auf das kleinste mögliche $i$.\\
		Nach der Simulation laufe \texttt{A} von hinten nach vorne durch bis wir den ersten nicht-leeren Eintrag an Stelle $h$ finden. Sollte es keinen geben gebe einen Error aus, ansonsten ist $u_\ell = w_{\ell_1} \ldots w_{\ell_h}$ und $i_\ell = \texttt{A[}h\texttt{]}$.\\
		Setze $w_{\ell+1} = w_{\ell_h} \ldots w_{\ell_{\left| w_\ell \right|}}, \ell = \ell+1$.
	}
	\Comment{Jetzt ist $w_\ell = \varepsilon$.}
	Gebe $(u_1, \ldots, u_{\ell-1})$ und $(i_1, \ldots, i_{\ell-1})$ aus.
	\caption{First-Longest-Match-Analyse}
	\label{alg:algorithm}
\end{algorithm}\\
In Abbildung~\ref{fig:nfa} seht ihr wie der \(\varepsilon\)-NFA der nach Zeile 3 entsteht aussieht.

\begin{figure}
	\centering
	\begin{tikzpicture}[->, >=stealth', shorten >=1pt, auto, semithick, every state/.style={inner sep=0pt, minimum size=20pt}]
		\node[state, initial, initial text=] (0) at (0, 0) {$q_0$};
		
		\node at (1.7, 2.5) {$\mathcal{A}_1$};
		\draw[black, rounded corners] (1.5, 2.3) rectangle (4.5, 1.3);
		\node[state] (01) at (2, 1.8) {$q_0^1$};
		\node[state] (f1) at (4, 1.8) {$q_f^1$};
		\node (d1) at (3, 1.8) {$\cdots$};
		
		\node at (1.7, .8) {$\mathcal{A}_2$};
		\draw[black, rounded corners] (1.5, .6) rectangle (4.5, -.4);
		\node[state] (02) at (2, .1) {$q_0^2$};
		\node[state] (f2) at (4, .1) {$q_f^2$};
		\node (d2) at (3, .1) {$\cdots$};
		
		\node (dv) at (3, -1.1) {$\vdots$};		
		
		\node at (1.7, -1.6) {$\mathcal{A}_n$};
		\draw[black, rounded corners] (1.5, -1.8) rectangle (4.5, -2.8);
		\node[state] (0n) at (2, -2.3) {$q_0^n$};
		\node[state] (fn) at (4, -2.3) {$q_f^n$};
		\node (dn) at (3, -2.3) {$\cdots$};
		
		\node[state, accepting] (f) at (6, 0) {$q_f$};
		
		\path (0) edge node {$\varepsilon$} (01)
			(0) edge node[below left] {$\varepsilon$} (02)
			(0) edge node[below left] {$\varepsilon$} (0n)
			(f1) edge node {$1$} (f)
			(f2) edge node[below left] {$2$} (f)
			(fn) edge node[below right] {$n$} (f);	
	\end{tikzpicture}
	\caption{Skizze des NFAs aus Algorithmus~\ref{alg:algorithm}.}
	\label{fig:nfa}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section{Kellerautomaten und Kontextfreie Sprachen}\label{sec:contextfree}

\newpage
\section{Kontextsensitive Sprachen}\label{sec:contextsensitive}

\newpage
\section{Prozesskalküle und Petri-Netze}\label{sec:process}

\newpage
\bibliography{sources}\addcontentsline{toc}{section}{References}
\bibliographystyle{alpha}

% eof
\end{document}
